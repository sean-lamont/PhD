{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# import json\n",
    "import new_env\n",
    "import copy\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "class AST:\n",
    "    def __init__(self, node, children=[], parent=None):\n",
    "        self.node = node\n",
    "        self.children = children\n",
    "        self.parent = [parent]\n",
    "    def _print(self, depth=1):\n",
    "        print (depth * \"--- \" + self.node.value)\n",
    "        if len(self.children) > 0:\n",
    "            for child in self.children:\n",
    "                child._print(depth+1)\n",
    "        \n",
    "class Token:\n",
    "    def __init__(self, value, _type, arity=None):\n",
    "        self.value = value\n",
    "        self._type = _type\n",
    "        self.arity = arity\n",
    "    \n",
    "#assume ast has been passed with ast.node as function \n",
    "def func_to_ast(ast, tokens, arity):\n",
    "    if len(tokens) == 0:\n",
    "        return ast\n",
    "\n",
    "    node = tokens[0]\n",
    "    tokens.pop(0)\n",
    "\n",
    "    new_node = AST(node, children = [], parent=ast)\n",
    "    \n",
    "    if node._type == \"variable\":\n",
    "\n",
    "        ast.children.append(new_node)\n",
    "\n",
    "    elif node._type == \"func\" or node._type == \"lambda\":\n",
    "\n",
    "        new_ast = func_to_ast(new_node, tokens, node.arity)\n",
    "        ast.children.append(new_ast)\n",
    "      \n",
    "        \n",
    "    if arity == 1:\n",
    "        return ast\n",
    "    else:\n",
    "        return func_to_ast(ast, tokens, arity-1)\n",
    "    \n",
    "def tokens_to_ast(tokens):\n",
    "    ast = AST(tokens[0], children=[])\n",
    "    tokens.pop(0)\n",
    "    return func_to_ast(ast, tokens, ast.node.arity)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def polished_to_tokens_2(goal):\n",
    "    polished_goal = goal.split(\" \")\n",
    "    tokens = []\n",
    "    \n",
    "    while len(polished_goal) > 0:\n",
    "        if polished_goal[0] == '@':\n",
    "            polished_goal.pop(0)\n",
    "            arity = 1\n",
    "            \n",
    "            while polished_goal[0] == '@':\n",
    "                arity += 1\n",
    "                polished_goal.pop(0)\n",
    "                \n",
    "            func = polished_goal[0]\n",
    "            polished_goal.pop(0)\n",
    "\n",
    "            if func[0] == 'C':\n",
    "                #should only be one string after the library \n",
    "                func = func + \"|\" + polished_goal[0]\n",
    "                polished_goal.pop(0)\n",
    "            \n",
    "            #otherwise variable func, and nothing following it\n",
    "\n",
    "            tokens.append(Token(func, \"func\", arity))\n",
    "        \n",
    "        #variable or constant case\n",
    "        else:\n",
    "            var = polished_goal[0]\n",
    "            polished_goal.pop(0)\n",
    "            #lambda case\n",
    "            if var[0] == \"|\":\n",
    "                tokens.append(Token(\"\".join(var), \"lambda\", 2))\n",
    "            else:\n",
    "            \n",
    "                if var[0] == \"C\":\n",
    "                    #need to append this and the next as constants are space separated\n",
    "                    var = var + polished_goal[0]\n",
    "                    polished_goal.pop(0)\n",
    "\n",
    "                tokens.append(Token(\"\".join(var), \"variable\"))\n",
    "\n",
    "    return tokens\n",
    "            \n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def hierarchy_pos(G, root, levels=None, width=1., height=1.):\n",
    "    '''If there is a cycle that is reachable from root, then this will see infinite recursion.\n",
    "       G: the graph\n",
    "       root: the root node\n",
    "       levels: a dictionary\n",
    "               key: level number (starting from 0)\n",
    "               value: number of nodes in this level\n",
    "       width: horizontal space allocated for drawing\n",
    "       height: vertical space allocated for drawing'''\n",
    "    TOTAL = \"total\"\n",
    "    CURRENT = \"current\"\n",
    "    def make_levels(levels, node=root, currentLevel=0, parent=None):\n",
    "        \"\"\"Compute the number of nodes for each level\n",
    "        \"\"\"\n",
    "        if not currentLevel in levels:\n",
    "            levels[currentLevel] = {TOTAL : 0, CURRENT : 0}\n",
    "        levels[currentLevel][TOTAL] += 1\n",
    "        neighbors = G.neighbors(node)\n",
    "        for neighbor in neighbors:\n",
    "            if not neighbor == parent:\n",
    "                levels =  make_levels(levels, neighbor, currentLevel + 1, node)\n",
    "        return levels\n",
    "\n",
    "    def make_pos(pos, node=root, currentLevel=0, parent=None, vert_loc=0):\n",
    "        dx = 1/levels[currentLevel][TOTAL]\n",
    "        left = dx/2\n",
    "        pos[node] = ((left + dx*levels[currentLevel][CURRENT])*width, vert_loc)\n",
    "        levels[currentLevel][CURRENT] += 1\n",
    "        neighbors = G.neighbors(node)\n",
    "        for neighbor in neighbors:\n",
    "            if not neighbor == parent:\n",
    "                pos = make_pos(pos, neighbor, currentLevel + 1, node, vert_loc-vert_gap)\n",
    "        return pos\n",
    "    if levels is None:\n",
    "        levels = make_levels({})\n",
    "    else:\n",
    "        levels = {l:{TOTAL: levels[l], CURRENT:0} for l in levels}\n",
    "    vert_gap = height / (max([l for l in levels])+1)\n",
    "    return make_pos({})\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def print_graph(ast):\n",
    "    G = nx.DiGraph()\n",
    "    add_node(ast, G)\n",
    "    \n",
    "    labels = nx.get_node_attributes(G, 'value')\n",
    "    pos = hierarchy_pos(G,ast.node)    \n",
    "    plt.figure(1,figsize=(15,30)) \n",
    "    nx.draw(G, pos=pos, labels=labels, with_labels=True,\n",
    "            arrowsize=20,\n",
    "            node_color='none',\n",
    "            node_size=6000)#, font_weight='bold')\n",
    "    \n",
    "    \n",
    "    labels = {e: G.get_edge_data(e[0], e[1])[\"child\"] for e in G.edges()}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "    ax = plt.gca() # to get the current axis\n",
    "    ax.collections[0].set_edgecolor(\"#FF0000\") \n",
    "    plt.savefig(\"Graph.png\", format=\"PNG\")\n",
    "    #plt.show()\n",
    "# 0\n",
    "\n",
    "def add_node(ast, graph):\n",
    "    graph.add_node(ast.node, value = str(ast.node.value))\n",
    "    for i, child in enumerate(ast.children):\n",
    "        graph.add_edge(ast.node, child.node, child=i)\n",
    "        add_node(child, graph)\n",
    "    \n",
    "#verticalalignment='bottom'\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def add_lambda_children(lambda_ast):\n",
    "    #child should be '|' with first child of that as variable, and rest as quantified scope\n",
    "    var = lambda_ast.children[0].node.value\n",
    "    sub_tree = lambda_ast.children[1]\n",
    "    \n",
    "    if sub_tree.node.value == var:\n",
    "        return lambda_ast\n",
    "    \n",
    "    def apply_lambda(ast, var):\n",
    "        if ast.node.value == var:\n",
    "            lambda_ast.children.append(ast)\n",
    "            ast.parent.append(lambda_ast)\n",
    "            for child in ast.children:\n",
    "                apply_lambda(child,var) \n",
    "        else:\n",
    "            for child in ast.children:\n",
    "                apply_lambda(child,var)\n",
    "        return\n",
    "    \n",
    "    apply_lambda(sub_tree, var)\n",
    "    return lambda_ast\n",
    "\n",
    "def process_lambdas(ast):\n",
    "    ret = []\n",
    "    def get_lambdas(ast):\n",
    "        if ast.node._type == \"lambda\":\n",
    "            ret.append(ast)\n",
    "        for child in ast.children:\n",
    "                get_lambdas(child)\n",
    "        return    \n",
    "    get_lambdas(ast)\n",
    "    \n",
    "    for l in ret:\n",
    "        add_lambda_children(l)\n",
    "    \n",
    "    return ast\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def merge_leaves(ast):\n",
    "    lambda_tokens = []\n",
    "    #only merge leaf nodes if they're within the same quantified scope\n",
    "    \n",
    "    def run_lambdas(lambda_ast):\n",
    "        var = lambda_ast.children[0].node.value\n",
    "        \n",
    "        #check for edge case lambda x: x\n",
    "        if len(lambda_ast.children) == 1:\n",
    "            return lambda_ast\n",
    "        \n",
    "        sub_tree = lambda_ast.children[1]\n",
    "        lambda_token = lambda_ast.children[0]\n",
    "        lambda_tokens.append(lambda_token)\n",
    "        \n",
    "        \n",
    "        def merge_lambda(ast, var):\n",
    "            #if lambda variable, and leaf node, point parents to original node\n",
    "            if ast.node.value == var and ast.children == []:\n",
    "                #this way ensures no duplicates \n",
    "                for parent in ast.parent:\n",
    "                    new_children = []\n",
    "                    flag = False\n",
    "                    for c in parent.children:\n",
    "                        if c.node.value != var or c.children != []:\n",
    "                            new_children.append(c)\n",
    "                        elif flag == False:\n",
    "                            new_children.append(lambda_token)\n",
    "                            flag = True\n",
    "                    parent.children = new_children\n",
    "                \n",
    "            for child in ast.children:\n",
    "                merge_lambda(child,var) \n",
    "\n",
    "            return ast \n",
    "\n",
    "        merge_lambda(sub_tree, var)\n",
    "        return lambda_ast\n",
    "\n",
    "    def merge_all_lambdas(ast):\n",
    "        ret = []\n",
    "        def get_lambdas(ast):\n",
    "            if ast.node._type == \"lambda\":\n",
    "                ret.append(ast)\n",
    "            for child in ast.children:\n",
    "                    get_lambdas(child)\n",
    "            return    \n",
    "\n",
    "        get_lambdas(ast)\n",
    "\n",
    "        for l in ret:\n",
    "            run_lambdas(l)\n",
    "\n",
    "        return ast\n",
    "    \n",
    "    merge_all_lambdas(ast)\n",
    "    \n",
    "#     #TODO no quantifier case\n",
    "#     print (set(lambda_tokens) - leaf_tokens)\n",
    "\n",
    "                \n",
    "    return ast\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def rename(ast):\n",
    "    if ast.node.value[0] == 'V':\n",
    "        if ast.children != []:\n",
    "            ast.node.value = \"VARFUNC\"\n",
    "        else:\n",
    "            ast.node.value = \"VAR\"\n",
    "\n",
    "    for child in ast.children:\n",
    "        rename(child)\n",
    "    \n",
    "    return ast\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def goal_to_graph(polished_goal):\n",
    "    return rename(merge_leaves(process_lambdas(tokens_to_ast(polished_to_tokens_2(polished_goal)))))\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# all_graphs = [goal_to_graph(g) for g in polished_goals]\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "with open(\"new_db.json\") as fp:\n",
    "    new_db = json.load(fp)\n",
    "\n",
    "\n",
    "polished_goals = []\n",
    "for val in new_db.values():\n",
    "    polished_goals.append(val[2])\n",
    "\n",
    "tokens = list(set([token.value for polished_goal in polished_goals for token in polished_to_tokens_2(polished_goal)  if token.value[0] != 'V']))\n",
    "\n",
    "tokens.append(\"VAR\")\n",
    "tokens.append(\"VARFUNC\")\n",
    "tokens.append(\"UNKNOWN\")\n",
    "\n",
    "#print (len(tokens))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "enc.fit(np.array(tokens).reshape(-1,1))\n",
    "\n",
    "e = enc.transform(np.array(tokens).reshape(-1,1))\n",
    "\n",
    "preds = enc.inverse_transform(e)\n",
    "\n",
    "#ensure encoding is correct\n",
    "assert [preds[:,0][i] for i in range(preds.shape[0])] == tokens\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "enc.inverse_transform(enc.transform(np.array(tokens[:10]).reshape(-1,1)))[:,0]\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "def nodes_list_to_senders_receivers(node_list):\n",
    "    senders = []\n",
    "    receivers = []\n",
    "    \n",
    "    for i, node in enumerate(node_list):\n",
    "        for child in node.children:\n",
    "            senders.append(i)\n",
    "            receivers.append(node_list.index(child))\n",
    "    return senders, receivers\n",
    "\n",
    "def nodes_list(g, result=[]):\n",
    "    result.append(g)\n",
    "    \n",
    "    for child in g.children:\n",
    "        nodes_list(child, result)\n",
    "            \n",
    "    return list(set(result))\n",
    "\n",
    "#possible add edge number, to consider ordering of the arguments\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(\"gnn_dataset.pk\", 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "X = [(d[0], d[1]) for d in dataset]\n",
    "y = [d[2] for d in dataset]\n",
    "\n",
    "# unique_exps = list(set([x for xs in X for x in xs]))\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# graph_dict = {g: graph_to_jgraph(goal_to_graph(g)) for g in tqdm(unique_exps)}\n",
    "\n",
    "\n",
    "# with open(\"graph_dataset.pk\", 'wb') as f:\n",
    "#     pickle.dump(graph_dict, f)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class F_p_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F_p_module, self).__init__()    \n",
    "        \n",
    "        self.fc = nn.Linear(256, 256)\n",
    "        \n",
    "        #self.bn = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(F.relu(self.fc(x)))\n",
    "        return  x\n",
    "\n",
    "    \n",
    "\n",
    "class F_i_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F_i_module, self).__init__()    \n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        \n",
    "        #self.bn2 = nn.BatchNorm1d(256)\n",
    "              \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return  x\n",
    "    \n",
    "\n",
    "class F_o_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F_o_module, self).__init__()    \n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        \n",
    "        #self.bn2 = nn.BatchNorm1d(256)\n",
    "              \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return  x\n",
    "    \n",
    "    \n",
    "    \n",
    "class F_x_module(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(F_x_module, self).__init__()    \n",
    "        \n",
    "        self.fc1 = nn.Linear(input_shape, 256)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.fc1(x)\n",
    "\n",
    "\n",
    "    \n",
    "class F_c_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F_c_module, self).__init__()    \n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        return torch.sigmoid(self.fc3(self.fc2(x)))\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "def init_graph(nodes, embedding_net):\n",
    "    nodes = embedding_net(nodes)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "\n",
    "def run_embedding_iteration(nodes, senders, receivers, F_p, F_i, F_o):\n",
    "    \n",
    "    if senders.shape[0] > 1:\n",
    "\n",
    "        nodes_with_neighbors = torch.concat([torch.stack([nodes[i] for i in senders]), \n",
    "                                             torch.stack([nodes[i] for i in receivers])], axis=-1)\n",
    "\n",
    "\n",
    "        F_i_outputs = F_i(nodes_with_neighbors)\n",
    "\n",
    "        F_o_outputs = F_o(nodes_with_neighbors)\n",
    "\n",
    "        \n",
    "        F_p_inputs = [x + torch.divide(1., (len(torch.where(senders == i)) + len(torch.where(receivers == i))))\n",
    "                                                 * (torch.sum(F_i_outputs[torch.where(receivers == i)]) + torch.sum(F_o_outputs[torch.where(senders == i)])) for i,x in enumerate(nodes)]\n",
    "        \n",
    "        \n",
    "        F_p_inputs = torch.stack(F_p_inputs).to(device)\n",
    "        \n",
    "        nodes = F_p(F_p_inputs)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        return nodes, senders, receivers\n",
    "    \n",
    "        \n",
    "        \n",
    "    nodes = F_p(F_p_inputs)\n",
    "    \n",
    "    return nodes, senders, receivers\n",
    "\n",
    "\n",
    "def generate_graph_embedding(graph, F_p, F_i, F_o, F_x, conv1, conv2, num_iterations):\n",
    "    \n",
    "    start_t = time.time()\n",
    "    nodes, _, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    receivers = torch.LongTensor(receivers).to(device)\n",
    "    \n",
    "    senders = torch.LongTensor(senders).to(device)\n",
    "\n",
    "    \n",
    "    nodes = torch.tensor(nodes, requires_grad=True).to(device)\n",
    "    \n",
    "    nodes = F_x(nodes)\n",
    "        \n",
    "    for t in range(num_iterations):\n",
    "        nodes, senders, receivers = run_embedding_iteration(nodes, senders, receivers, F_p, F_i, F_o)\n",
    "    \n",
    "   \n",
    "    nodes = conv2(conv1(nodes.reshape(nodes.shape[1], nodes.shape[0])))\n",
    "\n",
    "    \n",
    "    g_embedding = torch.nn.MaxPool1d(1, nodes.shape[1])(nodes)\n",
    "   \n",
    "\n",
    "    \n",
    "    return g_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[20]:\n",
    "\n",
    "\n",
    "with open(\"graph_dataset.pk\", 'rb') as f:\n",
    "    graph_dict = pickle.load( f)\n",
    "\n",
    "\n",
    "# In[21]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn jax dataset into torch\n",
    "torch_graph_dict = {}\n",
    "\n",
    "for k,v in graph_dict.items():\n",
    "    nodes, _, receivers, senders, _, _, _ = v\n",
    "\n",
    "    edges = torch.tensor([senders.tolist(), receivers.tolist()], dtype=torch.long)\n",
    "    \n",
    "    nodes = torch.tensor(nodes, dtype=torch.float)\n",
    "    \n",
    "    torch_graph_dict[k] = Data(x = nodes, edge_index = edges)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn.functional import dropout\n",
    "\n",
    "#no edge weights with this model\n",
    "# sum (MLP(node, parents))\n",
    "\n",
    "in_channels = out_channels = 256\n",
    "\n",
    "#F_o summed over children \n",
    "class Child_Aggregation(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='sum', flow='target_to_source') \n",
    "        \n",
    "        self.mlp = Seq(Dropout(), Linear(2 * in_channels, out_channels),\n",
    "                       ReLU(), Dropout(),\n",
    "                       Linear(out_channels, out_channels))\n",
    "\n",
    "        \n",
    "    def message(self, x_i, x_j):\n",
    "        # x_i has shape [E, in_channels]\n",
    "        # x_j has shape [E, in_channels]\n",
    "        \n",
    "        tmp = torch.cat([x_i, x_j], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
    "        return self.mlp(tmp)\n",
    "\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        #edge index 0 for degree wrt children\n",
    "\n",
    "        deg = degree(edge_index[0], x.size(0), dtype=x.dtype)\n",
    "        \n",
    "        deg_inv = 1. / deg\n",
    "        \n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "        \n",
    "        return deg_inv.view(-1,1) * self.propagate(edge_index, x=x)\n",
    "\n",
    "    \n",
    "#F_i summed over parents \n",
    "class Parent_Aggregation(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='sum', flow='source_to_target') \n",
    "        \n",
    "        self.mlp = Seq(Dropout(), Linear(2 * in_channels, out_channels),\n",
    "                       ReLU(), Dropout(),\n",
    "                       Linear(out_channels, out_channels))\n",
    "\n",
    "        \n",
    "    def message(self, x_i, x_j):\n",
    "        # x_i has shape [E, in_channels]\n",
    "        # x_j has shape [E, in_channels]\n",
    "        \n",
    "        tmp = torch.cat([x_i, x_j], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
    "        \n",
    "        return self.mlp(tmp)\n",
    "\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "    \n",
    "        #edge index 1 for degree wrt parents\n",
    "        deg = degree(edge_index[1], x.size(0), dtype=x.dtype)\n",
    "        \n",
    "        deg_inv = 1. / deg\n",
    "        \n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "        \n",
    "        return deg_inv.view(-1,1) * self.propagate(edge_index, x=x)\n",
    "\n",
    "class Final_Agg(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Final_Agg, self).__init__()    \n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim * 3, embedding_dim * 2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(dropout(torch.relu(self.fc(dropout(x)))))\n",
    "        return  x\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "class F_x_module(nn.Module):\n",
    "    def __init__(self, input_shape, embedding_dim):\n",
    "        super(F_x_module, self).__init__()    \n",
    "        \n",
    "        self.fc1 = nn.Linear(input_shape, embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.fc1(x)\n",
    "\n",
    "class F_c_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F_c_module, self).__init__()    \n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(dropout(x)))\n",
    "        \n",
    "        return torch.sigmoid(self.fc3(dropout(F.relu(self.fc2(dropout(x))))))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_embedding(graph, MLP_Agg, F_i_sum, F_o_sum, F_x, conv1, conv2, num_iterations):\n",
    "    \n",
    "    nodes = graph.x\n",
    "    edges = graph.edge_index\n",
    "  \n",
    "    nodes = F_x(nodes.to(device))\n",
    "        \n",
    "    for t in range(num_iterations):\n",
    "        fi_sum = F_i_sum(nodes.to(device), edges.to(device))\n",
    "        fo_sum = F_o_sum(nodes.to(device), edges.to(device))\n",
    "        node_update = MLP_Agg(torch.cat([nodes, fi_sum, fo_sum], axis=1).to(device))\n",
    "        nodes = nodes + node_update\n",
    "    \n",
    "    nodes = conv2(conv1(nodes.reshape(nodes.shape[1], nodes.shape[0])))\n",
    "    \n",
    "    g_embedding = torch.nn.MaxPool1d(1, nodes.shape[1])(nodes)\n",
    "    \n",
    "    return g_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss(preds, targets):\n",
    "    return -1. * torch.sum(targets * torch.log(preds) + (1 - targets) * torch.log((1. - preds)))\n",
    "\n",
    "def loss(graph_net, x_batch, y_batch, F_p, F_i, F_o, F_x, F_c, conv1, conv2, num_iterations):\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    for i in range(len(x_batch)):\n",
    "        x = x_batch[i]\n",
    "        \n",
    "        g0_embedding = graph_net(x[0], F_p, F_i, F_o, F_x, conv1, conv2, num_iterations).to(device)\n",
    "\n",
    "        g1_embedding  = graph_net(x[1], F_p, F_i, F_o, F_x, conv1, conv2, num_iterations).to(device)\n",
    "        \n",
    "        pred = F_c(torch.concat([g0_embedding, g1_embedding], axis=0).reshape(1,-1))[0][0]\n",
    "\n",
    "\n",
    "        preds.append(pred)\n",
    "        \n",
    "    eps = 1e-6\n",
    "    \n",
    "    \n",
    "    preds = torch.stack(preds).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds = torch.clip(preds, eps, 1-eps)\n",
    "            \n",
    "    return binary_loss(preds, torch.FloatTensor(y_batch).to(device))\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "step_size = 3e-4\n",
    "\n",
    "decay_rate = 0.02\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "fp = Final_Agg(embedding_dim).to(device)\n",
    "fi = Parent_Aggregation(embedding_dim, embedding_dim).to(device)\n",
    "fo = Child_Aggregation(embedding_dim, embedding_dim).to(device)\n",
    "fx = F_x_module(len(tokens), embedding_dim).to(device)\n",
    "fc = F_c_module().to(device)\n",
    "\n",
    "conv1 = torch.nn.Conv1d(embedding_dim, 512, 1, stride=1).to(device)\n",
    "conv2 = torch.nn.Conv1d(512, 1024, 1, stride=1).to(device)\n",
    "\n",
    "\n",
    "#fp = torch.load(\"torch_gnn_models/initial/fp\")\n",
    "#fi = torch.load(\"torch_gnn_models/initial/fi\")\n",
    "#fo = torch.load(\"torch_gnn_models/initial/fo\")\n",
    "#fx = torch.load(\"torch_gnn_models/initial/fx\")\n",
    "#fc = torch.load(\"torch_gnn_models/initial/fc\")\n",
    "\n",
    "\n",
    "optimiser_fp = torch.optim.Adam(list(fp.parameters()), lr=step_size, weight_decay=decay_rate)\n",
    "optimiser_fi = torch.optim.Adam(list(fi.parameters()), lr=step_size, weight_decay=decay_rate)\n",
    "optimiser_fo = torch.optim.Adam(list(fo.parameters()), lr=step_size, weight_decay=decay_rate)\n",
    "optimiser_fx = torch.optim.Adam(list(fx.parameters()), lr=step_size, weight_decay=decay_rate)\n",
    "optimiser_fc = torch.optim.Adam(list(fc.parameters()), lr=step_size, weight_decay=decay_rate)\n",
    "optimiser_fconv1 = torch.optim.Adam(list(conv1.parameters()), lr=step_size, weight_decay=decay_rate)\n",
    "optimiser_fconv2 = torch.optim.Adam(list(conv2.parameters()), lr=step_size, weight_decay=decay_rate)\n",
    "\n",
    "\n",
    "\n",
    "data_size = len(x_train)\n",
    "\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "\n",
    "def run():\n",
    "    for j in range(num_epochs):\n",
    "        for i in tqdm(range(0, int(data_size / batch_size))):\n",
    "\n",
    "            batch_idx = i + 1\n",
    "            from_idx = (batch_idx - 1) * batch_size\n",
    "            to_idx = batch_idx * batch_size\n",
    "\n",
    "            X_batch = [(torch_graph_dict[x[0]], torch_graph_dict[x[1]]) for x in x_train[from_idx:to_idx]]\n",
    "\n",
    "            y_batch = y_train[from_idx:to_idx]\n",
    "\n",
    "            optimiser_fp.zero_grad()\n",
    "            optimiser_fi.zero_grad()\n",
    "            optimiser_fo.zero_grad()\n",
    "            optimiser_fx.zero_grad()\n",
    "            optimiser_fc.zero_grad()\n",
    "            optimiser_fconv1.zero_grad()\n",
    "\n",
    "\n",
    "            loss_val = loss(generate_graph_embedding, X_batch, y_batch, fp, fi, fo, fx, fc,conv1,conv2, 1)\n",
    "\n",
    "            optimiser_fconv2.zero_grad()\n",
    "\n",
    "            loss_val.backward()          \n",
    "\n",
    "\n",
    "            optimiser_fp.step()\n",
    "            optimiser_fi.step()\n",
    "            optimiser_fo.step()\n",
    "            optimiser_fx.step()\n",
    "            optimiser_fc.step()\n",
    "\n",
    "            optimiser_fconv1.step()\n",
    "\n",
    "            optimiser_fconv2.step()\n",
    "\n",
    "\n",
    "            training_losses.append(loss_val.detach())\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                with open(\"training_losses_torch.pk\", \"wb+\") as f:\n",
    "                    pickle.dump(training_losses, f)\n",
    "\n",
    "\n",
    "                #print (\"Avg training loss: {}\".format(sum(training_losses) / len(training_losses)))\n",
    "                print (\"Curr training loss avg: {}\".format(sum(training_losses[-100:]) / len(training_losses[-100:])))\n",
    "            #if i % 500 == 0:\n",
    "\n",
    "                X_val = [(torch_graph_dict[x[0]], torch_graph_dict[x[1]]) for x in x_val]\n",
    "\n",
    "\n",
    "                inds = np.random.randint(0, len(X_val), 32)\n",
    "\n",
    "                x_val_batch = [X_val[k] for k in inds]\n",
    "                y_val_batch = [y_val[k] for k in inds]\n",
    "\n",
    "                loss_validation = loss(generate_graph_embedding, x_val_batch, y_val_batch, fp, fi, fo, fx, fc, conv1, conv2, 1)\n",
    "\n",
    "\n",
    "                val_losses.append(loss_validation.detach())\n",
    "\n",
    "                with open(\"val_losses_torch.pk\", \"wb+\") as f:\n",
    "                    pickle.dump(val_losses, f)\n",
    "\n",
    "\n",
    "                print (\"Avg val loss: {}\".format(sum(val_losses[-100:])/len(val_losses[-100:])))\n",
    "                print (\"Curr val loss: {}\".format(loss_validation))\n",
    "\n",
    "\n",
    "        print (\"Epoch {} done\".format(j))\n",
    "\n",
    "#         torch.save(fp, \"torch_gnn_models/initial/fp\")\n",
    "#         torch.save(fi, \"torch_gnn_models/initial/fi\")\n",
    "#         torch.save(fo, \"torch_gnn_models/initial/fo\")\n",
    "#         torch.save(fx, \"torch_gnn_models/initial/fx\")\n",
    "#         torch.save(fc, \"torch_gnn_models/initial/fc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 1/9200 [00:00<47:18,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curr training loss avg: 22.160371780395508\n",
      "Avg val loss: 22.140254974365234\n",
      "Curr val loss: 22.140254974365234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                       | 102/9200 [00:17<25:46,  5.88it/s]"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme",
   "language": "python",
   "name": "acme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
