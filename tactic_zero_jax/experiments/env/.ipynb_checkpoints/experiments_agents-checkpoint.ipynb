{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considerable-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from jax import random\n",
    "import sys\n",
    "from time import sleep\n",
    "import json\n",
    "import pexpect\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "import seq2seq\n",
    "from batch_predictor import BatchPredictor\n",
    "from checkpoint import Checkpoint\n",
    "\n",
    "from policy_networks import *\n",
    "import policy_networks\n",
    "\n",
    "import utp_model\n",
    "\n",
    "from new_env import *\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True) \n",
    "import numpy as np\n",
    "#jax.config.update(\"jax_enable_x64\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "necessary-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOLPATH = \"/home/sean/Documents/hol/HOL/bin/hol --maxheap=256\"\n",
    "HOLPATH = \"/home/sean/Documents/PhD/HOL4/HOL/bin/hol --maxheap=256\"\n",
    "\n",
    "#tactic_zero_path = \"/home/sean/Documents/PhD/git/repo/PhD/tacticzero/holgym/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liked-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "MORE_TACTICS = True\n",
    "if not MORE_TACTICS:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\"]\n",
    "    thm_tactic = [\"irule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\"]\n",
    "else:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "    thm_tactic = [\"irule\", \"drule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\", \"EQ_TAC\"]\n",
    "    \n",
    "tactic_pool = thms_tactic + thm_tactic + term_tactic + no_arg_tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "textile-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Move to another file \n",
    "\n",
    "def get_polish(raw_goal):\n",
    "        goal = construct_goal(raw_goal)\n",
    "        process.sendline(goal.encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer (HOLPP.add_string o pt);\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"top_goals();\".encode(\"utf-8\"))\n",
    "        process.expect(\"val it =\")\n",
    "        process.expect([\": goal list\", \":\\r\\n +goal list\"])\n",
    "\n",
    "        polished_raw = process.before.decode(\"utf-8\")\n",
    "        polished_subgoals = re.sub(\"“|”\",\"\\\"\", polished_raw)\n",
    "        polished_subgoals = re.sub(\"\\r\\n +\",\" \", polished_subgoals)\n",
    "\n",
    "        pd = eval(polished_subgoals)\n",
    "        \n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"drop();\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer default_pt;\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "\n",
    "        data = [{\"polished\":{\"assumptions\": e[0][0], \"goal\":e[0][1]},\n",
    "                 \"plain\":{\"assumptions\": e[1][0], \"goal\":e[1][1]}}\n",
    "                for e in zip(pd, [([], raw_goal)])]\n",
    "        return data \n",
    "    \n",
    "def construct_goal(goal):\n",
    "    s = \"g \" + \"`\" + goal + \"`;\"\n",
    "    return s\n",
    "\n",
    "def gather_encoded_content_(history, encoder):\n",
    "    fringe_sizes = []\n",
    "    contexts = []\n",
    "    reverted = []\n",
    "    for i in history:\n",
    "        c = i[\"content\"]\n",
    "        contexts.extend(c)\n",
    "        fringe_sizes.append(len(c))\n",
    "    for e in contexts:\n",
    "        g = revert_with_polish(e)\n",
    "        reverted.append(g.strip().split())\n",
    "    out = []\n",
    "    sizes = []\n",
    "    for goal in reverted:\n",
    "        out_, sizes_ = encoder.encode([goal])\n",
    "        out.append(torch.cat(out_.split(1), dim=2).squeeze(0))\n",
    "        sizes.append(sizes_)\n",
    "\n",
    "    representations = out\n",
    "\n",
    "    return representations, contexts, fringe_sizes\n",
    "\n",
    "def parse_theory(pg):\n",
    "    theories = re.findall(r'C\\$(\\w+)\\$ ', pg)\n",
    "    theories = set(theories)\n",
    "    for th in EXCLUDED_THEORIES:\n",
    "        theories.discard(th)\n",
    "    return list(theories)\n",
    "\n",
    "def revert_with_polish(context):\n",
    "    target = context[\"polished\"]\n",
    "    assumptions = target[\"assumptions\"]\n",
    "    goal = target[\"goal\"]\n",
    "    for i in reversed(assumptions): \n",
    "        #goal = \"@ @ D$min$==> {} {}\".format(i, goal)\n",
    "        goal = \"@ @ C$min$ ==> {} {}\".format(i, goal)\n",
    "\n",
    "    return goal \n",
    "\n",
    "def split_by_fringe(goal_set, goal_scores, fringe_sizes):\n",
    "    # group the scores by fringe\n",
    "    fs = []\n",
    "    gs = []\n",
    "    counter = 0\n",
    "    for i in fringe_sizes:\n",
    "        end = counter + i\n",
    "        fs.append(goal_scores[counter:end])\n",
    "        gs.append(goal_set[counter:end])\n",
    "        counter = end\n",
    "    return gs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "obvious-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "High level agent class \n",
    "\n",
    "'''\n",
    "class Agent:\n",
    "    def __init__(self, tactic_pool):\n",
    "        self.tactic_pool = tactic_pool    \n",
    "        self.load_encoder()\n",
    "    \n",
    "    def load_agent(self):\n",
    "        pass\n",
    "    \n",
    "    def load_encoder(self):\n",
    "        pass\n",
    "        \n",
    "    def run(self, env, max_steps):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "Vanilla Torch implementation of TacticZero\n",
    "\n",
    "'''\n",
    "class TorchVanilla(Agent):\n",
    "    def __init__(self, tactic_pool):\n",
    "        super().__init__(tactic_pool)\n",
    "\n",
    "        self.ARG_LEN = 5\n",
    "        learning_rate = 1e-5\n",
    "\n",
    "        self.context_rate = 5e-5\n",
    "        self.tac_rate = 5e-5\n",
    "        self.arg_rate = 5e-5\n",
    "        self.term_rate = 5e-5\n",
    "        \n",
    "        self.gamma = 0.99 # 0.9\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.context_net = utp_model.ContextPolicy().to(self.device)\n",
    "        self.tac_net = utp_model.TacPolicy(len(tactic_pool)).to(self.device)\n",
    "        self.arg_net = utp_model.ArgPolicy(len(tactic_pool), 256).to(self.device)\n",
    "        self.term_net = utp_model.TermPolicy(len(tactic_pool), 256).to(self.device)\n",
    "\n",
    "        self.optimizer_context = torch.optim.RMSprop(list(self.context_net.parameters()), lr=self.context_rate)\n",
    "        self.optimizer_tac = torch.optim.RMSprop(list(self.tac_net.parameters()), lr=self.tac_rate)\n",
    "        self.optimizer_arg = torch.optim.RMSprop(list(self.arg_net.parameters()), lr=self.arg_rate)\n",
    "        self.optimizer_term = torch.optim.RMSprop(list(self.term_net.parameters()), lr=self.term_rate)\n",
    "\n",
    "     \n",
    "    def load_encoder(self):\n",
    "        checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "\n",
    "        checkpoint = Checkpoint.load(checkpoint_path)\n",
    "        seq2seq = checkpoint.model\n",
    "        input_vocab = checkpoint.input_vocab\n",
    "        output_vocab = checkpoint.output_vocab\n",
    "\n",
    "        self.encoder = BatchPredictor(seq2seq, input_vocab, output_vocab)\n",
    "        return \n",
    "\n",
    "        \n",
    "    def run(self, env, encoded_fact_pool, allowed_arguments_ids, candidate_args, max_steps=50):\n",
    "        \n",
    "        \n",
    "        fringe_pool = []\n",
    "        tac_pool = []\n",
    "        arg_pool = []\n",
    "        action_pool = []\n",
    "        reward_pool = []\n",
    "        reward_print = []\n",
    "        tac_print = []\n",
    "        induct_arg = []\n",
    "        proved = 0\n",
    "        iteration_rewards = []\n",
    "        steps = 0        \n",
    "        \n",
    "        trace = []\n",
    "        \n",
    "        start_t = time.time()\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            \n",
    "            # gather all the goals in the history\n",
    "            try:\n",
    "                representations, context_set, fringe_sizes = gather_encoded_content(env.history, self.encoder)\n",
    "            except Exception as e:\n",
    "                print (\"Encoder error {}\".format(e))\n",
    "                return (\"Encoder error\", str(e))\n",
    "\n",
    "\n",
    "            representations = torch.stack([i.to(self.device) for i in representations])\n",
    "            context_scores = self.context_net(representations)\n",
    "            contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "            fringe_scores = []\n",
    "            for s in scores_by_fringe:\n",
    "                fringe_score = torch.sum(s) \n",
    "                fringe_scores.append(fringe_score)\n",
    "            fringe_scores = torch.stack(fringe_scores)\n",
    "            fringe_probs = F.softmax(fringe_scores, dim=0)\n",
    "            fringe_m = Categorical(fringe_probs)\n",
    "            fringe = fringe_m.sample()\n",
    "            fringe_pool.append(fringe_m.log_prob(fringe))\n",
    "\n",
    "            # take the first context in the chosen fringe for now\n",
    "            try:\n",
    "                target_context = contexts_by_fringe[fringe][0]\n",
    "            except:\n",
    "                print (\"error {} {}\".format(contexts_by_fringe, fringe))\n",
    "\n",
    "            target_goal = target_context[\"polished\"][\"goal\"]\n",
    "            target_representation = representations[context_set.index(target_context)]\n",
    "\n",
    "\n",
    "            tac_input = target_representation#.unsqueeze(0)\n",
    "            tac_input = tac_input.to(self.device)\n",
    "\n",
    "            tac_probs = self.tac_net(tac_input)\n",
    "            tac_m = Categorical(tac_probs)\n",
    "            tac = tac_m.sample()\n",
    "            tac_pool.append(tac_m.log_prob(tac))\n",
    "            action_pool.append(tactic_pool[tac])\n",
    "            tac_print.append(tac_probs.detach())\n",
    "\n",
    "\n",
    "            tac_tensor = tac.to(self.device)\n",
    "\n",
    "            if tactic_pool[tac] in no_arg_tactic:\n",
    "                tactic = tactic_pool[tac]\n",
    "                arg_probs = []\n",
    "                arg_probs.append(torch.tensor(0))\n",
    "                arg_pool.append(arg_probs)\n",
    "                \n",
    "            elif tactic_pool[tac] == \"Induct_on\":\n",
    "                arg_probs = []\n",
    "                candidates = []\n",
    "\n",
    "                tokens = target_goal.split()\n",
    "                tokens = list(dict.fromkeys(tokens))\n",
    "                tokens = [[t] for t in tokens if t[0] == \"V\"]\n",
    "                if tokens:\n",
    "                    token_representations, _ = self.encoder.encode(tokens)\n",
    "                \n",
    "                    encoded_tokens = torch.cat(token_representations.split(1), dim=2).squeeze(0)\n",
    "                \n",
    "                    target_representation_list = [target_representation for _ in tokens]\n",
    "\n",
    "                    target_representations = torch.cat(target_representation_list)\n",
    "\n",
    "                    candidates = torch.cat([encoded_tokens, target_representations], dim=1)\n",
    "                    candidates = candidates.to(self.device)\n",
    "\n",
    "\n",
    "                    scores = self.term_net(candidates, tac_tensor)\n",
    "                    term_probs = F.softmax(scores, dim=0)\n",
    "                    try:\n",
    "                        term_m = Categorical(term_probs.squeeze(1))\n",
    "                    except:\n",
    "                        print(\"probs: {}\".format(term_probs))                                          \n",
    "                        print(\"candidates: {}\".format(candidates.shape))\n",
    "                        print(\"scores: {}\".format(scores))\n",
    "                        print(\"tokens: {}\".format(tokens))\n",
    "                        exit()\n",
    "                    term = term_m.sample()\n",
    "                    arg_probs.append(term_m.log_prob(term))\n",
    "                    induct_arg.append(tokens[term])                \n",
    "                    tm = tokens[term][0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "                    arg_pool.append(arg_probs)\n",
    "                    if tm:\n",
    "                        tactic = \"Induct_on `{}`\".format(tm)\n",
    "                    else:\n",
    "                        print(\"tm is empty\")\n",
    "                        print(tokens)\n",
    "                        # only to raise an error\n",
    "                        tactic = \"Induct_on\"\n",
    "                else:\n",
    "                    arg_probs.append(torch.tensor(0))\n",
    "                    induct_arg.append(\"No variables\")\n",
    "                    arg_pool.append(arg_probs)\n",
    "                    tactic = \"Induct_on\"\n",
    "            else:\n",
    "                hidden0 = hidden1 = target_representation\n",
    "                hidden0 = hidden0.to(self.device)\n",
    "                hidden1 = hidden1.to(self.device)\n",
    "\n",
    "                hidden = (hidden0, hidden1)\n",
    "                \n",
    "                # concatenate the candidates with hidden states.\n",
    "\n",
    "                hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "                hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "                \n",
    "                hiddenl = torch.cat(hiddenl)\n",
    "\n",
    "                candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "                candidates = candidates.to(self.device)\n",
    "                            \n",
    "                input = tac_tensor\n",
    "                \n",
    "                # run it once before predicting the first argument\n",
    "                hidden, _ = self.arg_net(input, candidates, hidden)\n",
    "\n",
    "                # the indices of chosen args\n",
    "                arg_step = []\n",
    "                arg_step_probs = []\n",
    "                \n",
    "                if tactic_pool[tac] in thm_tactic:\n",
    "                    arg_len = 1\n",
    "                else:\n",
    "                    arg_len = self.ARG_LEN#ARG_LEN\n",
    "\n",
    "\n",
    "                for _ in range(arg_len):\n",
    "                    hidden, scores = self.arg_net(input, candidates, hidden)\n",
    "                    arg_probs = F.softmax(scores, dim=0)\n",
    "                    arg_m = Categorical(arg_probs.squeeze(1))\n",
    "                    arg = arg_m.sample()\n",
    "                    arg_step.append(arg)\n",
    "                    arg_step_probs.append(arg_m.log_prob(arg))\n",
    "\n",
    "                    hidden0 = hidden[0].squeeze().repeat(1, 1, 1)\n",
    "                    hidden1 = hidden[1].squeeze().repeat(1, 1, 1)\n",
    "                    \n",
    "                    # encoded chosen argument\n",
    "                    input = encoded_fact_pool[arg].unsqueeze(0)#.unsqueeze(0)\n",
    "\n",
    "                    # renew candidates                \n",
    "                    hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "                    hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "\n",
    "                    hiddenl = torch.cat(hiddenl)\n",
    "                    #appends both hidden and cell states (when paper only does hidden?)\n",
    "                    candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "                    candidates = candidates.to(self.device)\n",
    "\n",
    "                arg_pool.append(arg_step_probs)\n",
    "\n",
    "                tac = tactic_pool[tac]\n",
    "                arg = [candidate_args[j] for j in arg_step]\n",
    "\n",
    "                tactic = env.assemble_tactic(tac, arg)\n",
    "\n",
    "            action = (fringe.item(), 0, tactic)\n",
    "            \n",
    "            trace.append(action)\n",
    "\n",
    "            #print (action)\n",
    "            # reward, done = env.step(action)\n",
    "            try:\n",
    "                reward, done = env.step(action)\n",
    "\n",
    "            except:\n",
    "                print(\"Step exception raised.\")\n",
    "                return (\"Step error\", action)\n",
    "                # print(\"Fringe: {}\".format(env.history))\n",
    "                print(\"Handling: {}\".format(env.handling))\n",
    "                print(\"Using: {}\".format(env.using))\n",
    "                # try again\n",
    "                # counter = env.counter\n",
    "                frequency = env.frequency\n",
    "                env.close()\n",
    "                print(\"Aborting current game ...\")\n",
    "                print(\"Restarting environment ...\")\n",
    "                print(env.goal)\n",
    "                env = HolEnv(env.goal)\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            if t == 49:\n",
    "                reward = -5\n",
    "                \n",
    "            #could add environment state, but would grow rapidly\n",
    "            trace.append((reward, action))\n",
    "            \n",
    "            reward_print.append(reward)\n",
    "            reward_pool.append(reward)\n",
    "\n",
    "            steps += 1\n",
    "            total_reward = float(np.sum(reward_print))\n",
    "\n",
    "            if done == True:\n",
    "                print (\"Goal Proved in {} steps\".format(t+1))\n",
    "                break\n",
    "\n",
    "            if t == 49:\n",
    "                print(\"Failed.\")\n",
    "                #print(\"Rewards: {}\".format(reward_print))\n",
    "                # print(\"Rewards: {}\".format(reward_pool))\n",
    "                #print(\"Tactics: {}\".format(action_pool))\n",
    "                # print(\"Mean reward: {}\\n\".format(np.mean(reward_pool)))\n",
    "                #print(\"Total: {}\".format(total_reward))\n",
    "                iteration_rewards.append(total_reward)\n",
    "\n",
    "        \n",
    "        self.update_params(reward_pool, fringe_pool, arg_pool, tac_pool, steps)\n",
    "        \n",
    "        elapsed = time.time() - start_t\n",
    "\n",
    "        #print (elapsed)\n",
    "\n",
    "        return trace, steps, done\n",
    "\n",
    "    def update_params(self, reward_pool, fringe_pool, arg_pool, tac_pool, step_count):\n",
    "        # Update policy\n",
    "        # Discount reward\n",
    "        print(\"Updating parameters ... \")\n",
    "        running_add = 0\n",
    "        for i in reversed(range(step_count)):\n",
    "            if reward_pool[i] == 0:\n",
    "                running_add = 0\n",
    "            else:\n",
    "                running_add = running_add * self.gamma + reward_pool[i]\n",
    "                reward_pool[i] = running_add\n",
    "\n",
    "        self.optimizer_context.zero_grad()\n",
    "        self.optimizer_tac.zero_grad()\n",
    "        self.optimizer_arg.zero_grad()\n",
    "        self.optimizer_term.zero_grad()\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(step_count):\n",
    "            reward = reward_pool[i]\n",
    "            \n",
    "            fringe_loss = -fringe_pool[i] * (reward)\n",
    "            arg_loss = -torch.sum(torch.stack(arg_pool[i])) * (reward)\n",
    "            tac_loss = -tac_pool[i] * (reward)\n",
    "            \n",
    "            loss = fringe_loss + tac_loss + arg_loss\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        self.optimizer_context.step()\n",
    "        self.optimizer_tac.step()\n",
    "        self.optimizer_arg.step()\n",
    "        self.optimizer_term.step()\n",
    "        \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "indian-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, agent, goals, db_dir, encoded_db_dir, num_iterations):\n",
    "        self.agent = agent\n",
    "        self.goals = goals\n",
    "        self.num_iterations = num_iterations\n",
    "        self.load_db(db_dir)\n",
    "        self.load_encoded_db(encoded_db_dir)\n",
    "        \n",
    "    def train(self):\n",
    "        env = HolEnv(\"T\")\n",
    "        \n",
    "        env_errors = []\n",
    "        agent_errors = []\n",
    "        full_trace = []\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            iter_trace = {}\n",
    "            for i, goal in enumerate(self.goals):\n",
    "                print (\"Goal #{}\".format(str(i+1)))\n",
    "\n",
    "                try:\n",
    "                    env.reset(goal[1])\n",
    "                except Exception as e:\n",
    "                    print (\"Restarting environment..\")\n",
    "                    env = HolEnv(\"T\")\n",
    "                    env_errors.append((goal, e, i))\n",
    "                    continue\n",
    "                 \n",
    "                try:\n",
    "                    encoded_fact_pool, allowed_arguments_ids, candidate_args = self.gen_fact_pool(env, goal)\n",
    "                except:\n",
    "                    env_errors.append((goal, \"Error generating fact pool\", i))\n",
    "                    continue\n",
    "                    \n",
    "                result = self.agent.run(env, \n",
    "                                        encoded_fact_pool, \n",
    "                                        allowed_arguments_ids, candidate_args, max_steps=50)\n",
    "                \n",
    "                #agent run returns (error_msg, details) if error, else 3-tuple\n",
    "                if len(result) == 2:\n",
    "                    agent_errors.append((result, i))\n",
    "                else:\n",
    "                    trace, steps, done = result\n",
    "                    iter_trace[goal[0]] = (trace, steps, done)\n",
    "            \n",
    "            full_trace.append(iter_trace)\n",
    "            \n",
    "            return full_trace, env_errors, agent_errors\n",
    "            \n",
    "                \n",
    "                \n",
    "    def load_encoded_db(self, encoded_db_dir):\n",
    "        self.encoded_database = torch.load(encoded_db_dir)\n",
    "\n",
    "    def load_db(self, db_dir):\n",
    "        with open(db_dir) as f:\n",
    "            self.database = json.load(f)\n",
    "\n",
    "    def gen_fact_pool(self, env, goal):\n",
    "\n",
    "        allowed_theories = list(set(re.findall(r'C\\$(\\w+)\\$ ', goal[0])))\n",
    "        \n",
    "        goal_theory = goal[1]\n",
    "    \n",
    "        polished_goal = env.fringe[\"content\"][0][\"polished\"][\"goal\"]\n",
    "        \n",
    "        try:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(self.database):\n",
    "                if self.database[t][0] in allowed_theories and (self.database[t][0] != goal_theory or int(self.database[t][2]) < int(self.database[polished_goal][2])):\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "\n",
    "            env.toggle_simpset(\"diminish\", goal_theory)\n",
    "            #print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "        except:\n",
    "            allowed_arsguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(self.database):\n",
    "                if self.database[t][0] in allowed_theories:\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "            raise Exception(\"Theorem not found in database.\")\n",
    "\n",
    "        #print (\"Number of candidate facts to use: {}\".format(len(candidate_args)))\n",
    "        try:\n",
    "            encoded_fact_pool = torch.index_select(self.encoded_database, 0, torch.tensor(allowed_arguments_ids))\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Index select error {}\".format(e))\n",
    "        return encoded_fact_pool, allowed_arguments_ids, candidate_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "monetary-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TorchVanilla(tactic_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "impressive-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.json\") as fp:\n",
    "    dataset = json.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "educated-symphony",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing theories...\n",
      "Loading modules...\n",
      "Configuration done.\n",
      "DATATYPE ((sum :(α -> α + β) -> (β -> α + β) -> γ) (INL :α -> α + β) (INR :β -> α + β))\n",
      "∀(A :α -> bool) (B :β -> bool) (C :α -> bool) (D :β -> bool). A × B ∩ (C × D) = A ∩ C × (B ∩ D)\n",
      "DATATYPE ((list :α list -> (α -> α list -> α list) -> bool) ([] :α list) (CONS :α -> α list -> α list))\n"
     ]
    }
   ],
   "source": [
    "env = HolEnv(\"T\")\n",
    "paper_goals = []\n",
    "for goal in dataset:\n",
    "    try:\n",
    "        p_goal = env.get_polish(goal)\n",
    "        paper_goals.append((p_goal[0][\"polished\"]['goal'], goal))\n",
    "    except:\n",
    "        print (goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aboriginal-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(test, paper_goals,\"include_probability.json\", 'encoded_include_probability.pt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "elect-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trace, env_errors, agent_errors = exp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "linear-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Add logging, saving of logs, experiment metadata (date, agent info, database used etc.)\n",
    "#Add validation logic\n",
    "#Add replays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-quick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
