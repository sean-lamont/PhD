{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considerable-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from jax import random\n",
    "import sys\n",
    "from time import sleep\n",
    "import json\n",
    "import pexpect\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "import seq2seq\n",
    "from batch_predictor import BatchPredictor\n",
    "from checkpoint import Checkpoint\n",
    "\n",
    "from policy_networks import *\n",
    "import policy_networks\n",
    "\n",
    "import utp_model\n",
    "\n",
    "from new_env import *\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True) \n",
    "import numpy as np\n",
    "#jax.config.update(\"jax_enable_x64\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intelligent-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path_dir = \"/home/sean/Documents/PhD/tactic_zero_jax/env/model_params\"\n",
    "\n",
    "def save(params, path):\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(params, fp)\n",
    "\n",
    "def load(path):\n",
    "    with open(path, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "necessary-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOLPATH = \"/home/sean/Documents/hol/HOL/bin/hol --maxheap=256\"\n",
    "HOLPATH = \"/home/sean/Documents/PhD/HOL4/HOL/bin/hol --maxheap=256\"\n",
    "\n",
    "#tactic_zero_path = \"/home/sean/Documents/PhD/git/repo/PhD/tacticzero/holgym/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desperate-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"typed_database.json\") as f:\n",
    "    database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liked-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "MORE_TACTICS = True\n",
    "if not MORE_TACTICS:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\"]\n",
    "    thm_tactic = [\"irule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\"]\n",
    "else:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "    thm_tactic = [\"irule\", \"drule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\", \"EQ_TAC\"]\n",
    "    \n",
    "tactic_pool = thms_tactic + thm_tactic + term_tactic + no_arg_tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "textile-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Move to another file \n",
    "\n",
    "def get_polish(raw_goal):\n",
    "        goal = construct_goal(raw_goal)\n",
    "        process.sendline(goal.encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer (HOLPP.add_string o pt);\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"top_goals();\".encode(\"utf-8\"))\n",
    "        process.expect(\"val it =\")\n",
    "        process.expect([\": goal list\", \":\\r\\n +goal list\"])\n",
    "\n",
    "        polished_raw = process.before.decode(\"utf-8\")\n",
    "        polished_subgoals = re.sub(\"“|”\",\"\\\"\", polished_raw)\n",
    "        polished_subgoals = re.sub(\"\\r\\n +\",\" \", polished_subgoals)\n",
    "\n",
    "        pd = eval(polished_subgoals)\n",
    "        \n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"drop();\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer default_pt;\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "\n",
    "        data = [{\"polished\":{\"assumptions\": e[0][0], \"goal\":e[0][1]},\n",
    "                 \"plain\":{\"assumptions\": e[1][0], \"goal\":e[1][1]}}\n",
    "                for e in zip(pd, [([], raw_goal)])]\n",
    "        return data \n",
    "    \n",
    "def construct_goal(goal):\n",
    "    s = \"g \" + \"`\" + goal + \"`;\"\n",
    "    return s\n",
    "\n",
    "def gather_encoded_content_(history, encoder):\n",
    "    fringe_sizes = []\n",
    "    contexts = []\n",
    "    reverted = []\n",
    "    for i in history:\n",
    "        c = i[\"content\"]\n",
    "        contexts.extend(c)\n",
    "        fringe_sizes.append(len(c))\n",
    "    for e in contexts:\n",
    "        g = revert_with_polish(e)\n",
    "        reverted.append(g.strip().split())\n",
    "    out = []\n",
    "    sizes = []\n",
    "    for goal in reverted:\n",
    "        out_, sizes_ = encoder.encode([goal])\n",
    "        out.append(torch.cat(out_.split(1), dim=2).squeeze(0))\n",
    "        sizes.append(sizes_)\n",
    "\n",
    "    representations = out\n",
    "\n",
    "    return representations, contexts, fringe_sizes\n",
    "\n",
    "def parse_theory(pg):\n",
    "    theories = re.findall(r'C\\$(\\w+)\\$ ', pg)\n",
    "    theories = set(theories)\n",
    "    for th in EXCLUDED_THEORIES:\n",
    "        theories.discard(th)\n",
    "    return list(theories)\n",
    "\n",
    "def revert_with_polish(context):\n",
    "    target = context[\"polished\"]\n",
    "    assumptions = target[\"assumptions\"]\n",
    "    goal = target[\"goal\"]\n",
    "    for i in reversed(assumptions): \n",
    "        #goal = \"@ @ D$min$==> {} {}\".format(i, goal)\n",
    "        goal = \"@ @ C$min$ ==> {} {}\".format(i, goal)\n",
    "\n",
    "    return goal \n",
    "\n",
    "def split_by_fringe(goal_set, goal_scores, fringe_sizes):\n",
    "    # group the scores by fringe\n",
    "    fs = []\n",
    "    gs = []\n",
    "    counter = 0\n",
    "    for i in fringe_sizes:\n",
    "        end = counter + i\n",
    "        fs.append(goal_scores[counter:end])\n",
    "        gs.append(goal_set[counter:end])\n",
    "        counter = end\n",
    "    return gs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capital-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇒ LIST_REL R (l1 ++ l3) (l2 ++ l4)\n"
     ]
    }
   ],
   "source": [
    "with open(\"include_probability.json\") as f:\n",
    "    database = json.load(f)\n",
    "\n",
    "with open(\"polished_def_dict.json\") as f:\n",
    "    defs = json.load(f)\n",
    "\n",
    "fact_pool = list(defs.keys())\n",
    "\n",
    "encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "TARGET_THEORIES = [\"bool\", \"min\", \"list\"]\n",
    "GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "print (GOALS[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forbidden-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'seq2seq.models.EncoderRNN.EncoderRNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'seq2seq.models.DecoderRNN.DecoderRNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_path = \"models/2020_04_22_20_36_50\" # 91% accuracy model, only core theories\n",
    "#checkpoint_path = \"models/2020_04_26_20_11_28\" # 95% accuracy model, core theories + integer + sorting\n",
    "#checkpoint_path = \"models/2020_09_24_23_38_06\" # 98% accuracy model, core theories + integer + sorting | separate theory tokens\n",
    "#checkpoint_path = \"models/2020_11_28_16_45_10\" # 96-98% accuracy model, core theories + integer + sorting + real | separate theory tokens\n",
    "#checkpoint_path = \"models/2020_12_04_03_47_22\" # 97% accuracy model, core theories + integer + sorting + real + bag | separate theory tokens\n",
    "\n",
    "#checkpoint_path = \"models/2021_02_21_15_46_04\" # 98% accuracy model, up to probability theory\n",
    "\n",
    "checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "\n",
    "checkpoint = Checkpoint.load(checkpoint_path)\n",
    "seq2seq = checkpoint.model\n",
    "input_vocab = checkpoint.input_vocab\n",
    "output_vocab = checkpoint.output_vocab\n",
    "\n",
    "batch_encoder_ = BatchPredictor(seq2seq, input_vocab, output_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "virtual-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to give the log probability of pi(f | s) so gradient can be computed directly\n",
    "#also returns sampled index and contexts to determine goal to give tactic network\n",
    "def sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes):\n",
    "    context_scores = context_net(context_params, rng_key, jax_reps)\n",
    "    contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "    fringe_scores = []\n",
    "    for s in scores_by_fringe:\n",
    "        fringe_score = jnp.sum(s)\n",
    "        fringe_scores.append(fringe_score)\n",
    "    #TODO some fringes can be empty, but still give value 0 which assigns nonzero probability?\n",
    "    fringe_scores = jnp.stack(fringe_scores)\n",
    "    fringe_probs = jax.nn.log_softmax(fringe_scores)\n",
    "\n",
    "    #samples, gives an index (looks like it does gumbel softmax under the hood to keep differentiability?)\n",
    "    sampled_idx = jax.random.categorical(rng_key,fringe_probs)\n",
    "\n",
    "    log_prob = fringe_probs[sampled_idx]\n",
    "    #log_prob = jnp.log(prob)\n",
    "    return log_prob, (sampled_idx, contexts_by_fringe)\n",
    "                                                           \n",
    "#grad_log_context, (fringe_idx, contexts_by_fringe) = jax.grad(sample_fringe, has_aux=True)(context_params, apply_context, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "\n",
    "#takes a goal encoding and samples tactic from network, and returns log prob for gradient \n",
    "def sample_tactic(tactic_params, tac_net, rng_key, goal_endcoding, action_size=len(tactic_pool)):\n",
    "    tac_scores = tac_net(tactic_params, rng_key, goal_endcoding, action_size)\n",
    "    tac_scores = jnp.ravel(tac_scores)\n",
    "    #tac_scores = tac_scores - max(tac_scores)\n",
    "    #print (tac_scores)\n",
    "    #subtract max element for numerical stability \n",
    "    tac_probs = jax.nn.log_softmax(tac_scores)\n",
    "    tac_idx = jax.random.categorical(rng_key, tac_probs)\n",
    "    log_prob = tac_probs[tac_idx]#jnp.log(tac_probs[tac_idx])\n",
    "    #print (jnp.exp(log_prob).primal)#, jnp.exp(tac_probs), rng_key)\n",
    "    return log_prob, tac_idx\n",
    "\n",
    "#grad_log_tac, tac_idx = jax.grad(sample_tactic, has_aux=True)(tactic_params, apply_tac, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "\n",
    "#sampled_tac = tactic_pool[tac_idx]\n",
    "\n",
    "def sample_term(term_params, term_net, rng_key, candidates):\n",
    "    term_scores = term_net(term_params, rng_key, candidates)\n",
    "    term_scores = jnp.ravel(term_scores)\n",
    "    term_probs = jax.nn.log_softmax(term_scores)\n",
    "    term_idx = jax.random.categorical(rng_key, term_probs)\n",
    "    log_prob = term_probs[term_idx]#jnp.log(term_probs[term_idx])\n",
    "    return log_prob, term_idx\n",
    "\n",
    "#grad_log_term, term_idx = jax.grad(sample_term, has_aux=True)(term_params, apply_term, rng_key, candidates)#, tac_idx, len(tactic_pool), candidates.shape[1])\n",
    "\n",
    "#function for sampling single argument given previous arguments, \n",
    "def sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, tactic_size, embedding_dim):\n",
    "    hidden, arg_scores = arg_net(arg_params, rng_key, input_, candidates, hidden, tactic_size, embedding_dim)\n",
    "    arg_scores = jnp.ravel(arg_scores)\n",
    "    arg_probs = jax.nn.log_softmax(arg_scores)\n",
    "    arg_idx = jax.random.categorical(rng_key, arg_probs)\n",
    "    log_prob = arg_probs[arg_idx]#jnp.log(arg_probs[arg_idx])\n",
    "    return log_prob, (arg_idx, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "german-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_loss(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg, rng_key, env, encoded_fact_pool, candidate_args):\n",
    "    log_list = []\n",
    "    discounted_reward_list = []\n",
    "    trace = []\n",
    "    gamma = 0.99\n",
    "    for i in range(2):\n",
    "        _, rng_key = jax.random.split(rng_key)\n",
    "        \n",
    "        #print (\"Proof step {} of 50\\n\".format(i+1))\n",
    "        \n",
    "        try:\n",
    "            jax_reps, context_set, fringe_sizes = gather_encoded_content_(env.history, batch_encoder_)\n",
    "            #convert to jax\n",
    "        except:\n",
    "            print (\"Encoder error\")\n",
    "            if len(log_list) > 0:\n",
    "                return sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)])\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        jax_reps = jnp.stack([jnp.array(jax_reps[i][0].cpu()) for i in range(len(jax_reps))])\n",
    "        logs, reward, action = run_iter(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args)\n",
    "\n",
    "        log_list.append(logs)\n",
    "        discounted_reward_list.append(reward * (gamma ** i))\n",
    "        \n",
    "        trace.append((env.history, action))\n",
    "\n",
    "                \n",
    "        #if goal proven\n",
    "        if reward == 5:\n",
    "            print (\"Goal proved in {} steps\".format(i+1))\n",
    "            return sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)]), trace\n",
    "        \n",
    "        #timeout\n",
    "        if i == 49:\n",
    "            discounted_reward_list[-1] = -5.\n",
    "            \n",
    "    loss = sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)])\n",
    "    \n",
    "    return loss, trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "powerful-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iter(context_params, tactic_params, term_params, arg_params, context_net, tactic_net, term_net, arg_net, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args):\n",
    "    \n",
    "    log_context, (fringe_idx, contexts_by_fringe) = sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "    \n",
    "    try:\n",
    "        target_context = contexts_by_fringe[fringe_idx][0]\n",
    "    except:\n",
    "        print (\"error {} {}\".format(contexts_by_fringe), fringe_idx)\n",
    "    target_goal = target_context[\"polished\"][\"goal\"]\n",
    "    target_representation = jax_reps[context_set.index(target_context)]\n",
    "    \n",
    "    log_tac, tac_idx = sample_tactic(tactic_params, tactic_net, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "    \n",
    "    sampled_tac = tactic_pool[tac_idx]\n",
    "    arg_logs = []\n",
    "\n",
    "    tactic = sampled_tac\n",
    "    #for testing\n",
    "    \n",
    "    #sampled_tac = \"Induct_on\"\n",
    "\n",
    "    #if tactic requires no argument\n",
    "    if sampled_tac in no_arg_tactic:\n",
    "        full_tactic = sampled_tac #tactic_pool[tac]\n",
    "\n",
    "\n",
    "    #Induct_on case; use term policy to find which term to induct on \n",
    "    elif sampled_tac in term_tactic:\n",
    "\n",
    "        goal_tokens = target_goal.split()\n",
    "        term_tokens = [[t] for t in set(goal_tokens) if t[0] == \"V\"]\n",
    "        #add conditional if tokens is empty \n",
    "\n",
    "        #now want encodings for terms from AE\n",
    "\n",
    "        term_reps = []\n",
    "\n",
    "        for term in term_tokens:\n",
    "            term_rep, _ = batch_encoder_.encode([term])\n",
    "            #output is bidirectional so concat vectors\n",
    "            term_reps.append(torch.cat(term_rep.split(1), dim=2).squeeze(0))\n",
    "        \n",
    "        #no terms in expression, only contains literals (e.g. induct_on `0`)\n",
    "        if len(term_reps) == 0:\n",
    "            print (\"No variables to induct on for goal {}\".format(target_goal))\n",
    "            #return negative loss for now (positive overall as negative of log prob is positive)\n",
    "            return 1., -1., \"Induct no vars\"\n",
    "            \n",
    "            \n",
    "        # convert to jax\n",
    "        term_reps = jnp.stack([jnp.array(term_reps[i][0].cpu()) for i in range(len(term_reps))])\n",
    "\n",
    "        # now want inputs to term_net to be target_representation (i.e. goal) concatenated with terms\n",
    "        # models the policies conditional dependence of the term given the goal\n",
    "\n",
    "        #stack goal representation for each token\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in term_tokens])\n",
    "\n",
    "        #concat with term encodings to give candidate matrix\n",
    "        candidates = jnp.concatenate([goal_stack, term_reps], 1)\n",
    "\n",
    "        log_term, term_idx = sample_term(term_params, term_net, rng_key, candidates)\n",
    "\n",
    "        sampled_term = term_tokens[term_idx]\n",
    "\n",
    "        tm = sampled_term[0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "    \n",
    "        arg_logs = [log_term]\n",
    "        \n",
    "        if tm:\n",
    "            tactic = \"Induct_on `{}`\".format(tm)\n",
    "        else:\n",
    "            # only to raise an error\n",
    "            tactic = \"Induct_on\"\n",
    "        \n",
    "    #argument tactic\n",
    "    else:\n",
    "        #stack goals to possible arguments to feed into FFN\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in encoded_fact_pool])\n",
    "        candidates = jnp.concatenate([encoded_fact_pool, goal_stack], 1)\n",
    "        \n",
    "        #initial state set as goal\n",
    "        hidden = jnp.expand_dims(target_representation,0)\n",
    "        init_state = hk.LSTMState(hidden,hidden)\n",
    "    \n",
    "        # run through first with tac_idx to initialise state with tactic as c_0\n",
    "        hidden, _ = arg_net(arg_params, rng_key, tac_idx, candidates, init_state, len(tactic_pool), 256)\n",
    "        \n",
    "        ARG_LEN = 5\n",
    "        arg_inds = []\n",
    "        arg_logs = []\n",
    "        input_ = tac_idx\n",
    "        for _ in range(ARG_LEN):\n",
    "            log_arg, (arg_idx, hidden) = sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, len(tactic_pool), 256)\n",
    "            arg_logs.append(log_arg)\n",
    "            arg_inds.append(arg_idx)\n",
    "            input_ = jnp.expand_dims(encoded_fact_pool[arg_idx], 0)\n",
    "        \n",
    "        arg = [candidate_args[i] for i in arg_inds]\n",
    "\n",
    "        tactic = env.assemble_tactic(sampled_tac, arg)\n",
    "        \n",
    "    \n",
    "    \n",
    "    action = (int(fringe_idx), 0, tactic)\n",
    "    #print (\"Action {}:\\n\".format(action))\n",
    "    \n",
    "    try:\n",
    "        reward, done = env.step(action)\n",
    "\n",
    "    except:\n",
    "        print(\"Step exception raised.\")\n",
    "        # print(\"Fringe: {}\".format(env.history))\n",
    "        print(\"Handling: {}\".format(env.handling))\n",
    "        print(\"Using: {}\".format(env.using))\n",
    "        # try again\n",
    "        # counter = env.counter\n",
    "        frequency = env.frequency\n",
    "        env.close()\n",
    "        print(\"Aborting current game ...\")\n",
    "        print(\"Restarting environment ...\")\n",
    "        print(env.goal)\n",
    "        env = HolEnv(env.goal)\n",
    "        flag = False\n",
    "        return \n",
    "        \n",
    "    #print (\"Result: Reward {}\".format(reward))#, env.history[-1]))\n",
    "\n",
    "    \n",
    "    #negative as we want gradient ascent \n",
    "    \n",
    "    logs = (-log_tac - log_context  - sum(arg_logs))\n",
    "\n",
    "    return logs, reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "published-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(goals):\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(11)\n",
    "\n",
    "    init_context, apply_context = hk.transform(policy_networks._context_forward)\n",
    "    apply_context = jax.jit(apply_context)\n",
    "\n",
    "    init_tac, apply_tac = hk.transform(policy_networks._tac_forward)\n",
    "    apply_tac = partial(jax.jit, static_argnums=3)(apply_tac)\n",
    "\n",
    "    init_term, apply_term = hk.transform(policy_networks._term_no_tac_forward)\n",
    "    apply_term = jax.jit(apply_term)\n",
    "\n",
    "    init_arg, apply_arg = hk.transform(policy_networks._arg_forward)\n",
    "    apply_arg = partial(jax.jit, static_argnums=(5,6))(apply_arg)\n",
    "\n",
    "    #initialise these with e.g. random uniform, glorot, He etc. should exist outside function for action selection \n",
    "    context_params = init_context(rng_key, jax.random.normal(rng_key, (1,256)))\n",
    "\n",
    "    tactic_params = init_tac(rng_key, jax.random.normal(rng_key, (1,256)), len(tactic_pool))\n",
    "\n",
    "    #term_policy for now is only considering variables for induction, hence does not need any arguments \n",
    "    term_params = init_term(rng_key, jax.random.normal(rng_key, (1,512)))\n",
    "\n",
    "    hidden = jax.random.normal(rng_key, (1,256))\n",
    "\n",
    "    init_state = hk.LSTMState(hidden, hidden)\n",
    "\n",
    "    arg_params = init_arg(rng_key, jax.random.randint(rng_key, (), 0, len(tactic_pool)), jax.random.normal(rng_key, (1,512)), init_state, len(tactic_pool), 256)\n",
    "\n",
    "        \n",
    "    context_lr = 1e-4\n",
    "    tactic_lr = 1e-4\n",
    "    arg_lr = 1e-4\n",
    "    term_lr = 1e-4\n",
    "\n",
    "    context_optimiser = optax.rmsprop(context_lr)\n",
    "    tactic_optimiser = optax.rmsprop(tactic_lr)\n",
    "    arg_optimiser = optax.rmsprop(arg_lr)\n",
    "    term_optimiser = optax.rmsprop(term_lr)\n",
    "\n",
    "    opt_state_context = context_optimiser.init(context_params)\n",
    "    opt_state_tactic = tactic_optimiser.init(tactic_params)\n",
    "    opt_state_arg = arg_optimiser.init(arg_params)\n",
    "    opt_state_term = term_optimiser.init(term_params)\n",
    "    \n",
    "    proof_dict = {}\n",
    "    \n",
    "\n",
    "    for goal in goals:\n",
    "        g = goal[1]\n",
    "            \n",
    "        env = HolEnv(g)\n",
    "\n",
    "        theories = re.findall(r'C\\$(\\w+)\\$ ', goal[0])\n",
    "        theories = set(theories)\n",
    "        theories = list(theories)\n",
    "\n",
    "        allowed_theories = theories\n",
    "\n",
    "        goal_theory = g\n",
    "\n",
    "        #print (\"Target goal: {}\".format(g))\n",
    "        \n",
    "        try:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            goal_theory = g#database[polished_goal][0] # plain_database[goal][0]\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories and (database[t][0] != goal_theory or int(database[t][2]) < int(database[polished_goal][2])):\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "\n",
    "            env.toggle_simpset(\"diminish\", goal_theory)\n",
    "            #print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "        except:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories:\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "            #print(\"Theorem not found in database.\")\n",
    "\n",
    "        #print (\"Number of candidate facts to use: {}\".format(len(candidate_args)))\n",
    "\n",
    "        encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "        encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids))\n",
    "        \n",
    "        encoded_fact_pool = jnp.array(encoded_fact_pool)\n",
    "        \n",
    "        try:\n",
    "            gradients, trace = jax.grad(episode_loss, argnums=(0,1,2,3), has_aux=True)(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg,  rng_key, env, encoded_fact_pool, candidate_args)\n",
    "        except:\n",
    "            print (\"error\")\n",
    "            continue\n",
    "        \n",
    "        proof_dict[goal] = trace\n",
    "            \n",
    "\n",
    "        #update parameters\n",
    "        context_updates, opt_state_context = context_optimiser.update(gradients[0], opt_state_context)\n",
    "        context_params = optax.apply_updates(context_params, context_updates)\n",
    "\n",
    "        tactic_updates, opt_state_tactic = tactic_optimiser.update(gradients[1], opt_state_tactic)\n",
    "        tactic_params = optax.apply_updates(tactic_params, tactic_updates)\n",
    "\n",
    "        term_updates, opt_state_term = term_optimiser.update(gradients[2], opt_state_term)\n",
    "        term_params = optax.apply_updates(term_params, term_updates)\n",
    "\n",
    "        arg_updates, opt_state_arg = arg_optimiser.update(gradients[3], opt_state_arg)\n",
    "        arg_params = optax.apply_updates(arg_params, arg_updates)\n",
    "        break\n",
    "        #save trace \n",
    "#         save(proof_dict, path_dir + \"/trace\")\n",
    "        \n",
    "#         #save params after each proof attempt\n",
    "#         save(context_params, path_dir + \"/context_params\")\n",
    "#         save(opt_state_context, path_dir+\"/context_state\")\n",
    "#         save(tactic_params, path_dir+\"/tactic_params\")\n",
    "#         save(opt_state_tactic, path_dir+\"/tactic_state\")\n",
    "#         save(term_params, path_dir+\"/term_params\")\n",
    "#         save(opt_state_term, path_dir+\"/term_state\")\n",
    "#         save(arg_params, path_dir+\"/arg_params\")\n",
    "#         save(opt_state_arg, path_dir+\"/arg_state\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupational-dancing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 12:41:10.043147: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing theories...\n",
      "Loading modules...\n",
      "Configuration done.\n",
      "Removing simp lemmas from LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇒ LIST_REL R (l1 ++ l3) (l2 ++ l4)\n"
     ]
    }
   ],
   "source": [
    "TARGET_THEORIES = [\"list\"]\n",
    "GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "train(GOALS[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "electronic-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_encoded_content(history, encoder):\n",
    "    # figure out why this is slower than tests\n",
    "    # figured out: remember to do strip().split()\n",
    "    fringe_sizes = []\n",
    "    contexts = []\n",
    "    reverted = []\n",
    "    for i in history:\n",
    "        c = i[\"content\"]\n",
    "        contexts.extend(c)\n",
    "        fringe_sizes.append(len(c))\n",
    "    for e in contexts:\n",
    "        g = revert_with_polish(e)\n",
    "        reverted.append(g.strip().split())\n",
    "    # print(reverted)\n",
    "    # s1 = timeit.default_timer()\n",
    "    out, sizes = encoder.encode(reverted)\n",
    "    # merge two hidden variables\n",
    "    representations = torch.cat(out.split(1), dim=2).squeeze(0)\n",
    "    # print(representations.shape)\n",
    "    # s2 = timeit.default_timer()    \n",
    "    # print(s2-s1)\n",
    "\n",
    "    return representations, contexts, fringe_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "following-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utp_model\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def run_iteration(goals, mode=\"training\", ARG_LEN=5):\n",
    "    \n",
    "\n",
    "    learning_rate = 1e-5\n",
    "\n",
    "    context_rate = 5e-5\n",
    "    tac_rate = 5e-5\n",
    "    arg_rate = 5e-5\n",
    "    term_rate = 5e-5\n",
    "\n",
    "    gamma = 0.99 # 0.9\n",
    "\n",
    "    # for entropy regularization\n",
    "    trade_off = 1e-2\n",
    "\n",
    "    context_net = utp_model.ContextPolicy()\n",
    "\n",
    "    tac_net = utp_model.TacPolicy(len(tactic_pool))\n",
    "\n",
    "    arg_net = utp_model.ArgPolicy(len(tactic_pool), 256)\n",
    "\n",
    "    term_net = utp_model.TermPolicy(len(tactic_pool), 256)\n",
    "\n",
    "    context_net = context_net.to(device)\n",
    "    tac_net = tac_net.to(device)\n",
    "    arg_net = arg_net.to(device)\n",
    "    term_net = term_net.to(device)\n",
    "\n",
    "    optimizer_context = torch.optim.RMSprop(list(context_net.parameters()), lr=context_rate)\n",
    "\n",
    "    optimizer_tac = torch.optim.RMSprop(list(tac_net.parameters()), lr=tac_rate)\n",
    "\n",
    "    optimizer_arg = torch.optim.RMSprop(list(arg_net.parameters()), lr=arg_rate)\n",
    "\n",
    "    optimizer_term = torch.optim.RMSprop(list(term_net.parameters()), lr=term_rate)\n",
    "\n",
    "\n",
    "    torch.set_grad_enabled(mode==\"training\" or mode==\"subgoals\")\n",
    "    global iteration_counter\n",
    "    # state_pool = []\n",
    "    fringe_pool = []\n",
    "    tac_pool = []\n",
    "    arg_pool = []\n",
    "    reward_pool = []\n",
    "    reward_print = []\n",
    "    action_pool = []\n",
    "    steps = 0\n",
    "    flag = True\n",
    "    replay_flag = False\n",
    "    tac_print = []\n",
    "\n",
    "    induct_arg = []\n",
    "    proved = 0\n",
    "    iteration_rewards = []\n",
    "\n",
    "    for goal in goals:\n",
    "        start_t = time.time()\n",
    "        g = goal[1]\n",
    "            \n",
    "        print (g)\n",
    "        env = HolEnv(g)\n",
    "\n",
    "        theories = re.findall(r'C\\$(\\w+)\\$ ', goal[0])\n",
    "        theories = set(theories)\n",
    "        theories = list(theories)\n",
    "\n",
    "        allowed_theories = theories\n",
    "\n",
    "        goal_theory = g\n",
    "\n",
    "        #print (\"Target goal: {}\".format(g))\n",
    "        \n",
    "        try:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            goal_theory = g#database[polished_goal][0] # plain_database[goal][0]\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories and (database[t][0] != goal_theory or int(database[t][2]) < int(database[polished_goal][2])):\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "\n",
    "            env.toggle_simpset(\"diminish\", goal_theory)\n",
    "            #print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "        except:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories:\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "            #print(\"Theorem not found in database.\")\n",
    "\n",
    "        #print (\"Number of candidate facts to use: {}\".format(len(candidate_args)))\n",
    "\n",
    "        encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids, device=device))\n",
    "\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            # gather all the goals in the history\n",
    "            try:\n",
    "                representations, context_set, fringe_sizes = gather_encoded_content(env.history, batch_encoder_)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            representations = representations.to(device)\n",
    "            context_scores = context_net(representations)\n",
    "            contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "            fringe_scores = []\n",
    "            for s in scores_by_fringe:\n",
    "                # fringe_score = torch.prod(s) # TODO: make it sum\n",
    "                fringe_score = torch.sum(s) # TODO: make it sum\n",
    "                fringe_scores.append(fringe_score)\n",
    "            fringe_scores = torch.stack(fringe_scores)\n",
    "            fringe_probs = F.softmax(fringe_scores, dim=0)\n",
    "            fringe_m = Categorical(fringe_probs)\n",
    "            fringe = fringe_m.sample()\n",
    "            fringe_pool.append(fringe_m.log_prob(fringe))\n",
    "\n",
    "            # take the first context in the chosen fringe for now\n",
    "            try:\n",
    "                target_context = contexts_by_fringe[fringe][0]\n",
    "            except:\n",
    "                print (\"error {} {}\".format(contexts_by_fringe, fringe))\n",
    "\n",
    "           # target_context = contexts_by_fringe[fringe][0]\n",
    "            target_goal = target_context[\"polished\"][\"goal\"]\n",
    "            target_representation = representations[context_set.index(target_context)]\n",
    "            # print(target_representation.shape)\n",
    "            # exit()\n",
    "\n",
    "            # size: (1, max_contexts, max_assumptions+1, max_len)\n",
    "            tac_input = target_representation.unsqueeze(0)\n",
    "            tac_input = tac_input.to(device)\n",
    "\n",
    "            # compute scores of tactics\n",
    "            tac_probs = tac_net(tac_input)\n",
    "            # print(tac_probs)\n",
    "            tac_m = Categorical(tac_probs)\n",
    "            tac = tac_m.sample()\n",
    "            # log directly the log probability\n",
    "            tac_pool.append(tac_m.log_prob(tac))\n",
    "            action_pool.append(tactic_pool[tac])\n",
    "            tac_print.append(tac_probs.detach())\n",
    "            # print(len(fact_pool[0].strip().split()))\n",
    "            # exit()\n",
    "\n",
    "            tac_tensor = tac.to(device)\n",
    "\n",
    "\n",
    "            if tactic_pool[tac] in no_arg_tactic:\n",
    "                tactic = tactic_pool[tac]\n",
    "                arg_probs = []\n",
    "                arg_probs.append(torch.tensor(0))\n",
    "                arg_pool.append(arg_probs)\n",
    "            elif tactic_pool[tac] == \"Induct_on\":\n",
    "                arg_probs = []\n",
    "                candidates = []\n",
    "                # input = torch.cat([target_representation, tac_tensor], dim=1)\n",
    "                tokens = target_goal.split()\n",
    "                tokens = list(dict.fromkeys(tokens))\n",
    "                tokens = [[t] for t in tokens if t[0] == \"V\"]\n",
    "                if tokens:\n",
    "                    # concatenate target_representation to token\n",
    "                    # use seq2seq to compute the representation of a token\n",
    "                    # also we don't need to split an element in tokens because they are singletons\n",
    "                    # but we need to make it a list containing a singleton list, i.e., [['Vl']]\n",
    "\n",
    "                    token_representations, _ = batch_encoder_.encode(tokens)\n",
    "                    # reshaping\n",
    "                    encoded_tokens = torch.cat(token_representations.split(1), dim=2).squeeze(0)\n",
    "                    target_representation_list = [target_representation.unsqueeze(0) for _ in tokens]\n",
    "\n",
    "                    target_representations = torch.cat(target_representation_list)\n",
    "                    # size: (len(tokens), 512)\n",
    "                    candidates = torch.cat([encoded_tokens, target_representations], dim=1)\n",
    "                    candidates = candidates.to(device)\n",
    "\n",
    "                    # concat = [torch.cat([torch.tensor([input_vocab.stoi[i] for _ in range(256)], dtype=torch.float), target_representation]) for i in tokens]\n",
    "\n",
    "                    # candidates = torch.stack(concat)\n",
    "                    # candidates = candidates.to(device)\n",
    "\n",
    "                    scores = term_net(candidates, tac_tensor)\n",
    "                    term_probs = F.softmax(scores, dim=0)\n",
    "                    try:\n",
    "                        term_m = Categorical(term_probs.squeeze(1))\n",
    "                    except:\n",
    "                        print(\"probs: {}\".format(term_probs))                                          \n",
    "                        print(\"candidates: {}\".format(candidates.shape))\n",
    "                        print(\"scores: {}\".format(scores))\n",
    "                        print(\"tokens: {}\".format(tokens))\n",
    "                        exit()\n",
    "                    term = term_m.sample()\n",
    "                    arg_probs.append(term_m.log_prob(term))\n",
    "                    induct_arg.append(tokens[term])                \n",
    "                    tm = tokens[term][0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "                    arg_pool.append(arg_probs)\n",
    "                    if tm:\n",
    "                        tactic = \"Induct_on `{}`\".format(tm)\n",
    "                    else:\n",
    "                        print(\"tm is empty\")\n",
    "                        print(tokens)\n",
    "                        # only to raise an error\n",
    "                        tactic = \"Induct_on\"\n",
    "                else:\n",
    "                    arg_probs.append(torch.tensor(0))\n",
    "                    induct_arg.append(\"No variables\")\n",
    "                    arg_pool.append(arg_probs)\n",
    "                    tactic = \"Induct_on\"\n",
    "            else:\n",
    "                hidden0 = hidden1 = target_representation.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                hidden0 = hidden0.to(device)\n",
    "                hidden1 = hidden1.to(device)\n",
    "\n",
    "                hidden = (hidden0, hidden1)\n",
    "\n",
    "                # concatenate the candidates with hidden states.\n",
    "\n",
    "                hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "                hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "\n",
    "                hiddenl = torch.cat(hiddenl)\n",
    "\n",
    "                # size: (len(fact_pool), 512)\n",
    "                candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "                candidates = candidates.to(device)\n",
    "\n",
    "                input = tac_tensor\n",
    "                # run it once before predicting the first argument\n",
    "                hidden, _ = arg_net(input, candidates, hidden)\n",
    "\n",
    "                # the indices of chosen args\n",
    "                arg_step = []\n",
    "                arg_step_probs = []\n",
    "                if tactic_pool[tac] in thm_tactic:\n",
    "                    arg_len = 1\n",
    "                else:\n",
    "                    arg_len = ARG_LEN\n",
    "\n",
    "                    \n",
    "                for _ in range(arg_len):\n",
    "                    hidden, scores = arg_net(input, candidates, hidden)\n",
    "                    arg_probs = F.softmax(scores, dim=0)\n",
    "                    arg_m = Categorical(arg_probs.squeeze(1))\n",
    "                    arg = arg_m.sample()\n",
    "                    arg_step.append(arg)\n",
    "                    arg_step_probs.append(arg_m.log_prob(arg))\n",
    "\n",
    "                    hidden0 = hidden[0].squeeze().repeat(1, 1, 1)\n",
    "                    hidden1 = hidden[1].squeeze().repeat(1, 1, 1)\n",
    "                    # encoded chosen argument\n",
    "                    input = encoded_fact_pool[arg].unsqueeze(0).unsqueeze(0)\n",
    "                    # print(input.shape)\n",
    "\n",
    "                    # renew candidates                \n",
    "                    hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "                    hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "\n",
    "                    hiddenl = torch.cat(hiddenl)\n",
    "\n",
    "                    # size: (len(fact_pool), 512)\n",
    "                    candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "                    candidates = candidates.to(device)\n",
    "\n",
    "                arg_pool.append(arg_step_probs)\n",
    "\n",
    "                tac = tactic_pool[tac]\n",
    "                arg = [candidate_args[j] for j in arg_step]\n",
    "\n",
    "                tactic = env.assemble_tactic(tac, arg)\n",
    "\n",
    "            action = (fringe.item(), 0, tactic)\n",
    "\n",
    "\n",
    "            print (action)\n",
    "            # reward, done = env.step(action)\n",
    "            try:\n",
    "                # when step is performed, env.history (probably) changes\n",
    "                # if goal_index == 0:\n",
    "                #     raise \"boom\"\n",
    "                reward, done = env.step(action)\n",
    "\n",
    "            except:\n",
    "                print(\"Step exception raised.\")\n",
    "                # print(\"Fringe: {}\".format(env.history))\n",
    "                print(\"Handling: {}\".format(env.handling))\n",
    "                print(\"Using: {}\".format(env.using))\n",
    "                # try again\n",
    "                # counter = env.counter\n",
    "                frequency = env.frequency\n",
    "                env.close()\n",
    "                print(\"Aborting current game ...\")\n",
    "                print(\"Restarting environment ...\")\n",
    "                print(env.goal)\n",
    "                env = HolEnv(env.goal)\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            if t == 49:\n",
    "                reward = -5\n",
    "            # state_pool.append(state)\n",
    "            reward_print.append(reward)\n",
    "            # reward_pool.append(reward+trade_off*entropy)\n",
    "            reward_pool.append(reward)\n",
    "\n",
    "                # pg = ng\n",
    "\n",
    "            steps += 1\n",
    "            total_reward = float(np.sum(reward_print))\n",
    "\n",
    "            if done == True:\n",
    "                print (\"Goal Proved in {} steps\".format(i+1))\n",
    "                break\n",
    "                \n",
    "            if t == 49:\n",
    "                print(\"Failed.\")\n",
    "                print(\"Rewards: {}\".format(reward_print))\n",
    "                # print(\"Rewards: {}\".format(reward_pool))\n",
    "                print(\"Tactics: {}\".format(action_pool))\n",
    "                # print(\"Mean reward: {}\\n\".format(np.mean(reward_pool)))\n",
    "                print(\"Total: {}\".format(total_reward))\n",
    "                iteration_rewards.append(total_reward)\n",
    "\n",
    "        # Update policy\n",
    "        # Discount reward\n",
    "        print(\"Updating parameters ... \")\n",
    "        running_add = 0\n",
    "        for i in reversed(range(steps)):\n",
    "            if reward_pool[i] == 0:\n",
    "                running_add = 0\n",
    "            else:\n",
    "                running_add = running_add * gamma + reward_pool[i]\n",
    "                reward_pool[i] = running_add\n",
    "\n",
    "        optimizer_context.zero_grad()\n",
    "        optimizer_tac.zero_grad()\n",
    "        optimizer_arg.zero_grad()\n",
    "        optimizer_term.zero_grad()\n",
    "\n",
    "        for i in range(steps):\n",
    "            # size : (1,1,4,128)\n",
    "            total_loss = 0\n",
    "\n",
    "            # state = state_pool[i]\n",
    "            reward = reward_pool[i]\n",
    "\n",
    "            fringe_loss = -fringe_pool[i] * (reward)\n",
    "            arg_loss = -torch.sum(torch.stack(arg_pool[i])) * (reward)\n",
    "\n",
    "            tac_loss = -tac_pool[i] * (reward)\n",
    "\n",
    "            # entropy = fringe_pool[i] + torch.sum(torch.stack(arg_pool[i])) + tac_pool[i]\n",
    "\n",
    "            # loss = fringe_loss + tac_loss + arg_loss + trade_off*entropy\n",
    "            loss = fringe_loss + tac_loss + arg_loss\n",
    "            total_loss += loss\n",
    "            #loss.backward()\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        # optimizer.step()\n",
    "\n",
    "        optimizer_context.step()\n",
    "        optimizer_tac.step()\n",
    "        optimizer_arg.step()\n",
    "        optimizer_term.step()\n",
    "\n",
    "        fringe_pool = []\n",
    "        tac_pool = []\n",
    "        arg_pool = []\n",
    "        action_pool = []\n",
    "        reward_pool = []\n",
    "        reward_print = []\n",
    "        steps = 0\n",
    "        elapsed = time.time() - start_t\n",
    "\n",
    "        print (elapsed)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "entertaining-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_THEORIES = [\"list\"]\n",
    "GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#run_iteration(GOALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "conceptual-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "# do stuff\n",
    "elapsed = time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "transparent-craft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.245208740234375e-05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "assured-intelligence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 11.2 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "obvious-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "High level agent class \n",
    "\n",
    "'''\n",
    "class Agent:\n",
    "    def __init__(self, tactic_pool):\n",
    "        self.tactic_pool = tactic_pool    \n",
    "        self.load_encoder()\n",
    "    \n",
    "    def load_agent(self):\n",
    "        pass\n",
    "    \n",
    "    def load_encoder(self):\n",
    "        pass\n",
    "        \n",
    "    def run(self, env, max_steps):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "Vanilla Torch implementation of TacticZero\n",
    "\n",
    "'''\n",
    "class TorchVanilla(Agent):\n",
    "    def __init__(self, tactic_pool):\n",
    "        super().__init__(tactic_pool)\n",
    "\n",
    "        learning_rate = 1e-5\n",
    "\n",
    "        self.context_rate = 5e-5\n",
    "        self.tac_rate = 5e-5\n",
    "        self.arg_rate = 5e-5\n",
    "        self.term_rate = 5e-5\n",
    "\n",
    "        self.gamma = 0.99 # 0.9\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.context_net = utp_model.ContextPolicy().to(self.device)\n",
    "        self.tac_net = utp_model.TacPolicy(len(tactic_pool)).to(self.device)\n",
    "        self.arg_net = utp_model.ArgPolicy(len(tactic_pool), 256).to(self.device)\n",
    "        self.term_net = utp_model.TermPolicy(len(tactic_pool), 256).to(self.device)\n",
    "\n",
    "        self.optimizer_context = torch.optim.RMSprop(list(self.context_net.parameters()), lr=self.context_rate)\n",
    "        self.optimizer_tac = torch.optim.RMSprop(list(self.tac_net.parameters()), lr=self.tac_rate)\n",
    "        self.optimizer_arg = torch.optim.RMSprop(list(self.arg_net.parameters()), lr=self.arg_rate)\n",
    "        self.optimizer_term = torch.optim.RMSprop(list(self.term_net.parameters()), lr=self.term_rate)\n",
    "\n",
    "     \n",
    "    def load_encoder(self):\n",
    "        checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "\n",
    "        checkpoint = Checkpoint.load(checkpoint_path)\n",
    "        seq2seq = checkpoint.model\n",
    "        input_vocab = checkpoint.input_vocab\n",
    "        output_vocab = checkpoint.output_vocab\n",
    "\n",
    "        self.encoder = BatchPredictor(seq2seq, input_vocab, output_vocab)\n",
    "        return \n",
    "\n",
    "        \n",
    "    def run(self, env, encoded_fact_pool, allowed_arguments_ids, candidate_args, max_steps=50):\n",
    "        \n",
    "        \n",
    "        fringe_pool = []\n",
    "        tac_pool = []\n",
    "        arg_pool = []\n",
    "        action_pool = []\n",
    "        reward_pool = []\n",
    "        reward_print = []\n",
    "        tac_print = []\n",
    "        induct_arg = []\n",
    "        proved = 0\n",
    "        iteration_rewards = []\n",
    "        steps = 0        \n",
    "        \n",
    "        start_t = time.time()\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            \n",
    "            # gather all the goals in the history\n",
    "            try:\n",
    "                representations, context_set, fringe_sizes = gather_encoded_content(env.history, self.encoder)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                continue\n",
    "\n",
    "\n",
    "            representations = torch.stack([i.to(self.device) for i in representations])\n",
    "            context_scores = self.context_net(representations)\n",
    "            contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "            fringe_scores = []\n",
    "            for s in scores_by_fringe:\n",
    "                fringe_score = torch.sum(s) \n",
    "                fringe_scores.append(fringe_score)\n",
    "            fringe_scores = torch.stack(fringe_scores)\n",
    "            fringe_probs = F.softmax(fringe_scores, dim=0)\n",
    "            fringe_m = Categorical(fringe_probs)\n",
    "            fringe = fringe_m.sample()\n",
    "            fringe_pool.append(fringe_m.log_prob(fringe))\n",
    "\n",
    "            # take the first context in the chosen fringe for now\n",
    "            try:\n",
    "                target_context = contexts_by_fringe[fringe][0]\n",
    "            except:\n",
    "                print (\"error {} {}\".format(contexts_by_fringe, fringe))\n",
    "\n",
    "            target_goal = target_context[\"polished\"][\"goal\"]\n",
    "            target_representation = representations[context_set.index(target_context)]\n",
    "\n",
    "\n",
    "            tac_input = target_representation#.unsqueeze(0)\n",
    "            tac_input = tac_input.to(self.device)\n",
    "\n",
    "            tac_probs = self.tac_net(tac_input)\n",
    "            tac_m = Categorical(tac_probs)\n",
    "            tac = tac_m.sample()\n",
    "            tac_pool.append(tac_m.log_prob(tac))\n",
    "            action_pool.append(tactic_pool[tac])\n",
    "            tac_print.append(tac_probs.detach())\n",
    "\n",
    "\n",
    "            tac_tensor = tac.to(self.device)\n",
    "\n",
    "            if tactic_pool[tac] in no_arg_tactic:\n",
    "                tactic = tactic_pool[tac]\n",
    "                arg_probs = []\n",
    "                arg_probs.append(torch.tensor(0))\n",
    "                arg_pool.append(arg_probs)\n",
    "                \n",
    "            elif tactic_pool[tac] == \"Induct_on\":\n",
    "                arg_probs = []\n",
    "                candidates = []\n",
    "\n",
    "                tokens = target_goal.split()\n",
    "                tokens = list(dict.fromkeys(tokens))\n",
    "                tokens = [[t] for t in tokens if t[0] == \"V\"]\n",
    "                if tokens:\n",
    "                    token_representations, _ = self.encoder.encode(tokens)\n",
    "                \n",
    "                    encoded_tokens = torch.cat(token_representations.split(1), dim=2).squeeze(0)\n",
    "                \n",
    "                    target_representation_list = [target_representation for _ in tokens]\n",
    "\n",
    "                    target_representations = torch.cat(target_representation_list)\n",
    "\n",
    "                    candidates = torch.cat([encoded_tokens, target_representations], dim=1)\n",
    "                    candidates = candidates.to(self.device)\n",
    "\n",
    "\n",
    "                    scores = self.term_net(candidates, tac_tensor)\n",
    "                    term_probs = F.softmax(scores, dim=0)\n",
    "                    try:\n",
    "                        term_m = Categorical(term_probs.squeeze(1))\n",
    "                    except:\n",
    "                        print(\"probs: {}\".format(term_probs))                                          \n",
    "                        print(\"candidates: {}\".format(candidates.shape))\n",
    "                        print(\"scores: {}\".format(scores))\n",
    "                        print(\"tokens: {}\".format(tokens))\n",
    "                        exit()\n",
    "                    term = term_m.sample()\n",
    "                    arg_probs.append(term_m.log_prob(term))\n",
    "                    induct_arg.append(tokens[term])                \n",
    "                    tm = tokens[term][0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "                    arg_pool.append(arg_probs)\n",
    "                    if tm:\n",
    "                        tactic = \"Induct_on `{}`\".format(tm)\n",
    "                    else:\n",
    "                        print(\"tm is empty\")\n",
    "                        print(tokens)\n",
    "                        # only to raise an error\n",
    "                        tactic = \"Induct_on\"\n",
    "                else:\n",
    "                    arg_probs.append(torch.tensor(0))\n",
    "                    induct_arg.append(\"No variables\")\n",
    "                    arg_pool.append(arg_probs)\n",
    "                    tactic = \"Induct_on\"\n",
    "            else:\n",
    "                hidden0 = hidden1 = target_representation#.unsqueeze(0).unsqueeze(0)\n",
    "                hidden0 = hidden0.to(self.device)\n",
    "                hidden1 = hidden1.to(self.device)\n",
    "\n",
    "                hidden = (hidden0, hidden1)\n",
    "                \n",
    "                # concatenate the candidates with hidden states.\n",
    "\n",
    "                hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "                hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "                \n",
    "                \n",
    "                hiddenl = torch.cat(hiddenl)\n",
    "\n",
    "                # size: (len(fact_pool), 512)\n",
    "                candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "                candidates = candidates.to(self.device)\n",
    "                            \n",
    "                input = tac_tensor\n",
    "                # run it once before predicting the first argument\n",
    "                hidden, _ = self.arg_net(input, candidates, hidden)\n",
    "\n",
    "                # the indices of chosen args\n",
    "                arg_step = []\n",
    "                arg_step_probs = []\n",
    "                if tactic_pool[tac] in thm_tactic:\n",
    "                    arg_len = 1\n",
    "                else:\n",
    "                    arg_len = 5#ARG_LEN\n",
    "\n",
    "\n",
    "                for _ in range(arg_len):\n",
    "                    hidden, scores = self.arg_net(input, candidates, hidden)\n",
    "                    arg_probs = F.softmax(scores, dim=0)\n",
    "                    arg_m = Categorical(arg_probs.squeeze(1))\n",
    "                    arg = arg_m.sample()\n",
    "                    arg_step.append(arg)\n",
    "                    arg_step_probs.append(arg_m.log_prob(arg))\n",
    "\n",
    "                    hidden0 = hidden[0].squeeze().repeat(1, 1, 1)\n",
    "                    hidden1 = hidden[1].squeeze().repeat(1, 1, 1)\n",
    "                    \n",
    "                    # encoded chosen argument\n",
    "                    input = encoded_fact_pool[arg].unsqueeze(0)#.unsqueeze(0)\n",
    "\n",
    "                    # renew candidates                \n",
    "                    hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "                    hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "\n",
    "                    hiddenl = torch.cat(hiddenl)\n",
    "                    #appends both hidden and cell states (when paper only does hidden?)\n",
    "                    candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "                    candidates = candidates.to(self.device)\n",
    "\n",
    "                arg_pool.append(arg_step_probs)\n",
    "\n",
    "                tac = tactic_pool[tac]\n",
    "                arg = [candidate_args[j] for j in arg_step]\n",
    "\n",
    "                tactic = env.assemble_tactic(tac, arg)\n",
    "\n",
    "            action = (fringe.item(), 0, tactic)\n",
    "\n",
    "\n",
    "            #print (action)\n",
    "            # reward, done = env.step(action)\n",
    "            try:\n",
    "                reward, done = env.step(action)\n",
    "\n",
    "            except:\n",
    "                print(\"Step exception raised.\")\n",
    "                break\n",
    "                # print(\"Fringe: {}\".format(env.history))\n",
    "                print(\"Handling: {}\".format(env.handling))\n",
    "                print(\"Using: {}\".format(env.using))\n",
    "                # try again\n",
    "                # counter = env.counter\n",
    "                frequency = env.frequency\n",
    "                env.close()\n",
    "                print(\"Aborting current game ...\")\n",
    "                print(\"Restarting environment ...\")\n",
    "                print(env.goal)\n",
    "                env = HolEnv(env.goal)\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            if t == 49:\n",
    "                reward = -5\n",
    "                \n",
    "            reward_print.append(reward)\n",
    "            reward_pool.append(reward)\n",
    "\n",
    "            steps += 1\n",
    "            total_reward = float(np.sum(reward_print))\n",
    "\n",
    "            if done == True:\n",
    "                print (\"Goal Proved in {} steps\".format(t+1))\n",
    "                break\n",
    "\n",
    "            if t == 49:\n",
    "                print(\"Failed.\")\n",
    "                #print(\"Rewards: {}\".format(reward_print))\n",
    "                # print(\"Rewards: {}\".format(reward_pool))\n",
    "                #print(\"Tactics: {}\".format(action_pool))\n",
    "                # print(\"Mean reward: {}\\n\".format(np.mean(reward_pool)))\n",
    "                #print(\"Total: {}\".format(total_reward))\n",
    "                iteration_rewards.append(total_reward)\n",
    "\n",
    "        \n",
    "        self.update_params(reward_pool, fringe_pool, arg_pool, tac_pool, steps)\n",
    "        \n",
    "        elapsed = time.time() - start_t\n",
    "\n",
    "        #print (elapsed)\n",
    "\n",
    "        return\n",
    "\n",
    "    def update_params(self, reward_pool, fringe_pool, arg_pool, tac_pool, step_count):\n",
    "        # Update policy\n",
    "        # Discount reward\n",
    "        print(\"Updating parameters ... \")\n",
    "        running_add = 0\n",
    "        for i in reversed(range(step_count)):\n",
    "            if reward_pool[i] == 0:\n",
    "                running_add = 0\n",
    "            else:\n",
    "                running_add = running_add * self.gamma + reward_pool[i]\n",
    "                reward_pool[i] = running_add\n",
    "\n",
    "        self.optimizer_context.zero_grad()\n",
    "        self.optimizer_tac.zero_grad()\n",
    "        self.optimizer_arg.zero_grad()\n",
    "        self.optimizer_term.zero_grad()\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(step_count):\n",
    "            reward = reward_pool[i]\n",
    "            \n",
    "            fringe_loss = -fringe_pool[i] * (reward)\n",
    "            arg_loss = -torch.sum(torch.stack(arg_pool[i])) * (reward)\n",
    "            tac_loss = -tac_pool[i] * (reward)\n",
    "            \n",
    "            loss = fringe_loss + tac_loss + arg_loss\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        self.optimizer_context.step()\n",
    "        self.optimizer_tac.step()\n",
    "        self.optimizer_arg.step()\n",
    "        self.optimizer_term.step()\n",
    "        \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "declared-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, agent, goals, db_dir, encoded_db_dir, num_iterations):\n",
    "        self.agent = agent\n",
    "        self.goals = goals\n",
    "        self.num_iterations = num_iterations\n",
    "        self.load_db(db_dir)\n",
    "        self.load_encoded_db(encoded_db_dir)\n",
    "        \n",
    "    def train(self):\n",
    "        env = HolEnv(\"T\")\n",
    "        for iteration in range(self.num_iterations):\n",
    "            for i, goal in enumerate(self.goals):\n",
    "                try:\n",
    "                    env.reset(goal[1])\n",
    "                except Exception as e:\n",
    "                    print (e)\n",
    "                    continue\n",
    "                 \n",
    "                encoded_fact_pool, allowed_arguments_ids, candidate_args = self.gen_fact_pool(env, goal)\n",
    "                self.agent.run(env, encoded_fact_pool, allowed_arguments_ids, candidate_args, max_steps=50)\n",
    "\n",
    "    def load_encoded_db(self, encoded_db_dir):\n",
    "        self.encoded_database = torch.load(encoded_db_dir)\n",
    "\n",
    "    def load_db(self, db_dir):\n",
    "        with open(db_dir) as f:\n",
    "            self.database = json.load(f)\n",
    "\n",
    "    def gen_fact_pool(self, env, goal):\n",
    "\n",
    "        allowed_theories = list(set(re.findall(r'C\\$(\\w+)\\$ ', goal[0])))\n",
    "        \n",
    "        goal_theory = goal[1]\n",
    "    \n",
    "        polished_goal = env.fringe[\"content\"][0][\"polished\"][\"goal\"]\n",
    "        \n",
    "        try:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(self.database):\n",
    "                if self.database[t][0] in allowed_theories and (self.database[t][0] != goal_theory or int(self.database[t][2]) < int(self.database[polished_goal][2])):\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "\n",
    "            env.toggle_simpset(\"diminish\", goal_theory)\n",
    "            #print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "        except:\n",
    "            allowed_arsguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(self.database):\n",
    "                if self.database[t][0] in allowed_theories:\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "            #print(\"Theorem not found in database.\")\n",
    "            print (\"sss\")\n",
    "\n",
    "        #print (\"Number of candidate facts to use: {}\".format(len(candidate_args)))\n",
    "        encoded_fact_pool = torch.index_select(self.encoded_database, 0, torch.tensor(allowed_arguments_ids))\n",
    "        \n",
    "        return encoded_fact_pool, allowed_arguments_ids, candidate_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "hollywood-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "visible-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TorchVanilla(tactic_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "balanced-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = GOALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "composite-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(test, goals,\"include_probability.json\", 'encoded_include_probability.pt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-poker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing theories...\n",
      "Loading modules...\n",
      "Configuration done.\n",
      "Initialization done. Main goal is:\n",
      "LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇒ LIST_REL R (l1 ++ l3) (l2 ++ l4).\n",
      "Removing simp lemmas from LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇒ LIST_REL R (l1 ++ l3) (l2 ++ l4)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇔ LIST_REL R (l1 ++ l3) (l2 ++ l4) ∧ LENGTH l1 = LENGTH l2 ∧ LENGTH l3 = LENGTH l4.\n",
      "Removing simp lemmas from LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇔ LIST_REL R (l1 ++ l3) (l2 ++ l4) ∧ LENGTH l1 = LENGTH l2 ∧ LENGTH l3 = LENGTH l4\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(ls :α list) (n :num). ALL_DISTINCT ls ⇒ ALL_DISTINCT (DROP n ls).\n",
      "Removing simp lemmas from ∀(ls :α list) (n :num). ALL_DISTINCT ls ⇒ ALL_DISTINCT (DROP n ls)\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :(α # β) list). ZIP (UNZIP l) = l.\n",
      "Removing simp lemmas from ∀(l :(α # β) list). ZIP (UNZIP l) = l\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :β list) (f1 :α -> γ) (f2 :β -> δ). LENGTH l1 = LENGTH l2 ⇒ ZIP (MAP f1 l1,l2) = MAP (λ(p :α # β). (f1 (FST p),SND p)) (ZIP (l1,l2)) ∧ ZIP (l1,MAP f2 l2) = MAP (λ(p :α # β). (FST p,f2 (SND p))) (ZIP (l1,l2)).\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :β list) (f1 :α -> γ) (f2 :β -> δ). LENGTH l1 = LENGTH l2 ⇒ ZIP (MAP f1 l1,l2) = MAP (λ(p :α # β). (f1 (FST p),SND p)) (ZIP (l1,l2)) ∧ ZIP (l1,MAP f2 l2) = MAP (λ(p :α # β). (FST p,f2 (SND p))) (ZIP (l1,l2))\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (f :num -> β) (n :num). LENGTH l = n ⇒ ZIP (l,GENLIST f n) = GENLIST (λ(x :num). (EL x l,f x)) n.\n",
      "Removing simp lemmas from ∀(l :α list) (f :num -> β) (n :num). LENGTH l = n ⇒ ZIP (l,GENLIST f n) = GENLIST (λ(x :num). (EL x l,f x)) n\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 48 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :β list). LENGTH l1 = LENGTH l2 ⇒ (ZIP (l1,l2) = ([] :(α # β) list) ⇔ l1 = ([] :α list) ∧ l2 = ([] :β list)).\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :β list). LENGTH l1 = LENGTH l2 ⇒ (ZIP (l1,l2) = ([] :(α # β) list) ⇔ l1 = ([] :α list) ∧ l2 = ([] :β list))\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(a :α list) (b :β list) (n :num). n ≤ LENGTH a ∧ LENGTH a = LENGTH b ⇒ ZIP (DROP n a,DROP n b) = DROP n (ZIP (a,b)).\n",
      "Removing simp lemmas from ∀(a :α list) (b :β list) (n :num). n ≤ LENGTH a ∧ LENGTH a = LENGTH b ⇒ ZIP (DROP n a,DROP n b) = DROP n (ZIP (a,b))\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "ZIP (([] :γ list),([] :δ list)) = ([] :(γ # δ) list) ∧ ∀(x1 :α) (l1 :α list) (x2 :β) (l2 :β list). ZIP (x1::l1,x2::l2) = (x1,x2)::ZIP (l1,l2).\n",
      "Removing simp lemmas from ZIP (([] :γ list),([] :δ list)) = ([] :(γ # δ) list) ∧ ∀(x1 :α) (l1 :α list) (x2 :β) (l2 :β list). ZIP (x1::l1,x2::l2) = (x1,x2)::ZIP (l1,l2)\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "WF (R :α -> α -> bool) ⇒ ∀(l :num) (s :α list -> bool). (∀(d :α list). d ∈ s ⇒ LENGTH d = l) ∧ (∃(a :α list). a ∈ s) ⇒ ∃(b :α list). b ∈ s ∧ ∀(c :α list). SHORTLEX R c b ⇒ c ∉ s.\n",
      "Removing simp lemmas from WF (R :α -> α -> bool) ⇒ ∀(l :num) (s :α list -> bool). (∀(d :α list). d ∈ s ⇒ LENGTH d = l) ∧ (∃(a :α list). a ∈ s) ⇒ ∃(b :α list). b ∈ s ∧ ∀(c :α list). SHORTLEX R c b ⇒ c ∉ s\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "WF (R :α -> α -> bool) ⇒ WF (SHORTLEX R).\n",
      "Removing simp lemmas from WF (R :α -> α -> bool) ⇒ WF (SHORTLEX R)\n",
      "same action\n",
      "Goal Proved in 6 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "WF (λ(L1 :α list) (L2 :α list). ∃(h :α). L2 = h::L1).\n",
      "Removing simp lemmas from WF (λ(L1 :α list) (L2 :α list). ∃(h :α). L2 = h::L1)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :β list). LENGTH l1 = LENGTH l2 ⇒ UNZIP (ZIP (l1,l2)) = (l1,l2).\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :β list). LENGTH l1 = LENGTH l2 ⇒ UNZIP (ZIP (l1,l2)) = (l1,l2)\n",
      "Goal Proved in 3 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "UNZIP ([] :(α # β) list) = (([] :α list),([] :β list)) ∧ UNZIP (((x :α),(y :β))::(t :(α # β) list)) = (let ((L1 :α list),(L2 :β list)) = UNZIP t in (x::L1,y::L2)).\n",
      "Removing simp lemmas from UNZIP ([] :(α # β) list) = (([] :α list),([] :β list)) ∧ UNZIP (((x :α),(y :β))::(t :(α # β) list)) = (let ((L1 :α list),(L2 :β list)) = UNZIP t in (x::L1,y::L2))\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(L :(α # β) list). UNZIP L = (MAP (FST :α # β -> α) L,MAP (SND :α # β -> β) L).\n",
      "Removing simp lemmas from ∀(L :(α # β) list). UNZIP L = (MAP (FST :α # β -> α) L,MAP (SND :α # β -> β) L)\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 25 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(e :α) (L :α list). UNIQUE e L ⇔ LENGTH (FILTER ($= e) L) = (1 :num).\n",
      "Removing simp lemmas from ∀(e :α) (L :α list). UNIQUE e L ⇔ LENGTH (FILTER ($= e) L) = (1 :num)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(e :α) (L :α list). UNIQUE e L ⇔ FILTER ($= e) L = [e].\n",
      "Removing simp lemmas from ∀(e :α) (L :α list). UNIQUE e L ⇔ FILTER ($= e) L = [e]\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list). set l1 ∪ set l2 = set (l1 ++ l2).\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list). set l1 ∪ set l2 = set (l1 ++ l2)\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(f :num -> α) (n :num). TL (GENLIST f (SUC n)) = GENLIST (f ∘ SUC) n.\n",
      "Removing simp lemmas from ∀(f :num -> α) (n :num). TL (GENLIST f (SUC n)) = GENLIST (f ∘ SUC) n\n",
      "same action\n",
      "Goal Proved in 20 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(h :α) (t :α list). TL (h::t) = t.\n",
      "Removing simp lemmas from ∀(h :α) (t :α list). TL (h::t) = t\n",
      "same action\n",
      "Goal Proved in 6 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(m :num) (n :num). TAKE n (TAKE m (l :α list)) = TAKE (MIN n m) l.\n",
      "Removing simp lemmas from ∀(m :num) (n :num). TAKE n (TAKE m (l :α list)) = TAKE (MIN n m) l\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(n :num) (m :num) (l :α list). TAKE ((n + m) :num) l = TAKE n l ++ TAKE m (DROP n l).\n",
      "Removing simp lemmas from ∀(n :num) (m :num) (l :α list). TAKE ((n + m) :num) l = TAKE n l ++ TAKE m (DROP n l)\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "TAKE (n :num) (l :α list) = splitAtPki ((K :bool -> α -> bool) ∘ $= n) (K :α list -> α list -> α list) l.\n",
      "Removing simp lemmas from TAKE (n :num) (l :α list) = splitAtPki ((K :bool -> α -> bool) ∘ $= n) (K :α list -> α list -> α list) l\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 23 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(n :num). TAKE n ([] :α list) = ([] :α list).\n",
      "Removing simp lemmas from ∀(n :num). TAKE n ([] :α list) = ([] :α list)\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (n :num). LENGTH l ≤ n ⇒ TAKE n l = l.\n",
      "Removing simp lemmas from ∀(l :α list) (n :num). LENGTH l ≤ n ⇒ TAKE n l = l\n",
      "Goal Proved in 7 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (m :num). m = LENGTH l ⇒ TAKE m l = l.\n",
      "Removing simp lemmas from ∀(l :α list) (m :num). m = LENGTH l ⇒ TAKE m l = l\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done. Main goal is:\n",
      "∀(l :α list). TAKE (LENGTH l) l = l.\n",
      "Removing simp lemmas from ∀(l :α list). TAKE (LENGTH l) l = l\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "TAKE (n :num) (GENLIST (f :num -> α) (m :num)) = GENLIST f (MIN n m).\n",
      "Removing simp lemmas from TAKE (n :num) (GENLIST (f :num -> α) (m :num)) = GENLIST f (MIN n m)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "TAKE (n :num) (l :α list) = ([] :α list) ⇔ n = (0 :num) ∨ l = ([] :α list).\n",
      "Removing simp lemmas from TAKE (n :num) (l :α list) = ([] :α list) ⇔ n = (0 :num) ∨ l = ([] :α list)\n",
      "Goal Proved in 4 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(xs :α list) (k :num) (n :num). TAKE k (DROP n xs) = DROP n (TAKE ((k + n) :num) xs).\n",
      "Removing simp lemmas from ∀(xs :α list) (k :num) (n :num). TAKE k (DROP n xs) = DROP n (TAKE ((k + n) :num) xs)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(n :num) (l :α list). TAKE n l ++ DROP n l = l.\n",
      "Removing simp lemmas from ∀(n :num) (l :α list). TAKE n l ++ DROP n l = l\n",
      "Goal Proved in 5 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(0 :num) < (n :num) ⇒ TAKE n ((x :α)::(xs :α list)) = x::TAKE (n − (1 :num)) xs.\n",
      "Removing simp lemmas from (0 :num) < (n :num) ⇒ TAKE n ((x :α)::(xs :α list)) = x::TAKE (n − (1 :num)) xs\n",
      "Goal Proved in 4 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(∀(l :α list). TAKE (0 :num) l = ([] :α list)) ∧ (∀(n :num). TAKE (NUMERAL (BIT1 n)) ([] :β list) = ([] :β list)) ∧ (∀(n :num). TAKE (NUMERAL (BIT2 n)) ([] :β list) = ([] :β list)) ∧ (∀(n :num) (h :γ) (t :γ list). TAKE (NUMERAL (BIT1 n)) (h::t) = h::TAKE (NUMERAL (BIT1 n) − (1 :num)) t) ∧ ∀(n :num) (h :γ) (t :γ list). TAKE (NUMERAL (BIT2 n)) (h::t) = h::TAKE (NUMERAL (BIT1 n)) t.\n",
      "Removing simp lemmas from (∀(l :α list). TAKE (0 :num) l = ([] :α list)) ∧ (∀(n :num). TAKE (NUMERAL (BIT1 n)) ([] :β list) = ([] :β list)) ∧ (∀(n :num). TAKE (NUMERAL (BIT2 n)) ([] :β list) = ([] :β list)) ∧ (∀(n :num) (h :γ) (t :γ list). TAKE (NUMERAL (BIT1 n)) (h::t) = h::TAKE (NUMERAL (BIT1 n) − (1 :num)) t) ∧ ∀(n :num) (h :γ) (t :γ list). TAKE (NUMERAL (BIT2 n)) (h::t) = h::TAKE (NUMERAL (BIT1 n)) t\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(n :num). LENGTH (l1 :α list) < n ⇒ TAKE n (l1 ++ (l2 :α list)) = l1 ++ TAKE (n − LENGTH l1) l2.\n",
      "Removing simp lemmas from ∀(n :num). LENGTH (l1 :α list) < n ⇒ TAKE n (l1 ++ (l2 :α list)) = l1 ++ TAKE (n − LENGTH l1) l2\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(n :num). n ≤ LENGTH (l1 :α list) ⇒ TAKE n (l1 ++ (l2 :α list)) = TAKE n l1.\n",
      "Removing simp lemmas from ∀(n :num). n ≤ LENGTH (l1 :α list) ⇒ TAKE n (l1 ++ (l2 :α list)) = TAKE n l1\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "TAKE (0 :num) (l :α list) = ([] :α list).\n",
      "Removing simp lemmas from TAKE (0 :num) (l :α list) = ([] :α list)\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(n :num) (l :α list). n < LENGTH l ⇒ TAKE (1 :num) (DROP n l) = [EL n l].\n",
      "Removing simp lemmas from ∀(n :num) (l :α list). n < LENGTH l ⇒ TAKE (1 :num) (DROP n l) = [EL n l]\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list). l ≠ ([] :α list) ⇒ TAKE (1 :num) l = [EL (0 :num) l].\n",
      "Removing simp lemmas from ∀(l :α list). l ≠ ([] :α list) ⇒ TAKE (1 :num) l = [EL (0 :num) l]\n",
      "Goal Proved in 28 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list). REVERSE l1 = l2 ⇔ l1 = REVERSE l2.\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list). REVERSE l1 = l2 ⇔ l1 = REVERSE l2\n",
      "Goal Proved in 15 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list). l1 = REVERSE l2 ⇔ l2 = REVERSE l1.\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list). l1 = REVERSE l2 ⇔ l2 = REVERSE l1\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 35 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(L :num list). SUM L = SUM_ACC L (0 :num).\n",
      "Removing simp lemmas from ∀(L :num list). SUM L = SUM_ACC L (0 :num)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(x :num) (l :num list). SUM (SNOC x l) = ((SUM l + x) :num).\n",
      "Removing simp lemmas from ∀(x :num) (l :num list). SUM (SNOC x l) = ((SUM l + x) :num)\n",
      "Goal Proved in 24 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(ls1 :α list) (ls2 :β list). LENGTH ls1 = LENGTH ls2 ∧ (∀(x :α) (y :β). (f :α # β -> num) (x,y) = (((g :α -> num) x + (h :β -> num) y) :num)) ⇒ SUM (MAP f (ZIP (ls1,ls2))) = ((SUM (MAP g ls1) + SUM (MAP h ls2)) :num).\n",
      "Removing simp lemmas from ∀(ls1 :α list) (ls2 :β list). LENGTH ls1 = LENGTH ls2 ∧ (∀(x :α) (y :β). (f :α # β -> num) (x,y) = (((g :α -> num) x + (h :β -> num) y) :num)) ⇒ SUM (MAP f (ZIP (ls1,ls2))) = ((SUM (MAP g ls1) + SUM (MAP h ls2)) :num)\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(f :α -> num) (g :α -> num) (ls :α list). SUM (MAP (λ(x :α). ((f x + g x) :num)) ls) = ((SUM (MAP f ls) + SUM (MAP g ls)) :num).\n",
      "Removing simp lemmas from ∀(f :α -> num) (g :α -> num) (ls :α list). SUM (MAP (λ(x :α). ((f x + g x) :num)) ls) = ((SUM (MAP f ls) + SUM (MAP g ls)) :num)\n",
      "Goal Proved in 13 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(f :α -> num) (x :α) (ls :α list). MEM x ls ⇒ f x ≤ SUM (MAP f ls).\n",
      "Removing simp lemmas from ∀(f :α -> num) (x :α) (ls :α list). MEM x ls ⇒ f x ≤ SUM (MAP f ls)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(ls :α list). SUM (MAP (f :α -> num) ls) = FOLDL (λ(a :num) (e :α). ((a + f e) :num)) (0 :num) ls.\n",
      "Removing simp lemmas from ∀(ls :α list). SUM (MAP (f :α -> num) ls) = FOLDL (λ(a :num) (e :α). ((a + f e) :num)) (0 :num) ls\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(ls :α list). ∑ (f :α -> num) (set ls) ≤ SUM (MAP f ls).\n",
      "Removing simp lemmas from ∀(ls :α list). ∑ (f :α -> num) (set ls) ≤ SUM (MAP f ls)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "FINITE (s :α -> bool) ⇒ ∑ (f :α -> num) s = SUM (MAP f (SET_TO_LIST s)).\n",
      "Removing simp lemmas from FINITE (s :α -> bool) ⇒ ∑ (f :α -> num) s = SUM (MAP f (SET_TO_LIST s))\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(ls :num list). SUM ls = (0 :num) ⇔ ∀(x :num). MEM x ls ⇒ x = (0 :num).\n",
      "Removing simp lemmas from ∀(ls :num list). SUM ls = (0 :num) ⇔ ∀(x :num). MEM x ls ⇒ x = (0 :num)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :num list) (l2 :num list). SUM (l1 ++ l2) = ((SUM l1 + SUM l2) :num).\n",
      "Removing simp lemmas from ∀(l1 :num list) (l2 :num list). SUM (l1 ++ l2) = ((SUM l1 + SUM l2) :num)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 19 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(L :num list) (n :num). SUM_ACC L n = ((SUM L + n) :num).\n",
      "Removing simp lemmas from ∀(L :num list) (n :num). SUM_ACC L n = ((SUM L + n) :num)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(f :β -> α) (splitAtPki (P :num -> γ -> bool) (k :γ list -> γ list -> β) (l :γ list)) = splitAtPki P (($o f :(γ list -> β) -> γ list -> α) ∘ k) l.\n",
      "Removing simp lemmas from (f :β -> α) (splitAtPki (P :num -> γ -> bool) (k :γ list -> γ list -> β) (l :γ list)) = splitAtPki P (($o f :(γ list -> β) -> γ list -> α) ∘ k) l\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "splitAtPki (P :num -> β -> bool) (k :β list -> β list -> α) (MAP (f :γ -> β) (l :γ list)) = splitAtPki (flip (($o :(β -> bool) -> (γ -> β) -> γ -> bool) ∘ P) f) (flip (($o :(β list -> α) -> (γ list -> β list) -> γ list -> α) ∘ k ∘ MAP f) (MAP f)) l.\n",
      "Removing simp lemmas from splitAtPki (P :num -> β -> bool) (k :β list -> β list -> α) (MAP (f :γ -> β) (l :γ list)) = splitAtPki (flip (($o :(β -> bool) -> (γ -> β) -> γ -> bool) ∘ P) f) (flip (($o :(β list -> α) -> (γ list -> β list) -> γ list -> α) ∘ k ∘ MAP f) (MAP f)) l\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "splitAtPki (P :num -> β -> bool) (k :β list -> β list -> α) (l :β list) = case OLEAST(i :num). i < LENGTH l ∧ P i (EL i l) of (NONE :num option) => k l ([] :β list) | SOME i => k (TAKE i l) (DROP i l).\n",
      "Removing simp lemmas from splitAtPki (P :num -> β -> bool) (k :β list -> β list -> α) (l :β list) = case OLEAST(i :num). i < LENGTH l ∧ P i (EL i l) of (NONE :num option) => k l ([] :β list) | SOME i => k (TAKE i l) (DROP i l)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(∀(i :num). i < LENGTH (l :α list) ⇒ ((P1 :num -> α -> bool) i (EL i l) ⇔ (P2 :num -> α -> bool) i (EL i l))) ⇒ splitAtPki P1 (k :α list -> α list -> β) l = splitAtPki P2 k l.\n",
      "Removing simp lemmas from (∀(i :num). i < LENGTH (l :α list) ⇒ ((P1 :num -> α -> bool) i (EL i l) ⇔ (P2 :num -> α -> bool) i (EL i l))) ⇒ splitAtPki P1 (k :α list -> α list -> β) l = splitAtPki P2 k l\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list) (P :num -> α -> bool) (k :α list -> α list -> β). EVERYi (λ(i :num). $¬ ∘ P i) l1 ∧ ((0 :num) < LENGTH l2 ⇒ P (LENGTH l1) (HD l2)) ⇒ splitAtPki P k (l1 ++ l2) = k l1 l2.\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list) (P :num -> α -> bool) (k :α list -> α list -> β). EVERYi (λ(i :num). $¬ ∘ P i) l1 ∧ ((0 :num) < LENGTH l2 ⇒ P (LENGTH l1) (HD l2)) ⇒ splitAtPki P k (l1 ++ l2) = k l1 l2\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(P :α list -> bool). P ([] :α list) ∧ (∀(l :α list). P l ⇒ ∀(x :α). P (SNOC x l)) ⇒ ∀(l :α list). P l.\n",
      "Removing simp lemmas from ∀(P :α list -> bool). P ([] :α list) ∧ (∀(l :α list). P l ⇒ ∀(x :α). P (SNOC x l)) ⇒ ∀(l :α list). P l\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 42 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(ll :α list). ll = ([] :α list) ∨ ∃(x :α) (l :α list). ll = SNOC x l.\n",
      "Removing simp lemmas from ∀(ll :α list). ll = ([] :α list) ∨ ∃(x :α) (l :α list). ll = SNOC x l\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(e :β) (f :α -> α list -> β -> β). ∃(fn :α list -> β). fn ([] :α list) = e ∧ ∀(x :α) (l :α list). fn (SNOC x l) = f x l (fn l).\n",
      "Removing simp lemmas from ∀(e :β) (f :α -> α list -> β -> β). ∃(fn :α list -> β). fn ([] :α list) = e ∧ ∀(x :α) (l :α list). fn (SNOC x l) = f x l (fn l)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(x :α) (l :α list). SNOC x l = l ++ [x].\n",
      "Removing simp lemmas from ∀(x :α) (l :α list). SNOC x l = l ++ [x]\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(x :α) (y :α list) (a :α) (b :α list). SNOC x y = SNOC a b ⇔ x = a ∧ y = b.\n",
      "Removing simp lemmas from ∀(x :α) (y :α list) (a :α) (b :α list). SNOC x y = SNOC a b ⇔ x = a ∧ y = b\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(([(f :β -> α)] <*> [(x :β)]) :α list) = [f x].\n",
      "Removing simp lemmas from (([(f :β -> α)] <*> [(x :β)]) :α list) = [f x]\n",
      "Goal Proved in 5 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "LIST_BIND (l :α list) (λ(x :α). [x]) = l.\n",
      "Removing simp lemmas from LIST_BIND (l :α list) (λ(x :α). [x]) = l\n",
      "same action\n",
      "Goal Proved in 13 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "LIST_BIND [(x :β)] (f :β -> α list) = f x.\n",
      "Removing simp lemmas from LIST_BIND [(x :β)] (f :β -> α list) = f x\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(((fs :(β -> α) list) <*> [(x :β)]) :α list) = (([(λ(f :β -> α). f x)] <*> fs) :α list).\n",
      "Removing simp lemmas from (((fs :(β -> α) list) <*> [(x :β)]) :α list) = (([(λ(f :β -> α). f x)] <*> fs) :α list)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(([(f :β -> α)] <*> (l :β list)) :α list) = MAP f l.\n",
      "Removing simp lemmas from (([(f :β -> α)] <*> (l :β list)) :α list) = MAP f l\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "([HD (xs :α list)] = xs ⇔ LENGTH xs = (1 :num)) ∧ (xs = [HD xs] ⇔ LENGTH xs = (1 :num)).\n",
      "Removing simp lemmas from ([HD (xs :α list)] = xs ⇔ LENGTH xs = (1 :num)) ∧ (xs = [HD xs] ⇔ LENGTH xs = (1 :num))\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "transitive (R :α -> α -> bool) ⇒ transitive (SHORTLEX R).\n",
      "Removing simp lemmas from transitive (R :α -> α -> bool) ⇒ transitive (SHORTLEX R)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "total (RC (R :α -> α -> bool)) ⇒ total (RC (SHORTLEX R)).\n",
      "Removing simp lemmas from total (RC (R :α -> α -> bool)) ⇒ total (RC (SHORTLEX R))\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(¬SHORTLEX (R :α -> α -> bool) ([] :α list) ([] :α list) ∧ ¬SHORTLEX R ((h1 :α)::(t1 :α list)) ([] :α list)) ∧ SHORTLEX R ([] :α list) ((h2 :α)::(t2 :α list)) ∧ (SHORTLEX R (h1::t1) (h2::t2) ⇔ LENGTH t1 < LENGTH t2 ∨ LENGTH t1 = LENGTH t2 ∧ (R h1 h2 ∨ h1 = h2 ∧ SHORTLEX R t1 t2)).\n",
      "Removing simp lemmas from (¬SHORTLEX (R :α -> α -> bool) ([] :α list) ([] :α list) ∧ ¬SHORTLEX R ((h1 :α)::(t1 :α list)) ([] :α list)) ∧ SHORTLEX R ([] :α list) ((h2 :α)::(t2 :α list)) ∧ (SHORTLEX R (h1::t1) (h2::t2) ⇔ LENGTH t1 < LENGTH t2 ∨ LENGTH t1 = LENGTH t2 ∧ (R h1 h2 ∨ h1 = h2 ∧ SHORTLEX R t1 t2))\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "¬SHORTLEX (R :α -> α -> bool) (l :α list) ([] :α list).\n",
      "Removing simp lemmas from ¬SHORTLEX (R :α -> α -> bool) (l :α list) ([] :α list)\n",
      "Goal Proved in 6 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(∀(x :α) (y :α). (R1 :α -> α -> bool) x y ⇒ (R2 :α -> α -> bool) x y) ⇒ SHORTLEX R1 (x :α list) (y :α list) ⇒ SHORTLEX R2 x y.\n",
      "Removing simp lemmas from (∀(x :α) (y :α). (R1 :α -> α -> bool) x y ⇒ (R2 :α -> α -> bool) x y) ⇒ SHORTLEX R1 (x :α list) (y :α list) ⇒ SHORTLEX R2 x y\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list). SHORTLEX (R :α -> α -> bool) l1 l2 ⇒ LENGTH l1 ≤ LENGTH l2.\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list). SHORTLEX (R :α -> α -> bool) l1 l2 ⇒ LENGTH l1 ≤ LENGTH l2\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "FINITE (s :α -> bool) ⇒ SET_TO_LIST s = if s = (∅ :α -> bool) then ([] :α list) else CHOICE s::SET_TO_LIST (REST s).\n",
      "Removing simp lemmas from FINITE (s :α -> bool) ⇒ SET_TO_LIST s = if s = (∅ :α -> bool) then ([] :α list) else CHOICE s::SET_TO_LIST (REST s)\n",
      "Goal Proved in 5 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "SET_TO_LIST {(x :α)} = [x].\n",
      "Removing simp lemmas from SET_TO_LIST {(x :α)} = [x]\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 8 steps\n",
      "Updating parameters ... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done. Main goal is:\n",
      "∀(s :α -> bool). FINITE s ⇒ set (SET_TO_LIST s) = s.\n",
      "Removing simp lemmas from ∀(s :α -> bool). FINITE s ⇒ set (SET_TO_LIST s) = s\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(P :(α -> bool) -> bool). (∀(s :α -> bool). (FINITE s ∧ s ≠ (∅ :α -> bool) ⇒ P (REST s)) ⇒ P s) ⇒ ∀(v :α -> bool). P v.\n",
      "Removing simp lemmas from ∀(P :(α -> bool) -> bool). (∀(s :α -> bool). (FINITE s ∧ s ≠ (∅ :α -> bool) ⇒ P (REST s)) ⇒ P s) ⇒ ∀(v :α -> bool). P v\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(s :α -> bool). FINITE s ⇒ ∀(x :α). x ∈ s ⇔ MEM x (SET_TO_LIST s).\n",
      "Removing simp lemmas from ∀(s :α -> bool). FINITE s ⇒ ∀(x :α). x ∈ s ⇔ MEM x (SET_TO_LIST s)\n",
      "Goal Proved in 4 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "SET_TO_LIST (∅ :α -> bool) = ([] :α list).\n",
      "Removing simp lemmas from SET_TO_LIST (∅ :α -> bool) = ([] :α list)\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(s :α -> bool). FINITE s ⇒ LENGTH (SET_TO_LIST s) = CARD s.\n",
      "Removing simp lemmas from ∀(s :α -> bool). FINITE s ⇒ LENGTH (SET_TO_LIST s) = CARD s\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "REVERSE ([] :β list) = ([] :β list) ∧ ∀(x :α) (l :α list). REVERSE (x::l) = SNOC x (REVERSE l).\n",
      "Removing simp lemmas from REVERSE ([] :β list) = ([] :β list) ∧ ∀(x :α) (l :α list). REVERSE (x::l) = SNOC x (REVERSE l)\n",
      "Goal Proved in 3 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(x :α) (l :α list). REVERSE (SNOC x l) = x::REVERSE l.\n",
      "Removing simp lemmas from ∀(x :α) (l :α list). REVERSE (SNOC x l) = x::REVERSE l\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list). REVERSE (REVERSE l) = l.\n",
      "Removing simp lemmas from ∀(l :α list). REVERSE (REVERSE l) = l\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(L :α list). REVERSE L = REV L ([] :α list).\n",
      "Removing simp lemmas from ∀(L :α list). REVERSE L = REV L ([] :α list)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(REVERSE :β list -> β list) ∘ (REVERSE :β list -> β list) ∘ (f :α -> β list) = f.\n",
      "Removing simp lemmas from (REVERSE :β list -> β list) ∘ (REVERSE :β list -> β list) ∘ (f :α -> β list) = f\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "REVERSE (GENLIST (f :num -> α) (n :num)) = GENLIST (λ(m :num). f (PRE n − m)) n.\n",
      "Removing simp lemmas from REVERSE (GENLIST (f :num -> α) (n :num)) = GENLIST (λ(m :num). f (PRE n − m)) n\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "REVERSE (l :α list) = [(e :α)] ⇔ l = [e].\n",
      "Removing simp lemmas from REVERSE (l :α list) = [(e :α)] ⇔ l = [e]\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "REVERSE (l :α list) = ([] :α list) ⇔ l = ([] :α list).\n",
      "Removing simp lemmas from REVERSE (l :α list) = ([] :α list) ⇔ l = ([] :α list)\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list). REVERSE (l1 ++ l2) = REVERSE l2 ++ REVERSE l1.\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list). REVERSE (l1 ++ l2) = REVERSE l2 ++ REVERSE l1\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list). REVERSE l1 = REVERSE l2 ⇔ l1 = l2.\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list). REVERSE l1 = REVERSE l2 ⇔ l1 = l2\n",
      "Goal Proved in 11 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(L1 :α list) (L2 :α list). REV L1 L2 = REVERSE L1 ++ L2.\n",
      "Removing simp lemmas from ∀(L1 :α list) (L2 :α list). REV L1 L2 = REVERSE L1 ++ L2\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(f1 :α -> β option) (f2 :α -> β option) (x1 :α list) (x2 :α list). x1 = x2 ∧ (∀(a :α). MEM a x2 ⇒ f1 a = f2 a) ⇒ OPT_MMAP f1 x1 = OPT_MMAP f2 x2.\n",
      "Removing simp lemmas from ∀(f1 :α -> β option) (f2 :α -> β option) (x1 :α list) (x2 :α list). x1 = x2 ∧ (∀(a :α). MEM a x2 ⇒ f1 a = f2 a) ⇒ OPT_MMAP f1 x1 = OPT_MMAP f2 x2\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "oHD ([] :α list) = (NONE :α option) ∧ oHD ((h :β)::(t :β list)) = SOME h.\n",
      "Removing simp lemmas from oHD ([] :α list) = (NONE :α option) ∧ oHD ((h :β)::(t :β list)) = SOME h\n",
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(xs :α list) (n :num). oEL n xs = if n < LENGTH xs then SOME (EL n xs) else (NONE :α option).\n",
      "Removing simp lemmas from ∀(xs :α list) (n :num). oEL n xs = if n < LENGTH xs then SOME (EL n xs) else (NONE :α option)\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "oEL (n :num) (TAKE (m :num) (xs :α list)) = SOME (x :α) ⇒ oEL n xs = SOME x.\n",
      "Removing simp lemmas from oEL (n :num) (TAKE (m :num) (xs :α list)) = SOME (x :α) ⇒ oEL n xs = SOME x\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(xs :α list) (i :num) (n :num) (x :α). oEL n (LUPDATE x i xs) = if i ≠ n then oEL n xs else if i < LENGTH xs then SOME x else (NONE :α option).\n",
      "Removing simp lemmas from ∀(xs :α list) (i :num) (n :num) (x :α). oEL n (LUPDATE x i xs) = if i ≠ n then oEL n xs else if i < LENGTH xs then SOME x else (NONE :α option)\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(xs :α list) (n :num) (y :α). oEL n xs = SOME y ⇔ n < LENGTH xs ∧ y = EL n xs.\n",
      "Removing simp lemmas from ∀(xs :α list) (n :num) (y :α). oEL n xs = SOME y ⇔ n < LENGTH xs ∧ y = EL n xs\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "oEL (n :num) (DROP (m :num) (xs :α list)) = oEL ((m + n) :num) xs.\n",
      "Removing simp lemmas from oEL (n :num) (DROP (m :num) (xs :α list)) = oEL ((m + n) :num) xs\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list). NULL l ⇔ LENGTH l = (0 :num).\n",
      "Removing simp lemmas from ∀(l :α list). NULL l ⇔ LENGTH l = (0 :num)\n",
      "Goal Proved in 13 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(n :num) (f :num -> α). NULL (GENLIST f n) ⇔ n = (0 :num).\n",
      "Removing simp lemmas from ∀(n :num) (f :num -> α). NULL (GENLIST f n) ⇔ n = (0 :num)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(P :α -> bool) (ls :α list). NULL (FILTER P ls) ⇔ ∀(x :α). MEM x ls ⇒ ¬P x.\n",
      "Removing simp lemmas from ∀(P :α -> bool) (ls :α list). NULL (FILTER P ls) ⇔ ∀(x :α). MEM x ls ⇒ ¬P x\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list). NULL l ⇔ l = ([] :α list).\n",
      "Removing simp lemmas from ∀(l :α list). NULL l ⇔ l = ([] :α list)\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 26 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "NULL ((l1 :α list) ++ (l2 :α list)) ⇔ NULL l1 ∧ NULL l2.\n",
      "Removing simp lemmas from NULL ((l1 :α list) ++ (l2 :α list)) ⇔ NULL l1 ∧ NULL l2\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "NULL ([] :α list) ∧ ∀(h :α) (t :α list). ¬NULL (h::t).\n",
      "Removing simp lemmas from NULL ([] :α list) ∧ ∀(h :α) (t :α list). ¬NULL (h::t)\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list). set (nub l) = set l.\n",
      "Removing simp lemmas from ∀(l :α list). set (nub l) = set l\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "nub ([] :α list) = ([] :α list).\n",
      "Removing simp lemmas from nub ([] :α list) = ([] :α list)\n",
      "Goal Proved in 4 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "nub (l :α list) = ([] :α list) ⇔ l = ([] :α list).\n",
      "Removing simp lemmas from nub (l :α list) = ([] :α list) ⇔ l = ([] :α list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal Proved in 2 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :α list). nub (l1 ++ l2) = nub (FILTER (λ(x :α). ¬MEM x l2) l1) ++ nub l2.\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :α list). nub (l1 ++ l2) = nub (FILTER (λ(x :α). ¬MEM x l2) l1) ++ nub l2\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "NRC (R :α -> α -> bool) (n :num) (x :α) (y :α) ⇔ ∃(ls :α list). LRC R ls x y ∧ LENGTH ls = n.\n",
      "Removing simp lemmas from NRC (R :α -> α -> bool) (n :num) (x :α) (y :α) ⇔ ∃(ls :α list). LRC R ls x y ∧ LENGTH ls = n\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list). ¬NULL l ⇔ ∃(e :α). MEM e l.\n",
      "Removing simp lemmas from ∀(l :α list). ¬NULL l ⇔ ∃(e :α). MEM e l\n",
      "same action\n",
      "same action\n",
      "Goal Proved in 35 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(x :α list) ≠ ([] :α list) ⇔ (0 :num) < LENGTH x.\n",
      "Removing simp lemmas from (x :α list) ≠ ([] :α list) ⇔ (0 :num) < LENGTH x\n",
      "Goal Proved in 13 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(a1 :α list) (a0 :α). ([] :α list) ≠ a0::a1.\n",
      "Removing simp lemmas from ∀(a1 :α list) (a0 :α). ([] :α list) ≠ a0::a1\n",
      "same action\n",
      "Goal Proved in 5 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(P :α -> bool) (l :α list). ¬EXISTS P l ⇔ EVERY ($¬ ∘ P) l.\n",
      "Removing simp lemmas from ∀(P :α -> bool) (l :α list). ¬EXISTS P l ⇔ EVERY ($¬ ∘ P) l\n",
      "Goal Proved in 3 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(P :α -> bool) (l :α list). ¬EVERY P l ⇔ EXISTS ($¬ ∘ P) l.\n",
      "Removing simp lemmas from ∀(P :α -> bool) (l :α list). ¬EVERY P l ⇔ EXISTS ($¬ ∘ P) l\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(h1 :α) (h2 :α). h1 ≠ h2 ⇒ ∀(l1 :α list) (l2 :α list). h1::l1 ≠ h2::l2.\n",
      "Removing simp lemmas from ∀(h1 :α) (h2 :α). h1 ≠ h2 ⇒ ∀(l1 :α list) (l2 :α list). h1::l1 ≠ h2::l2\n",
      "Goal Proved in 3 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(a1 :α list) (a0 :α). a0::a1 ≠ ([] :α list).\n",
      "Removing simp lemmas from ∀(a1 :α list) (a0 :α). a0::a1 ≠ ([] :α list)\n",
      "Goal Proved in 5 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(∀(x :α). (P :α -> bool) x ⇒ (Q :α -> bool) x) ⇒ EXISTS P (l :α list) ⇒ EXISTS Q l.\n",
      "Removing simp lemmas from (∀(x :α). (P :α -> bool) x ⇒ (Q :α -> bool) x) ⇒ EXISTS P (l :α list) ⇒ EXISTS Q l\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "(∀(x :α). (P :α -> bool) x ⇒ (Q :α -> bool) x) ⇒ EVERY P (l :α list) ⇒ EVERY Q l.\n",
      "Removing simp lemmas from (∀(x :α). (P :α -> bool) x ⇒ (Q :α -> bool) x) ⇒ EVERY P (l :α list) ⇒ EVERY Q l\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "LENGTH (FST (ps :α list # β list)) = LENGTH (SND ps) ∧ MEM (p :α # β) (ZIP ps) ⇒ MEM (FST p) (FST ps) ∧ MEM (SND p) (SND ps).\n",
      "Removing simp lemmas from LENGTH (FST (ps :α list # β list)) = LENGTH (SND ps) ∧ MEM (p :α # β) (ZIP ps) ⇒ MEM (FST p) (FST ps) ∧ MEM (SND p) (SND ps)\n",
      "same action\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "\"Vp_1'\"\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l1 :α list) (l2 :β list) (p :α # β). LENGTH l1 = LENGTH l2 ⇒ (MEM p (ZIP (l1,l2)) ⇔ ∃(n :num). n < LENGTH l1 ∧ p = (EL n l1,EL n l2)).\n",
      "Removing simp lemmas from ∀(l1 :α list) (l2 :β list) (p :α # β). LENGTH l1 = LENGTH l2 ⇒ (MEM p (ZIP (l1,l2)) ⇔ ∃(n :num). n < LENGTH l1 ∧ p = (EL n l1,EL n l2))\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (x :α). MEM x (TL l) ⇒ MEM x l.\n",
      "Removing simp lemmas from ∀(l :α list) (x :α). MEM x (TL l) ⇒ MEM x l\n",
      "same action\n",
      "Goal Proved in 12 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "MEM (e :α) (l :α list) ⇔ ∃(pfx :α list) (sfx :α list). l = pfx ++ [e] ++ sfx ∧ ¬MEM e sfx.\n",
      "Removing simp lemmas from MEM (e :α) (l :α list) ⇔ ∃(pfx :α list) (sfx :α list). l = pfx ++ [e] ++ sfx ∧ ¬MEM e sfx\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "MEM (e :α) (l :α list) ⇔ ∃(pfx :α list) (sfx :α list). l = pfx ++ [e] ++ sfx ∧ ¬MEM e pfx.\n",
      "Removing simp lemmas from MEM (e :α) (l :α list) ⇔ ∃(pfx :α list) (sfx :α list). l = pfx ++ [e] ++ sfx ∧ ¬MEM e pfx\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(x :α) (l :α list). MEM x l ⇔ ∃(l1 :α list) (l2 :α list). l = l1 ++ x::l2.\n",
      "Removing simp lemmas from ∀(x :α) (l :α list). MEM x l ⇔ ∃(l1 :α list) (l2 :α list). l = l1 ++ x::l2\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(y :α) (x :α) (l :α list). MEM y (SNOC x l) ⇔ y = x ∨ MEM y l.\n",
      "Removing simp lemmas from ∀(y :α) (x :α) (l :α list). MEM y (SNOC x l) ⇔ y = x ∨ MEM y l\n",
      "Goal Proved in 3 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(s :α -> bool). FINITE s ⇒ ∀(x :α). MEM x (SET_TO_LIST s) ⇔ x ∈ s.\n",
      "Removing simp lemmas from ∀(s :α -> bool). FINITE s ⇒ ∀(x :α). MEM x (SET_TO_LIST s) ⇔ x ∈ s\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (x :α). MEM x (REVERSE l) ⇔ MEM x l.\n",
      "Removing simp lemmas from ∀(l :α list) (x :α). MEM x (REVERSE l) ⇔ MEM x l\n",
      "Goal Proved in 1 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(f :α -> β) (l :α list) (a :α). MEM a l ⇒ MEM (f a) (MAP f l).\n",
      "Removing simp lemmas from ∀(f :α -> β) (l :α list) (a :α). MEM a l ⇒ MEM (f a) (MAP f l)\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (f :α -> β) (x :β). MEM x (MAP f l) ⇔ ∃(y :α). x = f y ∧ MEM y l.\n",
      "Removing simp lemmas from ∀(l :α list) (f :α -> β) (x :β). MEM x (MAP f l) ⇔ ∃(y :α). x = f y ∧ MEM y l\n",
      "same action\n",
      "Goal Proved in 36 steps\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (x :α) (y :α) (i :num). MEM x (LUPDATE y i l) ⇒ x = y ∨ MEM x l.\n",
      "Removing simp lemmas from ∀(l :α list) (x :α) (y :α) (i :num). MEM x (LUPDATE y i l) ⇒ x = y ∨ MEM x l\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "∀(l :α list) (x :α) (y :α) (i :num). MEM x (LUPDATE y i l) ⇔ i < LENGTH l ∧ x = y ∨ ∃(j :num). j < LENGTH l ∧ i ≠ j ∧ EL j l = x.\n",
      "Removing simp lemmas from ∀(l :α list) (x :α) (y :α) (i :num). MEM x (LUPDATE y i l) ⇔ i < LENGTH l ∧ x = y ∨ ∃(j :num). j < LENGTH l ∧ i ≠ j ∧ EL j l = x\n",
      "same action\n",
      "Failed.\n",
      "Updating parameters ... \n",
      "Initialization done. Main goal is:\n",
      "MEM (x :α) (GENLIST (f :num -> α) (n :num)) ⇔ ∃(m :num). m < n ∧ x = f m.\n",
      "Removing simp lemmas from MEM (x :α) (GENLIST (f :num -> α) (n :num)) ⇔ ∃(m :num). m < n ∧ x = f m\n"
     ]
    }
   ],
   "source": [
    "exp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stupid-hobby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "arabic-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing theories...\n",
      "Loading modules...\n",
      "Configuration done.\n"
     ]
    }
   ],
   "source": [
    "e = HolEnv(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "allied-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#fix error handling for step exception \n",
    "#Add logging, saving of logs, experiment metadata (date, agent info, database used etc.)\n",
    "#Add validation logic\n",
    "#Run on paper dataset\n",
    "#Add replays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
