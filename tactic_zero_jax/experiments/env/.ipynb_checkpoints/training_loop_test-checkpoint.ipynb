{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constitutional-implement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from jax import random\n",
    "import sys\n",
    "from time import sleep\n",
    "import json\n",
    "import pexpect\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "import seq2seq\n",
    "from batch_predictor import BatchPredictor\n",
    "from checkpoint import Checkpoint\n",
    "\n",
    "from policy_networks import *\n",
    "import policy_networks\n",
    "\n",
    "from new_env import *\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True) \n",
    "#jax.config.update(\"jax_enable_x64\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "working-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLPATH = \"/home/sean/Documents/PhD/HOL4/HOL/bin/hol --maxheap=256\"\n",
    "tactic_zero_path = \"/home/sean/Documents/PhD/git/repo/PhD/tacticzero/holgym/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "welcome-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"typed_database.json\") as f:\n",
    "    database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "human-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "MORE_TACTICS = True\n",
    "if not MORE_TACTICS:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\"]\n",
    "    thm_tactic = [\"irule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\"]\n",
    "else:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "    thm_tactic = [\"irule\", \"drule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\", \"EQ_TAC\"]\n",
    "    \n",
    "tactic_pool = thms_tactic + thm_tactic + term_tactic + no_arg_tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "logical-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polish(raw_goal):\n",
    "        goal = construct_goal(raw_goal)\n",
    "        process.sendline(goal.encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer (HOLPP.add_string o pt);\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"top_goals();\".encode(\"utf-8\"))\n",
    "        process.expect(\"val it =\")\n",
    "        process.expect([\": goal list\", \":\\r\\n +goal list\"])\n",
    "\n",
    "        polished_raw = process.before.decode(\"utf-8\")\n",
    "        polished_subgoals = re.sub(\"“|”\",\"\\\"\", polished_raw)\n",
    "        polished_subgoals = re.sub(\"\\r\\n +\",\" \", polished_subgoals)\n",
    "\n",
    "        pd = eval(polished_subgoals)\n",
    "        \n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"drop();\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer default_pt;\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "\n",
    "        data = [{\"polished\":{\"assumptions\": e[0][0], \"goal\":e[0][1]},\n",
    "                 \"plain\":{\"assumptions\": e[1][0], \"goal\":e[1][1]}}\n",
    "                for e in zip(pd, [([], raw_goal)])]\n",
    "        return data \n",
    "    \n",
    "def construct_goal(goal):\n",
    "    s = \"g \" + \"`\" + goal + \"`;\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aware-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_encoded_content_(history, encoder):\n",
    "    fringe_sizes = []\n",
    "    contexts = []\n",
    "    reverted = []\n",
    "    for i in history:\n",
    "        c = i[\"content\"]\n",
    "        contexts.extend(c)\n",
    "        fringe_sizes.append(len(c))\n",
    "    for e in contexts:\n",
    "        g = revert_with_polish(e)\n",
    "        reverted.append(g.strip().split())\n",
    "    out = []\n",
    "    sizes = []\n",
    "    for goal in reverted:\n",
    "        out_, sizes_ = encoder.encode([goal])\n",
    "        out.append(torch.cat(out_.split(1), dim=2).squeeze(0))\n",
    "        sizes.append(sizes_)\n",
    "\n",
    "    representations = out\n",
    "\n",
    "    return representations, contexts, fringe_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fiscal-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_theory(pg):\n",
    "    theories = re.findall(r'C\\$(\\w+)\\$ ', pg)\n",
    "    theories = set(theories)\n",
    "    for th in EXCLUDED_THEORIES:\n",
    "        theories.discard(th)\n",
    "    return list(theories)\n",
    "\n",
    "def revert_with_polish(context):\n",
    "    target = context[\"polished\"]\n",
    "    assumptions = target[\"assumptions\"]\n",
    "    goal = target[\"goal\"]\n",
    "    for i in reversed(assumptions): \n",
    "        #goal = \"@ @ D$min$==> {} {}\".format(i, goal)\n",
    "        goal = \"@ @ C$min$ ==> {} {}\".format(i, goal)\n",
    "\n",
    "    return goal \n",
    "\n",
    "def split_by_fringe(goal_set, goal_scores, fringe_sizes):\n",
    "    # group the scores by fringe\n",
    "    fs = []\n",
    "    gs = []\n",
    "    counter = 0\n",
    "    for i in fringe_sizes:\n",
    "        end = counter + i\n",
    "        fs.append(goal_scores[counter:end])\n",
    "        gs.append(goal_set[counter:end])\n",
    "        counter = end\n",
    "    return gs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "personal-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇒ LIST_REL R (l1 ++ l3) (l2 ++ l4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"include_probability.json\") as f:\n",
    "    database = json.load(f)\n",
    "\n",
    "#all theories in database\n",
    "#TARGET_THEORIES = [\"probability\", \"martingale\", \"lebesgue\", \"borel\", \"real_borel\", \"sigma_algebra\",\"util_prob\", \"fcp\", \"indexedLists\", \"rich_list\", \"list\", \"pred_set\",\"numpair\", \"basicSize\", \"numeral\", \"arithmetic\", \"prim_rec\", \"num\",\"marker\", \"bool\", \"min\", \"normalForms\", \"relation\", \"sum\", \"pair\", \"sat\",\"while\", \"bit\", \"logroot\", \"transc\", \"powser\", \"lim\", \"seq\", \"nets\",\"metric\", \"real\", \"realax\", \"hreal\", \"hrat\", \"quotient_sum\", \"quotient\",\"res_quan\", \"product\", \"iterate\", \"cardinal\", \"wellorder\",\"set_relation\", \"derivative\", \"real_topology\"]\n",
    "\n",
    "with open(\"polished_def_dict.json\") as f:\n",
    "    defs = json.load(f)\n",
    "\n",
    "fact_pool = list(defs.keys())\n",
    "\n",
    "encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "\n",
    "TARGET_THEORIES = [\"bool\", \"min\", \"list\"]\n",
    "GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "print (GOALS[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attended-dutch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'seq2seq.models.EncoderRNN.EncoderRNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'seq2seq.models.DecoderRNN.DecoderRNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_path = \"models/2020_04_22_20_36_50\" # 91% accuracy model, only core theories\n",
    "#checkpoint_path = \"models/2020_04_26_20_11_28\" # 95% accuracy model, core theories + integer + sorting\n",
    "#checkpoint_path = \"models/2020_09_24_23_38_06\" # 98% accuracy model, core theories + integer + sorting | separate theory tokens\n",
    "#checkpoint_path = \"models/2020_11_28_16_45_10\" # 96-98% accuracy model, core theories + integer + sorting + real | separate theory tokens\n",
    "\n",
    "#checkpoint_path = \"models/2020_12_04_03_47_22\" # 97% accuracy model, core theories + integer + sorting + real + bag | separate theory tokens\n",
    "\n",
    "checkpoint_path = \"models/2021_02_21_15_46_04\" # 98% accuracy model, up to probability theory\n",
    "\n",
    "#checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "\n",
    "checkpoint = Checkpoint.load(checkpoint_path)\n",
    "seq2seq = checkpoint.model\n",
    "input_vocab = checkpoint.input_vocab\n",
    "output_vocab = checkpoint.output_vocab\n",
    "\n",
    "batch_encoder_ = BatchPredictor(seq2seq, input_vocab, output_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "trying-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to give the log probability of pi(f | s) so gradient can be computed directly\n",
    "#also returns sampled index and contexts to determine goal to give tactic network\n",
    "def sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes):\n",
    "    context_scores = context_net(context_params, rng_key, jax_reps)\n",
    "    contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "    fringe_scores = []\n",
    "    for s in scores_by_fringe:\n",
    "        fringe_score = jnp.sum(s)\n",
    "        fringe_scores.append(fringe_score)\n",
    "    #TODO some fringes can be empty, but still give value 0 which assigns nonzero probability?\n",
    "    fringe_scores = jnp.stack(fringe_scores)\n",
    "    fringe_probs = jax.nn.log_softmax(fringe_scores)\n",
    "\n",
    "    #samples, gives an index (looks like it does gumbel softmax under the hood to keep differentiability?)\n",
    "    sampled_idx = jax.random.categorical(rng_key,fringe_probs)\n",
    "\n",
    "    log_prob = fringe_probs[sampled_idx]\n",
    "    #log_prob = jnp.log(prob)\n",
    "    return log_prob, (sampled_idx, contexts_by_fringe)\n",
    "                                                           \n",
    "#grad_log_context, (fringe_idx, contexts_by_fringe) = jax.grad(sample_fringe, has_aux=True)(context_params, apply_context, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "\n",
    "#takes a goal encoding and samples tactic from network, and returns log prob for gradient \n",
    "def sample_tactic(tactic_params, tac_net, rng_key, goal_endcoding, action_size=len(tactic_pool)):\n",
    "    tac_scores = tac_net(tactic_params, rng_key, goal_endcoding, action_size)\n",
    "    tac_scores = jnp.ravel(tac_scores)\n",
    "    #tac_scores = tac_scores - max(tac_scores)\n",
    "    #print (tac_scores)\n",
    "    #subtract max element for numerical stability \n",
    "    tac_probs = jax.nn.log_softmax(tac_scores)\n",
    "    tac_idx = jax.random.categorical(rng_key, tac_probs)\n",
    "    log_prob = tac_probs[tac_idx]#jnp.log(tac_probs[tac_idx])\n",
    "    print (jnp.exp(log_prob).primal)#, jnp.exp(tac_probs), rng_key)\n",
    "    return log_prob, tac_idx\n",
    "\n",
    "#grad_log_tac, tac_idx = jax.grad(sample_tactic, has_aux=True)(tactic_params, apply_tac, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "\n",
    "#sampled_tac = tactic_pool[tac_idx]\n",
    "\n",
    "def sample_term(term_params, term_net, rng_key, candidates):\n",
    "    term_scores = term_net(term_params, rng_key, candidates)\n",
    "    term_scores = jnp.ravel(term_scores)\n",
    "    term_probs = jax.nn.log_softmax(term_scores)\n",
    "    term_idx = jax.random.categorical(rng_key, term_probs)\n",
    "    log_prob = term_probs[term_idx]#jnp.log(term_probs[term_idx])\n",
    "    return log_prob, term_idx\n",
    "\n",
    "#grad_log_term, term_idx = jax.grad(sample_term, has_aux=True)(term_params, apply_term, rng_key, candidates)#, tac_idx, len(tactic_pool), candidates.shape[1])\n",
    "\n",
    "#function for sampling single argument given previous arguments, \n",
    "def sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, tactic_size, embedding_dim):\n",
    "    hidden, arg_scores = arg_net(arg_params, rng_key, input_, candidates, hidden, tactic_size, embedding_dim)\n",
    "    arg_scores = jnp.ravel(arg_scores)\n",
    "    arg_probs = jax.nn.log_softmax(arg_scores)\n",
    "    arg_idx = jax.random.categorical(rng_key, arg_probs)\n",
    "    log_prob = arg_probs[arg_idx]#jnp.log(arg_probs[arg_idx])\n",
    "    return log_prob, (arg_idx, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "charitable-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(context_params, tactic_params, term_params, arg_params, context_net, tactic_net, term_net, arg_net, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args):\n",
    "    \n",
    "    log_context, (fringe_idx, contexts_by_fringe) = sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "    \n",
    "    try:\n",
    "        target_context = contexts_by_fringe[fringe_idx][0]\n",
    "    except:\n",
    "        print (\"error {} {}\".format(contexts_by_fringe), fringe_idx)\n",
    "    target_goal = target_context[\"polished\"][\"goal\"]\n",
    "    target_representation = jax_reps[context_set.index(target_context)]\n",
    "    \n",
    "    log_tac, tac_idx = sample_tactic(tactic_params, tactic_net, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "    \n",
    "    sampled_tac = tactic_pool[tac_idx]\n",
    "    arg_logs = []\n",
    "\n",
    "    tactic = sampled_tac\n",
    "    #for testing\n",
    "    \n",
    "    #sampled_tac = \"Induct_on\"\n",
    "\n",
    "    #if tactic requires no argument\n",
    "    if sampled_tac in no_arg_tactic:\n",
    "        full_tactic = sampled_tac #tactic_pool[tac]\n",
    "\n",
    "\n",
    "    #Induct_on case; use term policy to find which term to induct on \n",
    "    elif sampled_tac in term_tactic:\n",
    "\n",
    "        goal_tokens = target_goal.split()\n",
    "        term_tokens = [[t] for t in set(goal_tokens) if t[0] == \"V\"]\n",
    "        #add conditional if tokens is empty \n",
    "\n",
    "        #now want encodings for terms from AE\n",
    "\n",
    "        term_reps = []\n",
    "\n",
    "        for term in term_tokens:\n",
    "            term_rep, _ = batch_encoder_.encode([term])\n",
    "            #output is bidirectional so concat vectors\n",
    "            term_reps.append(torch.cat(term_rep.split(1), dim=2).squeeze(0))\n",
    "        \n",
    "        print (len(term_reps))\n",
    "        # convert to jax\n",
    "        term_reps = jnp.stack([jnp.array(term_reps[i][0]) for i in range(len(term_reps))])\n",
    "\n",
    "        # now want inputs to term_net to be target_representation (i.e. goal) concatenated with terms\n",
    "        # models the policies conditional dependence of the term given the goal\n",
    "\n",
    "        #stack goal representation for each token\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in term_tokens])\n",
    "\n",
    "        #concat with term encodings to give candidate matrix\n",
    "        candidates = jnp.concatenate([goal_stack, term_reps], 1)\n",
    "\n",
    "        log_term, term_idx = sample_term(term_params, term_net, rng_key, candidates)\n",
    "\n",
    "        sampled_term = term_tokens[term_idx]\n",
    "\n",
    "        tm = sampled_term[0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "    \n",
    "        arg_logs = [log_term]\n",
    "        \n",
    "        if tm:\n",
    "            tactic = \"Induct_on `{}`\".format(tm)\n",
    "        else:\n",
    "            # only to raise an error\n",
    "            tactic = \"Induct_on\"\n",
    "        \n",
    "    #argument tactic\n",
    "    else:\n",
    "        #stack goals to possible arguments to feed into FFN\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in encoded_fact_pool])\n",
    "        candidates = jnp.concatenate([encoded_fact_pool, goal_stack], 1)\n",
    "        \n",
    "        #initial state set as goal\n",
    "        hidden = jnp.expand_dims(target_representation,0)\n",
    "        init_state = hk.LSTMState(hidden,hidden)\n",
    "    \n",
    "        # run through first with tac_idx to initialise state with tactic as c_0\n",
    "        hidden, _ = arg_net(arg_params, rng_key, tac_idx, candidates, init_state, len(tactic_pool), 256)\n",
    "        \n",
    "        ARG_LEN = 5\n",
    "        arg_inds = []\n",
    "        arg_logs = []\n",
    "        input_ = tac_idx\n",
    "        for _ in range(ARG_LEN):\n",
    "            log_arg, (arg_idx, hidden) = sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, len(tactic_pool), 256)\n",
    "            arg_logs.append(log_arg)\n",
    "            arg_inds.append(arg_idx)\n",
    "            input_ = jnp.expand_dims(encoded_fact_pool[arg_idx], 0)\n",
    "        \n",
    "        arg = [candidate_args[i] for i in arg_inds]\n",
    "\n",
    "        tactic = env.assemble_tactic(sampled_tac, arg)\n",
    "        \n",
    "    \n",
    "    \n",
    "    action = (int(fringe_idx), 0, tactic)\n",
    "    print (\"Action {}:\\n\".format(action))\n",
    "    \n",
    "    try:\n",
    "        reward, done = env.step(action)\n",
    "\n",
    "    except:\n",
    "        print(\"Step exception raised.\")\n",
    "        # print(\"Fringe: {}\".format(env.history))\n",
    "        print(\"Handling: {}\".format(env.handling))\n",
    "        print(\"Using: {}\".format(env.using))\n",
    "        # try again\n",
    "        # counter = env.counter\n",
    "        frequency = env.frequency\n",
    "        env.close()\n",
    "        print(\"Aborting current game ...\")\n",
    "        print(\"Restarting environment ...\")\n",
    "        print(env.goal)\n",
    "        env = HolEnv(env.goal)\n",
    "        flag = False\n",
    "        return \n",
    "        \n",
    "    print (\"Result: Reward {}\".format(reward))#, env.history[-1]))\n",
    "\n",
    "    \n",
    "    #negative as we want gradient ascent \n",
    "    \n",
    "    loss = (-log_tac - log_context  - sum(arg_logs)) * reward\n",
    "\n",
    "    return loss, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "incorporated-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(goals):\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(11)\n",
    "\n",
    "    init_context, apply_context = hk.transform(policy_networks._context_forward)\n",
    "    #apply_context = jax.jit(apply_context)\n",
    "\n",
    "    init_tac, apply_tac = hk.transform(policy_networks._tac_forward)\n",
    "    #apply_tac = partial(jax.jit, static_argnums=3)(apply_tac)\n",
    "\n",
    "    init_term, apply_term = hk.transform(policy_networks._term_no_tac_forward)\n",
    "    #apply_term = jax.jit(apply_term)\n",
    "\n",
    "    init_arg, apply_arg = hk.transform(policy_networks._arg_forward)\n",
    "    #apply_arg = partial(jax.jit, static_argnums=(5,6))(apply_arg)\n",
    "\n",
    "    #initialise these with e.g. random uniform, glorot, He etc. should exist outside function for action selection \n",
    "    context_params = init_context(rng_key, jax.random.normal(rng_key, (1,256)))\n",
    "\n",
    "    tactic_params = init_tac(rng_key, jax.random.normal(rng_key, (1,256)), len(tactic_pool))\n",
    "\n",
    "    #term_policy for now is only considering variables for induction, hence does not need any arguments \n",
    "    term_params = init_term(rng_key, jax.random.normal(rng_key, (1,512)))\n",
    "\n",
    "    hidden = jax.random.normal(rng_key, (1,256))\n",
    "\n",
    "    init_state = hk.LSTMState(hidden, hidden)\n",
    "\n",
    "    arg_params = init_arg(rng_key, jax.random.randint(rng_key, (), 0, len(tactic_pool)), jax.random.normal(rng_key, (1,512)), init_state, len(tactic_pool), 256)\n",
    "\n",
    "        \n",
    "    context_lr = 1e-4\n",
    "    tactic_lr = 1e-4\n",
    "    arg_lr = 1e-4\n",
    "    term_lr = 1e-4\n",
    "\n",
    "    context_optimiser = optax.rmsprop(context_lr)\n",
    "    tactic_optimiser = optax.rmsprop(tactic_lr)\n",
    "    arg_optimiser = optax.rmsprop(arg_lr)\n",
    "    term_optimiser = optax.rmsprop(term_lr)\n",
    "\n",
    "    opt_state_context = context_optimiser.init(context_params)\n",
    "    opt_state_tactic = tactic_optimiser.init(tactic_params)\n",
    "    opt_state_arg = arg_optimiser.init(arg_params)\n",
    "    opt_state_term = term_optimiser.init(term_params)\n",
    "    \n",
    "    \n",
    "    for goal in goals[2:]:\n",
    "        g = goal[1]\n",
    "            \n",
    "        env = HolEnv(g)\n",
    "\n",
    "        theories = re.findall(r'C\\$(\\w+)\\$ ', goal[0])\n",
    "        theories = set(theories)\n",
    "        theories = list(theories)\n",
    "\n",
    "        allowed_theories = theories\n",
    "\n",
    "        goal_theory = g\n",
    "\n",
    "        #print (\"Target goal: {}\".format(g))\n",
    "        \n",
    "        try:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            goal_theory = g#database[polished_goal][0] # plain_database[goal][0]\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories and (database[t][0] != goal_theory or int(database[t][2]) < int(database[polished_goal][2])):\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "\n",
    "            env.toggle_simpset(\"diminish\", goal_theory)\n",
    "            #print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "        except:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories:\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "            #print(\"Theorem not found in database.\")\n",
    "\n",
    "        print (\"Number of candidate facts to use: {}\".format(len(candidate_args)))\n",
    "\n",
    "        encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "        encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids))\n",
    "        \n",
    "        encoded_fact_pool = jnp.array(encoded_fact_pool)\n",
    "        \n",
    "        for i in range(50):\n",
    "            \n",
    "            _, rng_key = jax.random.split(rng_key)\n",
    "\n",
    "            \n",
    "            print (\"Proof step {} of 50\\n\".format(i+1))\n",
    "        \n",
    "            jax_reps, context_set, fringe_sizes = gather_encoded_content_(env.history, batch_encoder_)\n",
    "\n",
    "            #convert to jax\n",
    "            jax_reps = jnp.stack([jnp.array(jax_reps[i][0]) for i in range(len(jax_reps))])\n",
    "\n",
    "           \n",
    "            gradients, reward = jax.grad(loss, argnums=(0,1,2,3), has_aux=True)(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args)\n",
    "            \n",
    "#             if reward == -1:\n",
    "#                 print (tactic_params)\n",
    "#                 print (opt_state_tactic)\n",
    "#                 print (tactic_updates)\n",
    "            \n",
    "            #update parameters\n",
    "            context_updates, opt_state_context = context_optimiser.update(gradients[0], opt_state_context)\n",
    "            context_params = optax.apply_updates(context_params, context_updates)\n",
    "            \n",
    "            tactic_updates, opt_state_tactic = tactic_optimiser.update(gradients[1], opt_state_tactic)\n",
    "            tactic_params = optax.apply_updates(tactic_params, tactic_updates)\n",
    "            \n",
    "            term_updates, opt_state_term = term_optimiser.update(gradients[2], opt_state_term)\n",
    "            term_params = optax.apply_updates(term_params, term_updates)\n",
    "            \n",
    "            arg_updates, opt_state_arg = arg_optimiser.update(gradients[3], opt_state_arg)\n",
    "            arg_params = optax.apply_updates(arg_params, arg_updates)\n",
    "            \n",
    "            #if goal proven\n",
    "            if reward == 5:\n",
    "                print (\"Goal {} proved in {} steps\".format(g, i+1))\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sorted-conference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing theories...\n",
      "Loading modules...\n",
      "Configuration done.\n",
      "Removing simp lemmas from ∀(ls :α list) (n :num). ALL_DISTINCT ls ⇒ ALL_DISTINCT (DROP n ls)\n",
      "Number of candidate facts to use: 686\n",
      "Proof step 1 of 50\n",
      "\n",
      "0.093078464\n",
      "Action (0, 0, 'drule listTheory.EVERY2_MEM_MONO'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 2 of 50\n",
      "\n",
      "0.10797914\n",
      "Action (0, 0, 'rw[listTheory.EL_compute, listTheory.EL_compute, listTheory.EL_compute, listTheory.EL_compute, listTheory.EL_compute]'):\n",
      "\n",
      "Result: Reward 0.1\n",
      "Proof step 3 of 50\n",
      "\n",
      "0.13813129\n",
      "Action (0, 0, 'fs[listTheory.EVERY2_trans, listTheory.EVERY2_trans, listTheory.EVERY2_trans, listTheory.EVERY2_trans, listTheory.EVERY2_trans]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 4 of 50\n",
      "\n",
      "0.096507065\n",
      "Action (1, 0, 'metis_tac[boolTheory.FORALL_itself, boolTheory.FORALL_itself, boolTheory.FORALL_itself, boolTheory.FORALL_itself, boolTheory.FORALL_itself]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 5 of 50\n",
      "\n",
      "0.06429373\n",
      "Action (1, 0, 'drule listTheory.FOLDL'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 6 of 50\n",
      "\n",
      "0.11121741\n",
      "Action (0, 0, 'EQ_TAC'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 7 of 50\n",
      "\n",
      "0.13724889\n",
      "Action (1, 0, 'rw[listTheory.LIST_TO_SET_GENLIST, listTheory.LIST_TO_SET_GENLIST, listTheory.LIST_TO_SET_GENLIST, listTheory.LIST_TO_SET_GENLIST, listTheory.LIST_TO_SET_GENLIST]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 8 of 50\n",
      "\n",
      "0.12856826\n",
      "Action (1, 0, 'simp[listTheory.FOLDL_SNOC, listTheory.FOLDL_SNOC, listTheory.FOLDL_SNOC, listTheory.FOLDL_SNOC, listTheory.FOLDL_SNOC]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 9 of 50\n",
      "\n",
      "0.053141996\n",
      "Action (1, 0, 'drule listTheory.SWAP_REVERSE'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 10 of 50\n",
      "\n",
      "0.0996897\n",
      "2\n",
      "Action (0, 0, 'Induct_on `n`'):\n",
      "\n",
      "Result: Reward 0.1\n",
      "Proof step 11 of 50\n",
      "\n",
      "0.08420718\n",
      "Action (2, 0, 'metis_tac[boolTheory.COND_ABS, boolTheory.COND_ABS, boolTheory.COND_ABS, boolTheory.COND_ABS, boolTheory.COND_ABS]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 12 of 50\n",
      "\n",
      "0.15180576\n",
      "Action (1, 0, 'strip_tac'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 13 of 50\n",
      "\n",
      "0.095184855\n",
      "Action (1, 0, 'EQ_TAC'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 14 of 50\n",
      "\n",
      "0.16902342\n",
      "Action (0, 0, 'irule listTheory.splitAtPki_EQN'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 15 of 50\n",
      "\n",
      "0.13211644\n",
      "1\n",
      "Action (2, 0, 'Induct_on `ls`'):\n",
      "\n",
      "Result: Reward 0.1\n",
      "Proof step 16 of 50\n",
      "\n",
      "0.13144657\n",
      "Action (3, 0, 'irule listTheory.oEL_LUPDATE'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 17 of 50\n",
      "\n",
      "0.1388692\n",
      "Action (1, 0, 'irule listTheory.SET_TO_LIST_SING'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 18 of 50\n",
      "\n",
      "0.12583114\n",
      "Action (3, 0, 'rw[listTheory.SNOC_INDUCT, listTheory.SNOC_INDUCT, listTheory.SNOC_INDUCT, listTheory.SNOC_INDUCT, listTheory.SNOC_INDUCT]'):\n",
      "\n",
      "Result: Reward 0.1\n",
      "Proof step 19 of 50\n",
      "\n",
      "0.1378067\n",
      "Action (2, 0, 'fs[listTheory.LIST_REL_MEM_IMP, listTheory.LIST_REL_MEM_IMP, listTheory.LIST_REL_MEM_IMP, listTheory.LIST_REL_MEM_IMP, listTheory.LIST_REL_MEM_IMP]'):\n",
      "\n",
      "Result: Reward 0.2\n",
      "Proof step 20 of 50\n",
      "\n",
      "0.070371516\n",
      "Action (2, 0, 'metis_tac[listTheory.SHORTLEX_transitive, listTheory.SHORTLEX_transitive, listTheory.SHORTLEX_transitive, listTheory.SHORTLEX_transitive, listTheory.SHORTLEX_transitive]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 21 of 50\n",
      "\n",
      "0.13544019\n",
      "Action (3, 0, 'strip_tac'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 22 of 50\n",
      "\n",
      "0.10332948\n",
      "Action (5, 0, 'EQ_TAC'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 23 of 50\n",
      "\n",
      "0.077804096\n",
      "Action (4, 0, 'metis_tac[listTheory.LENGTH_CONS, listTheory.LENGTH_CONS, listTheory.LENGTH_CONS, listTheory.LENGTH_CONS, listTheory.LENGTH_CONS]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 24 of 50\n",
      "\n",
      "0.09845689\n",
      "2\n",
      "Action (3, 0, 'Induct_on `ls`'):\n",
      "\n",
      "Result: Reward 0.1\n",
      "Proof step 25 of 50\n",
      "\n",
      "0.21252924\n",
      "Action (3, 0, 'fs[listTheory.MAP_MAP_o, listTheory.MAP_MAP_o, listTheory.MAP_MAP_o, listTheory.MAP_MAP_o, listTheory.MAP_MAP_o]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 26 of 50\n",
      "\n",
      "0.06654093\n",
      "Action (6, 0, 'metis_tac[listTheory.NULL_APPEND, listTheory.NULL_APPEND, listTheory.NULL_APPEND, listTheory.NULL_APPEND, listTheory.NULL_APPEND]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 27 of 50\n",
      "\n",
      "0.10310116\n",
      "Action (3, 0, 'irule listTheory.oEL_LUPDATE'):\n",
      "\n",
      "same action\n",
      "Result: Reward -1\n",
      "Proof step 28 of 50\n",
      "\n",
      "0.11216332\n",
      "Action (0, 0, 'simp[boolTheory.DISJ_COMM, boolTheory.DISJ_COMM, boolTheory.DISJ_COMM, boolTheory.DISJ_COMM, boolTheory.DISJ_COMM]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 29 of 50\n",
      "\n",
      "0.1600753\n",
      "Action (0, 0, 'fs[listTheory.NULL_APPEND, listTheory.NULL_APPEND, listTheory.NULL_APPEND, listTheory.NULL_APPEND, listTheory.NULL_APPEND]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 30 of 50\n",
      "\n",
      "0.09557895\n",
      "Action (2, 0, 'irule listTheory.MEM_FLAT'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 31 of 50\n",
      "\n",
      "0.16411638\n",
      "Action (3, 0, 'rw[boolTheory.REFL_CLAUSE, boolTheory.REFL_CLAUSE, boolTheory.REFL_CLAUSE, boolTheory.REFL_CLAUSE, boolTheory.REFL_CLAUSE]'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 32 of 50\n",
      "\n",
      "0.104642704\n",
      "Action (6, 0, 'strip_tac'):\n",
      "\n",
      "Result: Reward 0.1\n",
      "Proof step 33 of 50\n",
      "\n",
      "0.11850068\n",
      "Action (4, 0, 'strip_tac'):\n",
      "\n",
      "Result: Reward 0.1\n",
      "Proof step 34 of 50\n",
      "\n",
      "0.072233886\n",
      "Action (8, 0, 'EQ_TAC'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 35 of 50\n",
      "\n",
      "0.14413781\n",
      "Action (7, 0, 'strip_tac'):\n",
      "\n",
      "Result: Reward -0.1\n",
      "Proof step 36 of 50\n",
      "\n",
      "0.23045906\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Need at least one array to stack.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGOALS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(goals)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;66;03m#convert to jax\u001b[39;00m\n\u001b[1;32m    102\u001b[0m             jax_reps \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mstack([jnp\u001b[38;5;241m.\u001b[39marray(jax_reps[i][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(jax_reps))])\n\u001b[0;32m--> 105\u001b[0m             gradients, reward \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtactic_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_tac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_term\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjax_reps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfringe_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_fact_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#             if reward == -1:\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#                 print (tactic_params)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#                 print (opt_state_tactic)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#                 print (tactic_updates)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m             \n\u001b[1;32m    112\u001b[0m             \u001b[38;5;66;03m#update parameters\u001b[39;00m\n\u001b[1;32m    113\u001b[0m             context_updates, opt_state_context \u001b[38;5;241m=\u001b[39m context_optimiser\u001b[38;5;241m.\u001b[39mupdate(gradients[\u001b[38;5;241m0\u001b[39m], opt_state_context)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(context_params, tactic_params, term_params, arg_params, context_net, tactic_net, term_net, arg_net, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;28mlen\u001b[39m(term_reps))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# convert to jax\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m term_reps \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm_reps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mterm_reps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# now want inputs to term_net to be target_representation (i.e. goal) concatenated with terms\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# models the policies conditional dependence of the term given the goal\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#stack goal representation for each token\u001b[39;00m\n\u001b[1;32m     51\u001b[0m goal_stack \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate([jnp\u001b[38;5;241m.\u001b[39mexpand_dims(target_representation,\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m term_tokens])\n",
      "File \u001b[0;32m~/Documents/venvs/jax/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:1570\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;129m@_wraps\u001b[39m(np\u001b[38;5;241m.\u001b[39mstack, skip_params\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(arrays, axis: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1569\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays):\n\u001b[0;32m-> 1570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed at least one array to stack.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1571\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument to jnp.stack is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Need at least one array to stack."
     ]
    }
   ],
   "source": [
    "train(GOALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-intensity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
