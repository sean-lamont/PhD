{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "considerable-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from jax import random\n",
    "import sys\n",
    "from time import sleep\n",
    "import json\n",
    "import pexpect\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "import seq2seq\n",
    "from batch_predictor import BatchPredictor\n",
    "from checkpoint import Checkpoint\n",
    "\n",
    "from policy_networks import *\n",
    "import policy_networks\n",
    "\n",
    "from new_env import *\n",
    "\n",
    "from jax.config import config\n",
    "#jax.config.update(\"jax_debug_nans\", True) \n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "from helpers import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "virtual-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to give the log probability of pi(f | s) so gradient can be computed directly\n",
    "#also returns sampled index and contexts to determine goal to give tactic network\n",
    "def sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes):\n",
    "    context_scores = context_net(context_params, rng_key, jax_reps)\n",
    "    contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "    fringe_scores = []\n",
    "    for s in scores_by_fringe:\n",
    "        fringe_score = jnp.sum(s)\n",
    "        fringe_scores.append(fringe_score)\n",
    "    fringe_scores = jnp.stack(fringe_scores)\n",
    "    fringe_probs = jax.nn.log_softmax(fringe_scores)\n",
    "    sampled_idx = jax.random.categorical(rng_key,fringe_probs)\n",
    "    log_prob = fringe_probs[sampled_idx]\n",
    "    return log_prob, (sampled_idx, contexts_by_fringe)\n",
    "                                                           \n",
    "#takes a goal encoding and samples tactic from network, and returns log prob for gradient \n",
    "def sample_tactic(tactic_params, tac_net, rng_key, goal_endcoding, action_size):\n",
    "    tac_scores = tac_net(tactic_params, rng_key, goal_endcoding, action_size)\n",
    "    tac_scores = jnp.ravel(tac_scores)\n",
    "    tac_probs = jax.nn.log_softmax(tac_scores)\n",
    "    tac_idx = jax.random.categorical(rng_key, tac_probs)\n",
    "    log_prob = tac_probs[tac_idx]\n",
    "    return log_prob, tac_idx\n",
    "\n",
    "def sample_term(term_params, term_net, rng_key, candidates):\n",
    "    term_scores = term_net(term_params, rng_key, candidates)\n",
    "    term_scores = jnp.ravel(term_scores)\n",
    "    term_probs = jax.nn.log_softmax(term_scores)\n",
    "    term_idx = jax.random.categorical(rng_key, term_probs)\n",
    "    log_prob = term_probs[term_idx]\n",
    "    return log_prob, term_idx\n",
    "\n",
    "#function for sampling single argument given previous arguments, \n",
    "def sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, tactic_size, embedding_dim):\n",
    "    hidden, arg_scores = arg_net(arg_params, rng_key, input_, candidates, hidden, tactic_size, embedding_dim)\n",
    "    arg_scores = jnp.ravel(arg_scores)\n",
    "    arg_probs = jax.nn.log_softmax(arg_scores)\n",
    "    arg_idx = jax.random.categorical(rng_key, arg_probs)\n",
    "    log_prob = arg_probs[arg_idx]\n",
    "    return log_prob, (arg_idx, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "german-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_loss(context_params,\n",
    "                 tactic_params,\n",
    "                 term_params,\n",
    "                 arg_params,\n",
    "                 apply_context,\n",
    "                 apply_tac,\n",
    "                 apply_term,\n",
    "                 apply_arg,\n",
    "                 rng_key,\n",
    "                 env,\n",
    "                 encoded_fact_pool,\n",
    "                 candidate_args,\n",
    "                 tactic_pool,\n",
    "                 batch_encoder_):\n",
    "    \n",
    "    log_list = []\n",
    "    discounted_reward_list = []\n",
    "    trace = []\n",
    "    gamma = 0.99\n",
    "    \n",
    "    for i in range(50):\n",
    "        _, rng_key = jax.random.split(rng_key)\n",
    "        \n",
    "        try:\n",
    "            jax_reps, context_set, fringe_sizes = gather_encoded_content(env.history, batch_encoder_)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print (\"Encoder error: {}\".format(e))\n",
    "            if len(log_list) > 0:\n",
    "                return sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)])\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        jax_reps = jnp.stack([jnp.array(jax_reps[i][0].cpu()) for i in range(len(jax_reps))])\n",
    "        logs, reward, action = run_iter(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args, tactic_pool, batch_encoder_)\n",
    "        log_list.append(logs)\n",
    "        discounted_reward_list.append(reward * (gamma ** i))\n",
    "        trace.append((env.history, action))\n",
    "\n",
    "        #if goal proven\n",
    "        if reward == 5:\n",
    "            print (\"Goal proved in {} steps\".format(i+1))\n",
    "            return sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)]), trace\n",
    "        \n",
    "        #timeout\n",
    "        if i == 49:\n",
    "            discounted_reward_list[-1] = -5.\n",
    "            \n",
    "    loss = sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)])\n",
    "    \n",
    "    return loss, trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "powerful-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iter(context_params, \n",
    "             tactic_params, \n",
    "             term_params, \n",
    "             arg_params, \n",
    "             context_net, \n",
    "             tactic_net,\n",
    "             term_net,\n",
    "             arg_net,\n",
    "             jax_reps,\n",
    "             context_set,\n",
    "             fringe_sizes, \n",
    "             rng_key, \n",
    "             env,\n",
    "             encoded_fact_pool,\n",
    "             candidate_args,\n",
    "             tactic_pool,\n",
    "             batch_encoder_):\n",
    "    \n",
    "    log_context, (fringe_idx, contexts_by_fringe) = sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "    \n",
    "    try:\n",
    "        target_context = contexts_by_fringe[fringe_idx][0]\n",
    "    except:\n",
    "        print (\"error {} {}\".format(contexts_by_fringe), fringe_idx)\n",
    "    target_goal = target_context[\"polished\"][\"goal\"]\n",
    "    target_representation = jax_reps[context_set.index(target_context)]\n",
    "    \n",
    "    log_tac, tac_idx = sample_tactic(tactic_params, \n",
    "                                     tactic_net,\n",
    "                                     rng_key,\n",
    "                                     jnp.expand_dims(target_representation,0),\n",
    "                                     len(tactic_pool))\n",
    "    \n",
    "    sampled_tac = tactic_pool[tac_idx]\n",
    "    arg_logs = []\n",
    "\n",
    "    tactic = sampled_tac\n",
    "\n",
    "    #if tactic requires no argument\n",
    "    if sampled_tac in no_arg_tactic:\n",
    "        full_tactic = sampled_tac #tactic_pool[tac]\n",
    "\n",
    "\n",
    "    #Induct_on case; use term policy to find which term to induct on \n",
    "    elif sampled_tac in term_tactic:\n",
    "\n",
    "        goal_tokens = target_goal.split()\n",
    "        term_tokens = [[t] for t in set(goal_tokens) if t[0] == \"V\"]\n",
    "        #add conditional if tokens is empty \n",
    "\n",
    "        #now want encodings for terms from AE\n",
    "\n",
    "        term_reps = []\n",
    "\n",
    "        for term in term_tokens:\n",
    "            term_rep, _ = batch_encoder_.encode([term])\n",
    "            #output is bidirectional so concat vectors\n",
    "            term_reps.append(torch.cat(term_rep.split(1), dim=2).squeeze(0))\n",
    "        \n",
    "        #no terms in expression, only contains literals (e.g. induct_on `0`)\n",
    "        if len(term_reps) == 0:\n",
    "            print (\"No variables to induct on for goal {}\".format(target_goal))\n",
    "            #return negative loss for now (positive overall as negative of log prob is positive)\n",
    "            return 1., -1., \"Induct no vars\" \n",
    "            \n",
    "        # convert to jax\n",
    "        term_reps = jnp.stack([jnp.array(term_reps[i][0].cpu()) for i in range(len(term_reps))])\n",
    "\n",
    "        # now want inputs to term_net to be target_representation (i.e. goal) concatenated with terms\n",
    "        # models the policies conditional dependence of the term given the goal\n",
    "\n",
    "        #stack goal representation for each token\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in term_tokens])\n",
    "\n",
    "        #concat with term encodings to give candidate matrix\n",
    "        candidates = jnp.concatenate([goal_stack, term_reps], 1)\n",
    "\n",
    "        log_term, term_idx = sample_term(term_params, term_net, rng_key, candidates)\n",
    "\n",
    "        sampled_term = term_tokens[term_idx]\n",
    "\n",
    "        tm = sampled_term[0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "    \n",
    "        arg_logs = [log_term]\n",
    "        \n",
    "        if tm:\n",
    "            tactic = \"Induct_on `{}`\".format(tm)\n",
    "        else:\n",
    "            # only to raise an error\n",
    "            tactic = \"Induct_on\"\n",
    "        \n",
    "    #argument tactic\n",
    "    else:\n",
    "        #stack goals to possible arguments to feed into FFN\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in encoded_fact_pool])\n",
    "        candidates = jnp.concatenate([encoded_fact_pool, goal_stack], 1)\n",
    "        \n",
    "        #initial state set as goal\n",
    "        hidden = jnp.expand_dims(target_representation,0)\n",
    "        init_state = hk.LSTMState(hidden,hidden)\n",
    "    \n",
    "        # run through first with tac_idx to initialise state with tactic as c_0\n",
    "        hidden, _ = arg_net(arg_params, rng_key, tac_idx, candidates, init_state, len(tactic_pool), 256)\n",
    "        \n",
    "        ARG_LEN = 5\n",
    "        arg_inds = []\n",
    "        arg_logs = []\n",
    "        input_ = tac_idx\n",
    "        for _ in range(ARG_LEN):\n",
    "            log_arg, (arg_idx, hidden) = sample_arg(arg_params,\n",
    "                                                    arg_net,\n",
    "                                                    rng_key,\n",
    "                                                    input_,\n",
    "                                                    candidates,\n",
    "                                                    hidden,\n",
    "                                                    len(tactic_pool),\n",
    "                                                    256)\n",
    "            arg_logs.append(log_arg)\n",
    "            arg_inds.append(arg_idx)\n",
    "            input_ = jnp.expand_dims(encoded_fact_pool[arg_idx], 0)\n",
    "        \n",
    "        arg = [candidate_args[i] for i in arg_inds]\n",
    "\n",
    "        tactic = env.assemble_tactic(sampled_tac, arg)\n",
    "        \n",
    "    \n",
    "    \n",
    "    action = (int(fringe_idx), 0, tactic)\n",
    "    #print (\"Action {}:\\n\".format(action))\n",
    "    \n",
    "    try:\n",
    "        reward, done = env.step(action)\n",
    "\n",
    "    except:\n",
    "        print(\"Step exception raised.\")\n",
    "        print(\"Handling: {}\".format(env.handling))\n",
    "        print(\"Using: {}\".format(env.using))\n",
    "        # try again\n",
    "        # counter = env.counter\n",
    "        frequency = env.frequency\n",
    "        env.close()\n",
    "        print(\"Aborting current game ...\")\n",
    "        print(\"Restarting environment ...\")\n",
    "        print(env.goal)\n",
    "        env = HolEnv(env.goal)\n",
    "        flag = False\n",
    "        return \n",
    "    \n",
    "    #negative as we want gradient ascent \n",
    "    \n",
    "    logs = (-log_tac - log_context  - sum(arg_logs))\n",
    "\n",
    "    return logs, reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "published-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(goals):\n",
    "    \n",
    "    MORE_TACTICS = True\n",
    "    if not MORE_TACTICS:\n",
    "        thms_tactic = [\"simp\", \"fs\", \"metis_tac\"]\n",
    "        thm_tactic = [\"irule\"]\n",
    "        term_tactic = [\"Induct_on\"]\n",
    "        no_arg_tactic = [\"strip_tac\"]\n",
    "    else:\n",
    "        thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "        thm_tactic = [\"irule\", \"drule\"]\n",
    "        term_tactic = [\"Induct_on\"]\n",
    "        no_arg_tactic = [\"strip_tac\", \"EQ_TAC\"]\n",
    "\n",
    "    tactic_pool = thms_tactic + thm_tactic + term_tactic + no_arg_tactic\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #checkpoint_path = \"models/2020_04_22_20_36_50\" # 91% accuracy model, only core theories\n",
    "    #checkpoint_path = \"models/2020_04_26_20_11_28\" # 95% accuracy model, core theories + integer + sorting\n",
    "    #checkpoint_path = \"models/2020_09_24_23_38_06\" # 98% accuracy model, core theories + integer + sorting | separate theory tokens\n",
    "    #checkpoint_path = \"models/2020_11_28_16_45_10\" # 96-98% accuracy model, core theories + integer + sorting + real | separate theory tokens\n",
    "    #checkpoint_path = \"models/2020_12_04_03_47_22\" # 97% accuracy model, core theories + integer + sorting + real + bag | separate theory tokens\n",
    "\n",
    "    #checkpoint_path = \"models/2021_02_21_15_46_04\" # 98% accuracy model, up to probability theory\n",
    "\n",
    "    checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "\n",
    "    checkpoint = Checkpoint.load(checkpoint_path)\n",
    "    seq2seq = checkpoint.model\n",
    "    input_vocab = checkpoint.input_vocab\n",
    "    output_vocab = checkpoint.output_vocab\n",
    "\n",
    "    batch_encoder_ = BatchPredictor(seq2seq, input_vocab, output_vocab)\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    path_dir = \"/home/sean/Documents/PhD/tactic_zero_jax/env/model_params\"\n",
    "\n",
    "    def save(params, path):\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(params, fp)\n",
    "\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as fp:\n",
    "            return pickle.load(fp)\n",
    "\n",
    "    # with open(\"typed_database.json\") as f:\n",
    "    #     database = json.load(f)\n",
    "\n",
    "    with open(\"include_probability.json\") as f:\n",
    "        database = json.load(f)\n",
    "\n",
    "#     with open(\"polished_def_dict.json\") as f:\n",
    "#         defs = json.load(f)\n",
    "\n",
    "#     fact_pool = list(defs.keys())\n",
    "\n",
    "    encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    rng_key = jax.random.PRNGKey(11)\n",
    "\n",
    "    init_context, apply_context = hk.transform(policy_networks._context_forward)\n",
    "    apply_context = jax.jit(apply_context)\n",
    "\n",
    "    init_tac, apply_tac = hk.transform(policy_networks._tac_forward)\n",
    "    apply_tac = partial(jax.jit, static_argnums=3)(apply_tac)\n",
    "\n",
    "    init_term, apply_term = hk.transform(policy_networks._term_no_tac_forward)\n",
    "    apply_term = jax.jit(apply_term)\n",
    "\n",
    "    init_arg, apply_arg = hk.transform(policy_networks._arg_forward)\n",
    "    apply_arg = partial(jax.jit, static_argnums=(5,6))(apply_arg)\n",
    "\n",
    "    #initialise these with e.g. random uniform, glorot, He etc. should exist outside function for action selection \n",
    "    context_params = init_context(rng_key, jax.random.normal(rng_key, (1,256)))\n",
    "\n",
    "    tactic_params = init_tac(rng_key, jax.random.normal(rng_key, (1,256)), len(tactic_pool))\n",
    "\n",
    "    #term_policy for now is only considering variables for induction, hence does not need any arguments \n",
    "    term_params = init_term(rng_key, jax.random.normal(rng_key, (1,512)))\n",
    "\n",
    "    hidden = jax.random.normal(rng_key, (1,256))\n",
    "\n",
    "    init_state = hk.LSTMState(hidden, hidden)\n",
    "\n",
    "    arg_params = init_arg(rng_key, \n",
    "                          jax.random.randint(rng_key, (), 0, len(tactic_pool)), \n",
    "                          jax.random.normal(rng_key, (1,512)), init_state, len(tactic_pool), 256)\n",
    "\n",
    "        \n",
    "    context_lr = 1e-4\n",
    "    tactic_lr = 1e-4\n",
    "    arg_lr = 1e-4\n",
    "    term_lr = 1e-4\n",
    "\n",
    "    context_optimiser = optax.rmsprop(context_lr)\n",
    "    tactic_optimiser = optax.rmsprop(tactic_lr)\n",
    "    arg_optimiser = optax.rmsprop(arg_lr)\n",
    "    term_optimiser = optax.rmsprop(term_lr)\n",
    "\n",
    "    opt_state_context = context_optimiser.init(context_params)\n",
    "    opt_state_tactic = tactic_optimiser.init(tactic_params)\n",
    "    opt_state_arg = arg_optimiser.init(arg_params)\n",
    "    opt_state_term = term_optimiser.init(term_params)\n",
    "    \n",
    "    proof_dict = {}\n",
    "    \n",
    "    for goal in goals:\n",
    "        g = goal[1]\n",
    "            \n",
    "        env = HolEnv(g)\n",
    "\n",
    "        theories = re.findall(r'C\\$(\\w+)\\$ ', goal[0])\n",
    "        theories = set(theories)\n",
    "        theories = list(theories)\n",
    "\n",
    "        allowed_theories = theories\n",
    "\n",
    "        goal_theory = g\n",
    "\n",
    "        #print (\"Target goal: {}\".format(g))\n",
    "        \n",
    "        try:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            goal_theory = g#database[polished_goal][0] # plain_database[goal][0]\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories and (database[t][0] != goal_theory or int(database[t][2]) < int(database[polished_goal][2])):\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "\n",
    "            env.toggle_simpset(\"diminish\", goal_theory)\n",
    "            #print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "        except:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories:\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "            #print(\"Theorem not found in database.\")\n",
    "\n",
    "        encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "        encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids))\n",
    "        \n",
    "        encoded_fact_pool = jnp.array(encoded_fact_pool)\n",
    "\n",
    "        try:\n",
    "            gradients, trace = jax.grad(episode_loss, argnums=(0,1,2,3), has_aux=True)(context_params,\n",
    "                                                                                       tactic_params,\n",
    "                                                                                       term_params,\n",
    "                                                                                       arg_params, \n",
    "                                                                                       apply_context,\n",
    "                                                                                       apply_tac,\n",
    "                                                                                       apply_term,\n",
    "                                                                                       apply_arg,\n",
    "                                                                                       rng_key,\n",
    "                                                                                       env,\n",
    "                                                                                       encoded_fact_pool,\n",
    "                                                                                       candidate_args,\n",
    "                                                                                       tactic_pool,\n",
    "                                                                                       batch_encoder_)\n",
    "        except Exception as e:\n",
    "            print (\"Error: {}\".format(e))\n",
    "            continue\n",
    "        \n",
    "        proof_dict[goal] = trace\n",
    "            \n",
    "\n",
    "        #update parameters\n",
    "        context_updates, opt_state_context = context_optimiser.update(gradients[0], opt_state_context)\n",
    "        context_params = optax.apply_updates(context_params, context_updates)\n",
    "\n",
    "        tactic_updates, opt_state_tactic = tactic_optimiser.update(gradients[1], opt_state_tactic)\n",
    "        tactic_params = optax.apply_updates(tactic_params, tactic_updates)\n",
    "\n",
    "        term_updates, opt_state_term = term_optimiser.update(gradients[2], opt_state_term)\n",
    "        term_params = optax.apply_updates(term_params, term_updates)\n",
    "\n",
    "        arg_updates, opt_state_arg = arg_optimiser.update(gradients[3], opt_state_arg)\n",
    "        arg_params = optax.apply_updates(arg_params, arg_updates)\n",
    "\n",
    "        #save trace \n",
    "        save(proof_dict, path_dir + \"/trace\")\n",
    "        \n",
    "        #save params after each proof attempt\n",
    "        save(context_params, path_dir + \"/context_params\")\n",
    "        save(opt_state_context, path_dir+\"/context_state\")\n",
    "        save(tactic_params, path_dir+\"/tactic_params\")\n",
    "        save(opt_state_tactic, path_dir+\"/tactic_state\")\n",
    "        save(term_params, path_dir+\"/term_params\")\n",
    "        save(opt_state_term, path_dir+\"/term_state\")\n",
    "        save(arg_params, path_dir+\"/arg_params\")\n",
    "        save(opt_state_arg, path_dir+\"/arg_state\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupational-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_THEORIES = [\"bool\", \"min\", \"list\"]\n",
    "# GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "\n",
    "# train(GOALS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
