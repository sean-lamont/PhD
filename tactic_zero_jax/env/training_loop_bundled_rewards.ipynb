{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considerable-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from jax import random\n",
    "import sys\n",
    "from time import sleep\n",
    "import json\n",
    "import pexpect\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "import seq2seq\n",
    "from batch_predictor import BatchPredictor\n",
    "from checkpoint import Checkpoint\n",
    "\n",
    "from policy_networks import *\n",
    "import policy_networks\n",
    "\n",
    "from new_env import *\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True) \n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "from jax import lax\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intelligent-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path_dir = \"/home/sean/Documents/PhD/tactic_zero_jax/env/model_params\"\n",
    "\n",
    "def save(params, path):\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(params, fp)\n",
    "\n",
    "def load(path):\n",
    "    with open(path, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "necessary-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOLPATH = \"/home/sean/Documents/hol/HOL/bin/hol --maxheap=256\"\n",
    "HOLPATH = \"/home/sean/Documents/PhD/HOL4/HOL/bin/hol --maxheap=256\"\n",
    "\n",
    "#tactic_zero_path = \"/home/sean/Documents/PhD/git/repo/PhD/tacticzero/holgym/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desperate-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"typed_database.json\") as f:\n",
    "    database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liked-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "MORE_TACTICS = True\n",
    "if not MORE_TACTICS:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\"]\n",
    "    thm_tactic = [\"irule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\"]\n",
    "else:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "    thm_tactic = [\"irule\", \"drule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\", \"EQ_TAC\"]\n",
    "    \n",
    "tactic_pool = thms_tactic + thm_tactic + term_tactic + no_arg_tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "textile-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Move to another file \n",
    "\n",
    "def get_polish(raw_goal):\n",
    "        goal = construct_goal(raw_goal)\n",
    "        process.sendline(goal.encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer (HOLPP.add_string o pt);\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"top_goals();\".encode(\"utf-8\"))\n",
    "        process.expect(\"val it =\")\n",
    "        process.expect([\": goal list\", \":\\r\\n +goal list\"])\n",
    "\n",
    "        polished_raw = process.before.decode(\"utf-8\")\n",
    "        polished_subgoals = re.sub(\"“|”\",\"\\\"\", polished_raw)\n",
    "        polished_subgoals = re.sub(\"\\r\\n +\",\" \", polished_subgoals)\n",
    "\n",
    "        pd = eval(polished_subgoals)\n",
    "        \n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"drop();\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer default_pt;\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "\n",
    "        data = [{\"polished\":{\"assumptions\": e[0][0], \"goal\":e[0][1]},\n",
    "                 \"plain\":{\"assumptions\": e[1][0], \"goal\":e[1][1]}}\n",
    "                for e in zip(pd, [([], raw_goal)])]\n",
    "        return data \n",
    "    \n",
    "def construct_goal(goal):\n",
    "    s = \"g \" + \"`\" + goal + \"`;\"\n",
    "    return s\n",
    "\n",
    "def gather_encoded_content_(history, encoder):\n",
    "    fringe_sizes = []\n",
    "    contexts = []\n",
    "    reverted = []\n",
    "    for i in history:\n",
    "        c = i[\"content\"]\n",
    "        contexts.extend(c)\n",
    "        fringe_sizes.append(len(c))\n",
    "    for e in contexts:\n",
    "        g = revert_with_polish(e)\n",
    "        reverted.append(g.strip().split())\n",
    "    out = []\n",
    "    sizes = []\n",
    "    for goal in reverted:\n",
    "        out_, sizes_ = encoder.encode([goal])\n",
    "        out.append(torch.cat(out_.split(1), dim=2).squeeze(0))\n",
    "        sizes.append(sizes_)\n",
    "\n",
    "    representations = out\n",
    "\n",
    "    return representations, contexts, fringe_sizes\n",
    "\n",
    "def parse_theory(pg):\n",
    "    theories = re.findall(r'C\\$(\\w+)\\$ ', pg)\n",
    "    theories = set(theories)\n",
    "    for th in EXCLUDED_THEORIES:\n",
    "        theories.discard(th)\n",
    "    return list(theories)\n",
    "\n",
    "def revert_with_polish(context):\n",
    "    target = context[\"polished\"]\n",
    "    assumptions = target[\"assumptions\"]\n",
    "    goal = target[\"goal\"]\n",
    "    for i in reversed(assumptions): \n",
    "        #goal = \"@ @ D$min$==> {} {}\".format(i, goal)\n",
    "        goal = \"@ @ C$min$ ==> {} {}\".format(i, goal)\n",
    "\n",
    "    return goal \n",
    "\n",
    "def split_by_fringe(goal_set, goal_scores, fringe_sizes):\n",
    "    # group the scores by fringe\n",
    "    fs = []\n",
    "    gs = []\n",
    "    counter = 0\n",
    "    for i in fringe_sizes:\n",
    "        end = counter + i\n",
    "        fs.append(goal_scores[counter:end])\n",
    "        gs.append(goal_set[counter:end])\n",
    "        counter = end\n",
    "    return gs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capital-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇒ LIST_REL R (l1 ++ l3) (l2 ++ l4)\n"
     ]
    }
   ],
   "source": [
    "with open(\"include_probability.json\") as f:\n",
    "    database = json.load(f)\n",
    "\n",
    "with open(\"polished_def_dict.json\") as f:\n",
    "    defs = json.load(f)\n",
    "\n",
    "fact_pool = list(defs.keys())\n",
    "\n",
    "encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "TARGET_THEORIES = [\"bool\", \"min\", \"list\"]\n",
    "GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "print (GOALS[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forbidden-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'seq2seq.models.EncoderRNN.EncoderRNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'seq2seq.models.DecoderRNN.DecoderRNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/sean/Documents/venvs/jax/lib/python3.9/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_path = \"models/2020_04_22_20_36_50\" # 91% accuracy model, only core theories\n",
    "#checkpoint_path = \"models/2020_04_26_20_11_28\" # 95% accuracy model, core theories + integer + sorting\n",
    "#checkpoint_path = \"models/2020_09_24_23_38_06\" # 98% accuracy model, core theories + integer + sorting | separate theory tokens\n",
    "#checkpoint_path = \"models/2020_11_28_16_45_10\" # 96-98% accuracy model, core theories + integer + sorting + real | separate theory tokens\n",
    "#checkpoint_path = \"models/2020_12_04_03_47_22\" # 97% accuracy model, core theories + integer + sorting + real + bag | separate theory tokens\n",
    "\n",
    "#checkpoint_path = \"models/2021_02_21_15_46_04\" # 98% accuracy model, up to probability theory\n",
    "\n",
    "checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "\n",
    "checkpoint = Checkpoint.load(checkpoint_path)\n",
    "seq2seq = checkpoint.model\n",
    "input_vocab = checkpoint.input_vocab\n",
    "output_vocab = checkpoint.output_vocab\n",
    "\n",
    "batch_encoder_ = BatchPredictor(seq2seq, input_vocab, output_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "virtual-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to give the log probability of pi(f | s) so gradient can be computed directly\n",
    "#also returns sampled index and contexts to determine goal to give tactic network\n",
    "def sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes):\n",
    "    context_scores = context_net(context_params, rng_key, jax_reps)\n",
    "    contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "    fringe_scores = []\n",
    "    for s in scores_by_fringe:\n",
    "        fringe_score = jnp.sum(s)\n",
    "        fringe_scores.append(fringe_score)\n",
    "    #TODO some fringes can be empty, but still give value 0 which assigns nonzero probability?\n",
    "    fringe_scores = jnp.stack(fringe_scores)\n",
    "    fringe_probs = jax.nn.log_softmax(fringe_scores)\n",
    "\n",
    "    #samples, gives an index (looks like it does gumbel softmax under the hood to keep differentiability?)\n",
    "    sampled_idx = jax.random.categorical(rng_key,fringe_probs)\n",
    "\n",
    "    log_prob = fringe_probs[sampled_idx]\n",
    "    #log_prob = jnp.log(prob)\n",
    "    return log_prob, (sampled_idx, contexts_by_fringe)\n",
    "                                                           \n",
    "#grad_log_context, (fringe_idx, contexts_by_fringe) = jax.grad(sample_fringe, has_aux=True)(context_params, apply_context, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "\n",
    "#takes a goal encoding and samples tactic from network, and returns log prob for gradient \n",
    "def sample_tactic(tactic_params, tac_net, rng_key, goal_endcoding, action_size=len(tactic_pool)):\n",
    "    tac_scores = tac_net(tactic_params, rng_key, goal_endcoding, action_size)\n",
    "    tac_scores = jnp.ravel(tac_scores)\n",
    "    #tac_scores = tac_scores - max(tac_scores)\n",
    "    #print (tac_scores)\n",
    "    #subtract max element for numerical stability \n",
    "    tac_probs = jax.nn.log_softmax(tac_scores)\n",
    "    tac_idx = jax.random.categorical(rng_key, tac_probs)\n",
    "    log_prob = tac_probs[tac_idx]#jnp.log(tac_probs[tac_idx])\n",
    "    #print (jnp.exp(log_prob).primal)#, jnp.exp(tac_probs), rng_key)\n",
    "    return log_prob, tac_idx\n",
    "\n",
    "#grad_log_tac, tac_idx = jax.grad(sample_tactic, has_aux=True)(tactic_params, apply_tac, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "\n",
    "#sampled_tac = tactic_pool[tac_idx]\n",
    "\n",
    "def sample_term(term_params, term_net, rng_key, candidates):\n",
    "    term_scores = term_net(term_params, rng_key, candidates)\n",
    "    term_scores = jnp.ravel(term_scores)\n",
    "    term_probs = jax.nn.log_softmax(term_scores)\n",
    "    term_idx = jax.random.categorical(rng_key, term_probs)\n",
    "    log_prob = term_probs[term_idx]#jnp.log(term_probs[term_idx])\n",
    "    return log_prob, term_idx\n",
    "\n",
    "#grad_log_term, term_idx = jax.grad(sample_term, has_aux=True)(term_params, apply_term, rng_key, candidates)#, tac_idx, len(tactic_pool), candidates.shape[1])\n",
    "\n",
    "#function for sampling single argument given previous arguments, \n",
    "def sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, tactic_size, embedding_dim):\n",
    "    hidden, arg_scores = arg_net(arg_params, rng_key, input_, candidates, hidden, tactic_size, embedding_dim)\n",
    "    arg_scores = jnp.ravel(arg_scores)\n",
    "    arg_probs = jax.nn.log_softmax(arg_scores)\n",
    "    arg_idx = jax.random.categorical(rng_key, arg_probs)\n",
    "    log_prob = arg_probs[arg_idx]#jnp.log(arg_probs[arg_idx])\n",
    "    return log_prob, (arg_idx, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "german-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_loss(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg, rng_key, env, encoded_fact_pool, candidate_args):\n",
    "    log_list = []\n",
    "    discounted_reward_list = []\n",
    "    trace = []\n",
    "    gamma = 0.99\n",
    "    for i in range(3):\n",
    "        _, rng_key = jax.random.split(rng_key)\n",
    "        \n",
    "        #print (\"Proof step {} of 50\\n\".format(i+1))\n",
    "        \n",
    "        try:\n",
    "            jax_reps, context_set, fringe_sizes = gather_encoded_content_(env.history, batch_encoder_)\n",
    "            #convert to jax\n",
    "        except:\n",
    "            print (\"Encoder error\")\n",
    "            if len(log_list) > 0:\n",
    "                return sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)])\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        jax_reps = jnp.stack([jnp.array(jax_reps[i][0].cpu()) for i in range(len(jax_reps))])\n",
    "        logs, reward, action = run_iter(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args)\n",
    "\n",
    "        log_list.append(logs)\n",
    "        discounted_reward_list.append(reward * (gamma ** i))\n",
    "        \n",
    "        trace.append((env.history, action))\n",
    "\n",
    "                \n",
    "        #if goal proven\n",
    "        if reward == 5:\n",
    "            print (\"Goal proved in {} steps\".format(i+1))\n",
    "            return sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)]), trace\n",
    "        \n",
    "        #timeout\n",
    "        if i == 49:\n",
    "            discounted_reward_list[-1] = -5.\n",
    "            \n",
    "    loss = sum([i[0] * i[1] for i in zip(log_list, discounted_reward_list)])\n",
    "    \n",
    "    return loss, trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "powerful-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iter(context_params, tactic_params, term_params, arg_params, context_net, tactic_net, term_net, arg_net, jax_reps, context_set, fringe_sizes, rng_key, env, encoded_fact_pool, candidate_args):\n",
    "    \n",
    "    log_context, (fringe_idx, contexts_by_fringe) = sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "    \n",
    "    try:\n",
    "        target_context = contexts_by_fringe[fringe_idx][0]\n",
    "    except:\n",
    "        print (\"error {} {}\".format(contexts_by_fringe), fringe_idx)\n",
    "    target_goal = target_context[\"polished\"][\"goal\"]\n",
    "    target_representation = jax_reps[context_set.index(target_context)]\n",
    "    \n",
    "    log_tac, tac_idx = sample_tactic(tactic_params, tactic_net, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "    \n",
    "    sampled_tac = tactic_pool[tac_idx]\n",
    "    arg_logs = []\n",
    "\n",
    "    tactic = sampled_tac\n",
    "    #for testing\n",
    "    \n",
    "    #sampled_tac = \"Induct_on\"\n",
    "\n",
    "    #if tactic requires no argument\n",
    "    if sampled_tac in no_arg_tactic:\n",
    "        full_tactic = sampled_tac #tactic_pool[tac]\n",
    "\n",
    "\n",
    "    #Induct_on case; use term policy to find which term to induct on \n",
    "    elif sampled_tac in term_tactic:\n",
    "\n",
    "        goal_tokens = target_goal.split()\n",
    "        term_tokens = [[t] for t in set(goal_tokens) if t[0] == \"V\"]\n",
    "        #add conditional if tokens is empty \n",
    "\n",
    "        #now want encodings for terms from AE\n",
    "\n",
    "        term_reps = []\n",
    "\n",
    "        for term in term_tokens:\n",
    "            term_rep, _ = batch_encoder_.encode([term])\n",
    "            #output is bidirectional so concat vectors\n",
    "            term_reps.append(torch.cat(term_rep.split(1), dim=2).squeeze(0))\n",
    "        \n",
    "        #no terms in expression, only contains literals (e.g. induct_on `0`)\n",
    "        if len(term_reps) == 0:\n",
    "            print (\"No variables to induct on for goal {}\".format(target_goal))\n",
    "            #return negative loss for now (positive overall as negative of log prob is positive)\n",
    "            return 1., -1., \"Induct no vars\"\n",
    "            \n",
    "            \n",
    "        # convert to jax\n",
    "        term_reps = jnp.stack([jnp.array(term_reps[i][0].cpu()) for i in range(len(term_reps))])\n",
    "\n",
    "        # now want inputs to term_net to be target_representation (i.e. goal) concatenated with terms\n",
    "        # models the policies conditional dependence of the term given the goal\n",
    "\n",
    "        #stack goal representation for each token\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in term_tokens])\n",
    "\n",
    "        #concat with term encodings to give candidate matrix\n",
    "        candidates = jnp.concatenate([goal_stack, term_reps], 1)\n",
    "\n",
    "        log_term, term_idx = sample_term(term_params, term_net, rng_key, candidates)\n",
    "\n",
    "        sampled_term = term_tokens[term_idx]\n",
    "\n",
    "        tm = sampled_term[0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "    \n",
    "        arg_logs = [log_term]\n",
    "        \n",
    "        if tm:\n",
    "            tactic = \"Induct_on `{}`\".format(tm)\n",
    "        else:\n",
    "            # only to raise an error\n",
    "            tactic = \"Induct_on\"\n",
    "        \n",
    "    #argument tactic\n",
    "    else:\n",
    "        #stack goals to possible arguments to feed into FFN\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in encoded_fact_pool])\n",
    "        candidates = jnp.concatenate([encoded_fact_pool, goal_stack], 1)\n",
    "        \n",
    "        #initial state set as goal\n",
    "        hidden = jnp.expand_dims(target_representation,0)\n",
    "        init_state = hk.LSTMState(hidden,hidden)\n",
    "    \n",
    "        # run through first with tac_idx to initialise state with tactic as c_0\n",
    "        hidden, _ = arg_net(arg_params, rng_key, tac_idx, candidates, init_state, len(tactic_pool), 256)\n",
    "        \n",
    "        ARG_LEN = 5\n",
    "        arg_inds = []\n",
    "        arg_logs = []\n",
    "        input_ = tac_idx\n",
    "        \n",
    "    \n",
    "        \n",
    "        for _ in range(ARG_LEN):\n",
    "            log_arg, (arg_idx, hidden) = sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, len(tactic_pool), 256)\n",
    "            arg_logs.append(log_arg)\n",
    "            arg_inds.append(arg_idx)\n",
    "            input_ = jnp.expand_dims(encoded_fact_pool[arg_idx], 0)\n",
    "        \n",
    "        arg = [candidate_args[i] for i in arg_inds]\n",
    "\n",
    "        tactic = env.assemble_tactic(sampled_tac, arg)\n",
    "        \n",
    "    \n",
    "    \n",
    "    action = (int(fringe_idx), 0, tactic)\n",
    "    #print (\"Action {}:\\n\".format(action))\n",
    "    \n",
    "    try:\n",
    "        reward, done = env.step(action)\n",
    "\n",
    "    except:\n",
    "        print(\"Step exception raised.\")\n",
    "        # print(\"Fringe: {}\".format(env.history))\n",
    "        print(\"Handling: {}\".format(env.handling))\n",
    "        print(\"Using: {}\".format(env.using))\n",
    "        # try again\n",
    "        # counter = env.counter\n",
    "        frequency = env.frequency\n",
    "        env.close()\n",
    "        print(\"Aborting current game ...\")\n",
    "        print(\"Restarting environment ...\")\n",
    "        print(env.goal)\n",
    "        env = HolEnv(env.goal)\n",
    "        flag = False\n",
    "        return \n",
    "        \n",
    "    #print (\"Result: Reward {}\".format(reward))#, env.history[-1]))\n",
    "\n",
    "    \n",
    "    #negative as we want gradient ascent \n",
    "    \n",
    "    logs = (-log_tac - log_context  - sum(arg_logs))\n",
    "\n",
    "    return logs, reward, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "published-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(goals):\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(11)\n",
    "\n",
    "    init_context, apply_context = hk.transform(policy_networks._context_forward)\n",
    "    #apply_context = jax.jit(apply_context)\n",
    "\n",
    "    init_tac, apply_tac = hk.transform(policy_networks._tac_forward)\n",
    "    #apply_tac = partial(jax.jit, static_argnums=3)(apply_tac)\n",
    "\n",
    "    init_term, apply_term = hk.transform(policy_networks._term_no_tac_forward)\n",
    "    #apply_term = jax.jit(apply_term)\n",
    "\n",
    "    init_arg, apply_arg = hk.transform(policy_networks._arg_forward)\n",
    "    #apply_arg = partial(jax.jit, static_argnums=(5, 6))(apply_arg)\n",
    "\n",
    "    #initialise these with e.g. random uniform, glorot, He etc. should exist outside function for action selection \n",
    "    context_params = init_context(rng_key, jax.random.normal(rng_key, (1,256)))\n",
    "\n",
    "    tactic_params = init_tac(rng_key, jax.random.normal(rng_key, (1,256)), len(tactic_pool))\n",
    "\n",
    "    #term_policy for now is only considering variables for induction, hence does not need any arguments \n",
    "    term_params = init_term(rng_key, jax.random.normal(rng_key, (1,512)))\n",
    "\n",
    "    hidden = jax.random.normal(rng_key, (1,256))\n",
    "\n",
    "    init_state = hk.LSTMState(hidden, hidden)\n",
    "\n",
    "    arg_params = init_arg(rng_key, jax.random.randint(rng_key, (), 0, len(tactic_pool)), jax.random.normal(rng_key, (1,512)), init_state, len(tactic_pool), 256)\n",
    "\n",
    "        \n",
    "    context_lr = 1e-4\n",
    "    tactic_lr = 1e-4\n",
    "    arg_lr = 1e-4\n",
    "    term_lr = 1e-4\n",
    "\n",
    "    context_optimiser = optax.rmsprop(context_lr)\n",
    "    tactic_optimiser = optax.rmsprop(tactic_lr)\n",
    "    arg_optimiser = optax.rmsprop(arg_lr)\n",
    "    term_optimiser = optax.rmsprop(term_lr)\n",
    "\n",
    "    opt_state_context = context_optimiser.init(context_params)\n",
    "    opt_state_tactic = tactic_optimiser.init(tactic_params)\n",
    "    opt_state_arg = arg_optimiser.init(arg_params)\n",
    "    opt_state_term = term_optimiser.init(term_params)\n",
    "    \n",
    "    proof_dict = {}\n",
    "    \n",
    "    for goal in goals[:2]:\n",
    "        start_time = time.time()\n",
    "\n",
    "        g = goal[1]\n",
    "            \n",
    "        env = HolEnv(g)\n",
    "\n",
    "        theories = re.findall(r'C\\$(\\w+)\\$ ', goal[0])\n",
    "        theories = set(theories)\n",
    "        theories = list(theories)\n",
    "\n",
    "        allowed_theories = theories\n",
    "\n",
    "        goal_theory = g\n",
    "\n",
    "        #print (\"Target goal: {}\".format(g))\n",
    "        \n",
    "        try:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            goal_theory = g#database[polished_goal][0] # plain_database[goal][0]\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories and (database[t][0] != goal_theory or int(database[t][2]) < int(database[polished_goal][2])):\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "\n",
    "            env.toggle_simpset(\"diminish\", goal_theory)\n",
    "            #print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "        except:\n",
    "            allowed_arguments_ids = []\n",
    "            candidate_args = []\n",
    "            for i,t in enumerate(database):\n",
    "                if database[t][0] in allowed_theories:\n",
    "                    allowed_arguments_ids.append(i)\n",
    "                    candidate_args.append(t)\n",
    "            #print(\"Theorem not found in database.\")\n",
    "\n",
    "        #print (\"Number of candidate facts to use: {}\".format(len(candidate_args)))\n",
    "\n",
    "        encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "        encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids))\n",
    "        \n",
    "        encoded_fact_pool = jnp.array(encoded_fact_pool)\n",
    "        \n",
    "        \n",
    "        gradients, trace = jax.grad(episode_loss, argnums=(0,1,2,3), has_aux=True)(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg,  rng_key, env, encoded_fact_pool, candidate_args)\n",
    "\n",
    "#         try:\n",
    "#             gradients, trace = jax.grad(episode_loss, argnums=(0,1,2,3), has_aux=True)(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg,  rng_key, env, encoded_fact_pool, candidate_args)\n",
    "#         except Exception as e:\n",
    "#             print (\"Error: {}\".format(e))\n",
    "#             continue\n",
    "        \n",
    "\n",
    "        proof_dict[goal] = trace\n",
    "            \n",
    "\n",
    "        #update parameters\n",
    "        context_updates, opt_state_context = context_optimiser.update(gradients[0], opt_state_context)\n",
    "        context_params = optax.apply_updates(context_params, context_updates)\n",
    "\n",
    "        tactic_updates, opt_state_tactic = tactic_optimiser.update(gradients[1], opt_state_tactic)\n",
    "        tactic_params = optax.apply_updates(tactic_params, tactic_updates)\n",
    "\n",
    "        term_updates, opt_state_term = term_optimiser.update(gradients[2], opt_state_term)\n",
    "        term_params = optax.apply_updates(term_params, term_updates)\n",
    "\n",
    "        arg_updates, opt_state_arg = arg_optimiser.update(gradients[3], opt_state_arg)\n",
    "        arg_params = optax.apply_updates(arg_params, arg_updates)\n",
    "        \n",
    "        print (\"Time for proof attempt: {}s\".format(str(round(time.time() - start_time, 2))))\n",
    "\n",
    "#         #save trace \n",
    "#         save(proof_dict, path_dir + \"/trace\")\n",
    "        \n",
    "#         #save params after each proof attempt\n",
    "#         save(context_params, path_dir + \"/context_params\")\n",
    "#         save(opt_state_context, path_dir+\"/context_state\")\n",
    "#         save(tactic_params, path_dir+\"/tactic_params\")\n",
    "#         save(opt_state_tactic, path_dir+\"/tactic_state\")\n",
    "#         save(term_params, path_dir+\"/term_params\")\n",
    "#         save(opt_state_term, path_dir+\"/term_state\")\n",
    "#         save(arg_params, path_dir+\"/arg_params\")\n",
    "#         save(opt_state_arg, path_dir+\"/arg_state\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupational-dancing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing theories...\n",
      "Loading modules...\n",
      "Configuration done.\n",
      "Removing simp lemmas from LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇒ LIST_REL R (l1 ++ l3) (l2 ++ l4)\n",
      "Time for proof attempt: 21.25s\n",
      "Importing theories...\n",
      "Loading modules...\n",
      "Configuration done.\n",
      "Removing simp lemmas from LIST_REL (R :α -> β -> bool) (l1 :α list) (l2 :β list) ∧ LIST_REL R (l3 :α list) (l4 :β list) ⇔ LIST_REL R (l1 ++ l3) (l2 ++ l4) ∧ LENGTH l1 = LENGTH l2 ∧ LENGTH l3 = LENGTH l4\n",
      "Time for proof attempt: 12.94s\n"
     ]
    }
   ],
   "source": [
    "# TARGET_THEORIES = [\"bool\"]\n",
    "# GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "\n",
    "train(GOALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "social-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:23\n",
    "# 2:27 unrolling args manually\n",
    "# 0:38 no jit for apply arg\n",
    "# 0:36 no jit for apply tac \n",
    "# 0:36 no jit for all\n",
    "# 0:12 expand_dims bug fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "organizational-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cProfile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "determined-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cProfile.run('train(GOALS)', sort='cumulative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "forbidden-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  vjp, linearize, expand dims , sample_arg,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "closed-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## from jax import vmap\n",
    "\n",
    "# r = jax.random.PRNGKey(1)\n",
    "# test = jax.random.normal(r, (612,256))\n",
    "# test\n",
    "\n",
    "# %time h = jnp.concatenate([jnp.expand_dims(test[0],0) for _ in range(1000)])\n",
    "\n",
    "# h.shape\n",
    "\n",
    "# y = jnp.expand_dims(test[0], 0)\n",
    "# %time k = jnp.concatenate([y for _ in range(1000)])\n",
    "\n",
    "# k.shape\n",
    "\n",
    "# new_arr = jnp.empty((1000, y.shape[1]))\n",
    "# for i in range(new_arr.shape[1]):\n",
    "#     new_arr = new_arr.at[i].set(y[0])\n",
    "\n",
    "# import numpy as np\n",
    "# z = np.array(y)\n",
    "\n",
    "# z = np.zeros((1000, y.shape[1]))\n",
    "\n",
    "# z.shape\n",
    "# for i in range(z.shape[0]):\n",
    "#     z[i] = y\n",
    "\n",
    "# # y\n",
    "\n",
    "# y = jnp.expand_dims(test[0], 0)\n",
    "\n",
    "\n",
    "# def concat(x, n):\n",
    "#     ret = np.zeros((n, x.shape[1]))\n",
    "#     for i in range(n):\n",
    "#         ret[i] = x\n",
    "#     return jnp.array(ret)\n",
    "\n",
    "# %time w = jax.jit(conc(y, 10000)\n",
    "# %time k = jnp.concatenate([y for _ in range(10000)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-aspect",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
