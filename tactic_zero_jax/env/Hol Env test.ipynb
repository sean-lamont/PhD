{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constitutional-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "import sys\n",
    "from time import sleep\n",
    "import json\n",
    "import pexpect\n",
    "import re\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "working-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLPATH = \"/home/sean/Documents/PhD/HOL4/HOL/bin/hol --maxheap=256\"\n",
    "TARGET_THEORIES = [\"pred_set\"] #[\"arithmetic\"]#[\"list\"] #[\"rich_list\"] # [\"integer\"] #[\"arithmetic\"] # [\"rich_list\"] #[\"pred_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "welcome-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"typed_database.json\") as f:\n",
    "    database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "shared-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goals = [value[4] for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "logical-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polish(raw_goal):\n",
    "        goal = construct_goal(raw_goal)\n",
    "        process.sendline(goal.encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer (HOLPP.add_string o pt);\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"top_goals();\".encode(\"utf-8\"))\n",
    "        process.expect(\"val it =\")\n",
    "        process.expect([\": goal list\", \":\\r\\n +goal list\"])\n",
    "\n",
    "        polished_raw = process.before.decode(\"utf-8\")\n",
    "        polished_subgoals = re.sub(\"“|”\",\"\\\"\", polished_raw)\n",
    "        polished_subgoals = re.sub(\"\\r\\n +\",\" \", polished_subgoals)\n",
    "\n",
    "        # print(\"content:{}\".format(subgoals))\n",
    "        # exit()\n",
    "        pd = eval(polished_subgoals)\n",
    "        \n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"drop();\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer default_pt;\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "\n",
    "        data = [{\"polished\":{\"assumptions\": e[0][0], \"goal\":e[0][1]},\n",
    "                 \"plain\":{\"assumptions\": e[1][0], \"goal\":e[1][1]}}\n",
    "                for e in zip(pd, [([], raw_goal)])]\n",
    "        return data # list(zip(pd, [([], raw_goal)]))f\n",
    "\n",
    "def construct_goal(goal):\n",
    "    s = \"g \" + \"`\" + goal + \"`;\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thermal-tenant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing theories...\n"
     ]
    }
   ],
   "source": [
    "process = pexpect.spawn(HOLPATH)\n",
    "\n",
    "theories = [\"listTheory\", \"bossLib\"]\n",
    "\n",
    "print(\"Importing theories...\")\n",
    "process.sendline(\"val _ = HOL_Interactive.toggle_quietdec();\".encode(\"utf-8\"))\n",
    "for i in theories:\n",
    "    process.sendline(\"open {};\".format(i).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "soviet-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # remove built-in simp lemmas\n",
    "# print(\"Removing simp lemmas...\")\n",
    "# process.sendline(\"delsimps [\\\"HD\\\", \\\"EL_restricted\\\", \\\"EL_simp_restricted\\\"];\")\n",
    "# #process.sendline(\"delsimps {};\".format(dels))\n",
    "# #process.sendline(\"delsimps {};\".format(dels2))\n",
    "# # process.sendline(\"delsimps {};\".format(dels3))\n",
    "# sleep(4)\n",
    "# # load utils\n",
    "# print(\"Loading modules...\")\n",
    "# process.sendline(\"use \\\"helper.sml\\\";\")\n",
    "# sleep(5)\n",
    "# # process.sendline(\"val _ = load \\\"Timeout\\\";\")\n",
    "# print(\"Configuration done.\")\n",
    "# process.expect('\\r\\n>')\n",
    "# # process.readline()\n",
    "# process.sendline(\"val _ = HOL_Interactive.toggle_quietdec();\".encode(\"utf-8\"))\n",
    "\n",
    "# # consumes hol4 head\n",
    "# process.expect('\\r\\n>')\n",
    "# #print (process.read())\n",
    "# # setup the goal\n",
    "# goal = goals[0]\n",
    "                      \n",
    "#  # a pair of polished goal list and original goal list\n",
    "# fringe = get_polish(goal)\n",
    "\n",
    "# #scripts = []\n",
    "# goal = construct_goal(goal)\n",
    "# process.sendline(goal.encode(\"utf-8\"))        \n",
    "# print(\"Initialization done. Main goal is:\\n{}.\".format(goal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "southwest-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (fringe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "metropolitan-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "tactic_zero_path = \"/home/sean/Documents/PhD/git/repo/PhD/tacticzero/holgym/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spatial-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = [{'content': [{'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'}, 'plain': {'assumptions': [], 'goal': 'REVERSE l = [] ⇔ l = []'}}], 'parent': None, 'goal': None, 'by_tactic': '', 'reward': None}, {'content': [{'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Clist$NIL Clist$NIL @ @ Cmin$= Clist$NIL Clist$NIL'}, 'plain': {'assumptions': [], 'goal': 'REVERSE [] = [] ⇔ [] = []'}}, {'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}, 'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}}], 'parent': 0, 'goal': 0, 'by_tactic': 'Induct_on `l`', 'reward': 0.1}, {'content': [{'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}, 'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}}], 'parent': 1, 'goal': 0, 'by_tactic': 'fs[EL_simp_restricted, LENGTH_ZIP, SUM_eq_0, LENGTH_EQ_NUM, FILTER_NEQ_NIL]', 'reward': 0.2}, {'content': [], 'parent': 2, 'goal': 0, 'by_tactic': 'fs[APPEND_LENGTH_EQ, LIST_REL_rules, FILTER_NEQ_NIL, REVERSE_11, LENGTH_REVERSE]', 'reward': 5}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "naked-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_encoded_content(history, encoder):\n",
    "    # figure out why this is slower than tests\n",
    "    # figured out: remember to do strip().split()\n",
    "    fringe_sizes = []\n",
    "    contexts = []\n",
    "    reverted = []\n",
    "    for i in history:\n",
    "        c = i[\"content\"]\n",
    "        contexts.extend(c)\n",
    "        fringe_sizes.append(len(c))\n",
    "    for e in contexts:\n",
    "        g = revert_with_polish(e)\n",
    "        reverted.append(g.strip().split())\n",
    "    out = []\n",
    "    sizes = []\n",
    "    for goal in reverted:\n",
    "        out_, sizes_ = batch_encoder.encode([goal])\n",
    "        out.append(torch.cat(out_.split(1), dim=2).squeeze(0))\n",
    "        sizes.append(sizes_)\n",
    "        \n",
    "    # s1 = timeit.default_timer()\n",
    "    #out, sizes = batch_encoder.encode(reverted)\n",
    "    \n",
    "    representations = out\n",
    "    # merge two hidden variables\n",
    "    #representations = torch.cat(out.split(1), dim=2).squeeze(0)\n",
    "    # print(representations.shape)\n",
    "    # s2 = timeit.default_timer()    \n",
    "    # print(s2-s1)\n",
    "\n",
    "    return representations, contexts, fringe_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "outside-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARG_LEN = 5 \n",
    "EXCLUDED_THEORIES = [\"min\"] #[\"min\", \"bool\"]\n",
    "def replay_known_proof(known_history, arg_len=ARG_LEN):\n",
    "    print(\"\\nReplaying a known proof of {} ...\".format(known_history[0][\"content\"][0][\"plain\"][\"goal\"]))\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    fringe_pool = []\n",
    "    tac_pool = []\n",
    "    arg_pool = []\n",
    "    reward_pool = []\n",
    "    reward_print = []\n",
    "    action_pool = []\n",
    "    steps = 0\n",
    "    flag = True\n",
    "    tac_print = []\n",
    "\n",
    "    induct_arg = []\n",
    "    proved = 0\n",
    "    iteration_rewards = []\n",
    "\n",
    "    # compute the encoded fact pool at the beginning of each episode\n",
    "    polished_goal = known_history[0][\"content\"][0][\"polished\"][\"goal\"]\n",
    "    allowed_theories = parse_theory(polished_goal)\n",
    "    allowed_theories = [t for t in allowed_theories if t not in EXCLUDED_THEORIES]\n",
    "    \n",
    "    try:\n",
    "        allowed_arguments_ids = []\n",
    "        candidate_args = []\n",
    "        goal_theory = database[polished_goal][0]\n",
    "        for i,t in enumerate(database):\n",
    "            if database[t][0] in allowed_theories and (database[t][0] != goal_theory or int(database[t][2]) < int(database[polished_goal][2])):\n",
    "                allowed_arguments_ids.append(i)\n",
    "                candidate_args.append(t)\n",
    "\n",
    "    except:\n",
    "        allowed_arguments_ids = []\n",
    "        candidate_args = []\n",
    "        for i,t in enumerate(database):\n",
    "            if database[t][0] in allowed_theories:\n",
    "                allowed_arguments_ids.append(i)\n",
    "                candidate_args.append(t)\n",
    "        print(\"Theorem not found in database.\")\n",
    "\n",
    "    #encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids, device=device))\n",
    "\n",
    "    # print(\"Facts: {}+{}\".format(num_init_facts, len(split_fact_pool)-num_init_facts))\n",
    "    print(\"Facts: {}\".format(len(allowed_arguments_ids)))\n",
    "\n",
    "    for t in range(10):\n",
    "        true_resulting_fringe = known_history[t+1]\n",
    "\n",
    "        # gather all the goals in the history\n",
    "       # try:\n",
    "        print (known_history)\n",
    "        #known_history[t+1]\n",
    "        representations, context_set, fringe_sizes = gather_encoded_content(known_history, batch_encoder)\n",
    "#         except::\n",
    "#             env.close()\n",
    "#             print(\"Skipping current game due to encoding error ...\")\n",
    "#             print(\"Restarting environment ...\")\n",
    "#             print(env.goal)                \n",
    "#             env = HolEnv(env.goal)\n",
    "#             flag = False\n",
    "#             break\n",
    "            \n",
    "        return representations, context_set, fringe_sizes\n",
    "        #representations = representations.to(device)\n",
    "        context_scores = context_net(representations)\n",
    "        contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "        fringe_scores = []\n",
    "        for s in scores_by_fringe:\n",
    "            # fringe_score = torch.prod(s) # TODO: make it sum\n",
    "            fringe_score = torch.sum(s) # TODO: make it sum\n",
    "            fringe_scores.append(fringe_score)\n",
    "        fringe_scores = torch.stack(fringe_scores)\n",
    "        fringe_probs = F.softmax(fringe_scores, dim=0)\n",
    "        fringe_m = Categorical(fringe_probs)\n",
    "        # fringe = fringe_m.sample()\n",
    "        \n",
    "        true_fringe = torch.tensor([true_resulting_fringe[\"parent\"]])\n",
    "        true_fringe = true_fringe.to(device)\n",
    "        \n",
    "        fringe_pool.append(fringe_m.log_prob(true_fringe))\n",
    "\n",
    "        # take the first context in the chosen fringe for now\n",
    "\n",
    "        target_context = contexts_by_fringe[true_fringe][0]\n",
    "        target_goal = target_context[\"polished\"][\"goal\"]\n",
    "        target_representation = representations[context_set.index(target_context)]\n",
    "        # print(target_representation.shape)\n",
    "        # exit()\n",
    "\n",
    "        # size: (1, max_contexts, max_assumptions+1, max_len)\n",
    "        tac_input = target_representation.unsqueeze(0)\n",
    "        tac_input = tac_input.to(device)\n",
    "\n",
    "        # compute scores of tactics\n",
    "        tac_probs = tac_net(tac_input)\n",
    "        # print(tac_probs)\n",
    "        tac_m = Categorical(tac_probs)\n",
    "\n",
    "        true_tactic_text = true_resulting_fringe[\"by_tactic\"]\n",
    "\n",
    "        # true_tactic_text = \"Induct_on `ll`\"\n",
    "        tac_args = re.findall(r'(.*?)\\[(.*?)\\]', true_tactic_text)\n",
    "        tac_term = re.findall(r'(.*?) `(.*?)`', true_tactic_text)     \n",
    "        tac_arg = re.findall(r'(.*?) (.*)', true_tactic_text)\n",
    "\n",
    "        if tac_args:\n",
    "            true_tac_text = tac_args[0][0]\n",
    "            true_args_text = tac_args[0][1].split(\", \")\n",
    "        elif tac_term: # order matters # TODO: make it irrelavant\n",
    "            true_tac_text = tac_term[0][0]\n",
    "            true_args_text = tac_term[0][1]\n",
    "        elif tac_arg: # order matters because tac_arg could match () ``\n",
    "            true_tac_text = tac_arg[0][0]\n",
    "            true_args_text = tac_arg[0][1]\n",
    "        else:\n",
    "            true_tac_text = true_tactic_text\n",
    "        \n",
    "        true_tac = torch.tensor([tactic_pool.index(true_tac_text)])\n",
    "        true_tac = true_tac.to(device)\n",
    "        # tac = tac_m.sample()\n",
    "        # log directly the log probability\n",
    "        tac_pool.append(tac_m.log_prob(true_tac))\n",
    "        \n",
    "        action_pool.append(tactic_pool[true_tac])\n",
    "        \n",
    "        tac_print.append(tac_probs.detach())\n",
    "        # print(len(fact_pool[0].strip().split()))\n",
    "        # exit()\n",
    "\n",
    "        tac_tensor = true_tac.to(device)\n",
    "\n",
    "        assert tactic_pool[true_tac.item()] == true_tac_text\n",
    "        \n",
    "        if tactic_pool[true_tac] in no_arg_tactic:\n",
    "            tactic = tactic_pool[true_tac]\n",
    "            arg_probs = []\n",
    "            arg_probs.append(torch.tensor(0))\n",
    "            arg_pool.append(arg_probs)\n",
    "        elif tactic_pool[true_tac] == \"Induct_on\":\n",
    "            arg_probs = []\n",
    "            candidates = []\n",
    "            # input = torch.cat([target_representation, tac_tensor], dim=1)\n",
    "            tokens = target_goal.split()\n",
    "            tokens = list(dict.fromkeys(tokens))\n",
    "            tokens = [[t] for t in tokens if t[0] == \"V\"]\n",
    "            if tokens:\n",
    "                # concatenate target_representation to token\n",
    "                # use seq2seq to compute the representation of a token\n",
    "                # also we don't need to split an element in tokens because they are singletons\n",
    "                # but we need to make it a list containing a singleton list, i.e., [['Vl']]\n",
    "\n",
    "                token_representations, _ = batch_encoder.encode(tokens)\n",
    "                # reshaping\n",
    "                encoded_tokens = torch.cat(token_representations.split(1), dim=2).squeeze(0)\n",
    "                target_representation_list = [target_representation.unsqueeze(0) for _ in tokens]\n",
    "\n",
    "                target_representations = torch.cat(target_representation_list)\n",
    "                # size: (len(tokens), 512)\n",
    "                candidates = torch.cat([encoded_tokens, target_representations], dim=1)\n",
    "                candidates = candidates.to(device)\n",
    "\n",
    "                # concat = [torch.cat([torch.tensor([input_vocab.stoi[i] for _ in range(256)], dtype=torch.float), target_representation]) for i in tokens]\n",
    "\n",
    "                # candidates = torch.stack(concat)\n",
    "                # candidates = candidates.to(device)\n",
    "\n",
    "                scores = term_net(candidates, tac_tensor)\n",
    "                term_probs = F.softmax(scores, dim=0)\n",
    "                try:\n",
    "                    term_m = Categorical(term_probs.squeeze(1))\n",
    "                except:\n",
    "                    print(\"probs: {}\".format(term_probs))                                          \n",
    "                    print(\"candidates: {}\".format(candidates.shape))\n",
    "                    print(\"scores: {}\".format(scores))\n",
    "                    print(\"tokens: {}\".format(tokens))\n",
    "                    exit()\n",
    "                    \n",
    "                true_term = torch.tensor([tokens.index([\"V\" + true_args_text])])\n",
    "                true_term = true_term.to(device)\n",
    "                # term = term_m.sample()\n",
    "                \n",
    "                arg_probs.append(term_m.log_prob(true_term))\n",
    "                induct_arg.append(tokens[true_term])\n",
    "                tm = tokens[true_term.item()][0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "                \n",
    "                assert tm == true_args_text\n",
    "                \n",
    "                arg_pool.append(arg_probs)\n",
    "                if tm:\n",
    "                    tactic = \"Induct_on `{}`\".format(tm)\n",
    "                else:\n",
    "                    # only to raise an error\n",
    "                    tactic = \"Induct_on\"\n",
    "            else:\n",
    "                arg_probs.append(torch.tensor(0))\n",
    "                induct_arg.append(\"No variables\")\n",
    "                arg_pool.append(arg_probs)\n",
    "                tactic = \"Induct_on\"\n",
    "        else:\n",
    "            hidden0 = hidden1 = target_representation.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            hidden0 = hidden0.to(device)\n",
    "            hidden1 = hidden1.to(device)\n",
    "\n",
    "            hidden = (hidden0, hidden1)\n",
    "\n",
    "            # concatenate the candidates with hidden states.\n",
    "\n",
    "            hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "            hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "\n",
    "            hiddenl = torch.cat(hiddenl)\n",
    "\n",
    "            # size: (len(fact_pool), 512)\n",
    "            candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "            candidates = candidates.to(device)\n",
    "\n",
    "            input = tac_tensor\n",
    "            # run it once before predicting the first argument\n",
    "            hidden, _ = arg_net(input, candidates, hidden)\n",
    "\n",
    "            # the indices of chosen args\n",
    "            arg_step = []\n",
    "            arg_step_probs = []\n",
    "            # print(\"True args text: {}\".format(true_args_text))\n",
    "            if tactic_pool[true_tac] in thm_tactic:\n",
    "                arg_len = 1\n",
    "            else:\n",
    "                arg_len = ARG_LEN\n",
    "\n",
    "            for i in range(arg_len):\n",
    "                hidden, scores = arg_net(input, candidates, hidden)\n",
    "                arg_probs = F.softmax(scores, dim=0)\n",
    "                arg_m = Categorical(arg_probs.squeeze(1))\n",
    "                if isinstance(true_args_text, list):\n",
    "                    try:\n",
    "                        name_parser = true_args_text[i].split(\".\")\n",
    "                    except:\n",
    "                        print(i)\n",
    "                        print(true_args_text)\n",
    "                        print(known_history)\n",
    "                        exit()\n",
    "                    theory_name = name_parser[0][:-6] # get rid of the \"Theory\" substring\n",
    "                    theorem_name = name_parser[1]\n",
    "                    true_arg_exp = reverse_database[(theory_name, theorem_name)]\n",
    "                else:\n",
    "                    name_parser = true_args_text.split(\".\")\n",
    "                    theory_name = name_parser[0][:-6] # get rid of the \"Theory\" substring\n",
    "                    theorem_name = name_parser[1]\n",
    "                    true_arg_exp = reverse_database[(theory_name, theorem_name)]    \n",
    "                true_arg = torch.tensor(candidate_args.index(true_arg_exp))\n",
    "                true_arg = true_arg.to(device)\n",
    "                # arg = arg_m.sample()\n",
    "                \n",
    "                arg_step.append(true_arg)\n",
    "                arg_step_probs.append(arg_m.log_prob(true_arg))\n",
    "\n",
    "                # hiddenc0 = hidden[0].squeeze().repeat(1, 1, 1)\n",
    "                # hiddenc1 = hidden[1].squeeze().repeat(1, 1, 1)\n",
    "\n",
    "                # encoded chosen argument\n",
    "                input = encoded_fact_pool[true_arg.item()].unsqueeze(0).unsqueeze(0)\n",
    "                # print(input.shape)\n",
    "\n",
    "                # renew candidates                \n",
    "                hc = torch.cat([hidden0.squeeze(), hidden1.squeeze()])\n",
    "                hiddenl = [hc.unsqueeze(0) for _ in allowed_arguments_ids]\n",
    "\n",
    "                hiddenl = torch.cat(hiddenl)\n",
    "\n",
    "                # size: (len(fact_pool), 512)\n",
    "                candidates = torch.cat([encoded_fact_pool, hiddenl], dim=1)\n",
    "                candidates = candidates.to(device)\n",
    "\n",
    "            arg_pool.append(arg_step_probs)\n",
    "\n",
    "            # tac = tactic_pool[true_tac]\n",
    "            # arg = [candidate_args[i] for i in arg_step]\n",
    "\n",
    "            # tactic = env.assemble_tactic(tac, arg)\n",
    "            # assert tactic == true_tactic_text\n",
    "\n",
    "        action = (true_fringe.item(), 0, true_tactic_text)\n",
    "        try:\n",
    "            reward = true_resulting_fringe[\"reward\"]\n",
    "            done = t+2 == len(known_history)\n",
    "\n",
    "        except:\n",
    "            print(\"Step exception raised.\")\n",
    "            # print(\"Fringe: {}\".format(env.history))\n",
    "            print(\"Handling: {}\".format(env.handling))\n",
    "            print(\"Using: {}\".format(env.using))\n",
    "            # try again\n",
    "            # counter = env.counter\n",
    "            frequency = env.frequency\n",
    "            env.close()\n",
    "            print(\"Aborting current game ...\")\n",
    "            print(\"Restarting environment ...\")\n",
    "            print(env.goal)\n",
    "            env = HolEnv(env.goal)\n",
    "            flag = False\n",
    "            break\n",
    "\n",
    "        # state_pool.append(state)\n",
    "        reward_print.append(reward)\n",
    "        # reward_pool.append(reward+trade_off*entropy)\n",
    "        reward_pool.append(reward)\n",
    "\n",
    "        # pg = ng\n",
    "\n",
    "        steps += 1\n",
    "        total_reward = float(np.sum(reward_print))\n",
    "        if done == True:\n",
    "            print(\"Proved in {} steps.\".format(t+1))\n",
    "            print(\"Rewards: {}\".format(reward_print))\n",
    "            # print(\"Tactics: {}\".format(action_pool))\n",
    "            # # print(\"Mean reward: {}\".format(np.mean(reward_pool)))\n",
    "            # print(\"Total: {}\".format(total_reward))\n",
    "            # print(\"Proof trace: {}\".format(extract_proof(known_history)))\n",
    "            # try:\n",
    "            #     print(\"Proof script: {}\".format(reconstruct_proof(known_history)))\n",
    "            # except:\n",
    "            #     print(\"Proof check failed with error.\")\n",
    "            #     proof_check_failure.append(known_history)\n",
    "\n",
    "            # exit()\n",
    "            proved += 1\n",
    "            # traces.append(env.history)\n",
    "            iteration_rewards.append(total_reward)\n",
    "            break\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time: {}  '.format(stop - start))\n",
    "\n",
    "\n",
    "    # Update policy\n",
    "    if True:\n",
    "        # Discount reward\n",
    "        running_add = 0\n",
    "        for i in reversed(range(steps)):\n",
    "            if reward_pool[i] == 0:\n",
    "                running_add = 0\n",
    "            else:\n",
    "                running_add = running_add * gamma + reward_pool[i]\n",
    "                reward_pool[i] = running_add\n",
    "\n",
    "        optimizer_context.zero_grad()\n",
    "        optimizer_tac.zero_grad()\n",
    "        optimizer_arg.zero_grad()\n",
    "        optimizer_term.zero_grad()\n",
    "\n",
    "        for i in range(steps):\n",
    "            # size : (1,1,4,128)\n",
    "            total_loss = 0\n",
    "\n",
    "            # state = state_pool[i]\n",
    "            reward = reward_pool[i]\n",
    "\n",
    "            fringe_loss = -fringe_pool[i] * (reward)\n",
    "            arg_loss = -torch.sum(torch.stack(arg_pool[i])) * (reward)\n",
    "\n",
    "            tac_loss = -tac_pool[i] * (reward)\n",
    "\n",
    "            # entropy = fringe_pool[i] + torch.sum(torch.stack(arg_pool[i])) + tac_pool[i]\n",
    "\n",
    "            # loss = fringe_loss + tac_loss + arg_loss + trade_off*entropy\n",
    "            loss = fringe_loss + tac_loss + arg_loss\n",
    "            # total_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "        # total_loss.backward()\n",
    "\n",
    "        # optimizer.step()\n",
    "\n",
    "        optimizer_context.step()\n",
    "        optimizer_tac.step()\n",
    "        optimizer_arg.step()\n",
    "        # optimizer_term.step()\n",
    "\n",
    "    prf = torch.mean(torch.stack(tac_print), 0)\n",
    "    print(\"Preferences: {}\\n\".format(prf))\n",
    "\n",
    "    # state_pool = []\n",
    "    fringe_pool = []\n",
    "    tac_pool = []\n",
    "    arg_pool = []\n",
    "    action_pool = []\n",
    "    reward_pool = []\n",
    "    reward_print = []\n",
    "    steps = 0\n",
    "    flag = True\n",
    "    \n",
    "    \n",
    "def parse_theory(pg):\n",
    "    theories = re.findall(r'C\\$(\\w+)\\$ ', pg)\n",
    "    theories = set(theories)\n",
    "    for th in EXCLUDED_THEORIES:\n",
    "        theories.discard(th)\n",
    "    return list(theories)\n",
    "\n",
    "def revert_with_polish(context):\n",
    "    target = context[\"polished\"]\n",
    "    assumptions = target[\"assumptions\"]\n",
    "    goal = target[\"goal\"]\n",
    "    for i in reversed(assumptions): \n",
    "        goal = \"@ @ Dmin$==> {} {}\".format(i, goal)\n",
    "    return goal \n",
    "\n",
    "def split_by_fringe(goal_set, goal_scores, fringe_sizes):\n",
    "    # group the scores by fringe\n",
    "    fs = []\n",
    "    gs = []\n",
    "    counter = 0\n",
    "    for i in fringe_sizes:\n",
    "        end = counter + i\n",
    "        fs.append(goal_scores[counter:end])\n",
    "        gs.append(goal_set[counter:end])\n",
    "        counter = end\n",
    "    return gs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unlike-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "import seq2seq\n",
    "#from seq2seq.evaluator import BatchPredictor\n",
    "#from seq2sebq.util.checkpoint import Checkpoint\n",
    "\n",
    "from batch_predictor import BatchPredictor\n",
    "from checkpoint import Checkpoint\n",
    "\n",
    "#checkpoint_path = \"models/2021_02_21_15_46_04\" # 98% accuracy model, up to probability theory\n",
    "checkpoint_path = \"models/2020_04_26_20_11_28\" # 95% accuracy model, core theories + integer + sorting\n",
    "\n",
    "#checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "#checkpoint_path = \"models/2020_09_24_23_38_06\" # 98% accuracy model, core theories + integer + sorting | separate theory tokens\n",
    "\n",
    "\n",
    "# logging.info(\"loading checkpoint from {}\".format(checkpoint_path))\n",
    "\n",
    "checkpoint = Checkpoint.load(checkpoint_path)\n",
    "seq2seq = checkpoint.model\n",
    "input_vocab = checkpoint.input_vocab\n",
    "output_vocab = checkpoint.output_vocab\n",
    "\n",
    "batch_encoder = BatchPredictor(seq2seq, input_vocab, output_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dramatic-sister",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replaying a known proof of REVERSE l = [] ⇔ l = [] ...\n",
      "Theorem not found in database.\n",
      "Facts: 0\n",
      "[{'content': [{'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'}, 'plain': {'assumptions': [], 'goal': 'REVERSE l = [] ⇔ l = []'}}], 'parent': None, 'goal': None, 'by_tactic': '', 'reward': None}, {'content': [{'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Clist$NIL Clist$NIL @ @ Cmin$= Clist$NIL Clist$NIL'}, 'plain': {'assumptions': [], 'goal': 'REVERSE [] = [] ⇔ [] = []'}}, {'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}, 'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}}], 'parent': 0, 'goal': 0, 'by_tactic': 'Induct_on `l`', 'reward': 0.1}, {'content': [{'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}, 'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}}], 'parent': 1, 'goal': 0, 'by_tactic': 'fs[EL_simp_restricted, LENGTH_ZIP, SUM_eq_0, LENGTH_EQ_NUM, FILTER_NEQ_NIL]', 'reward': 0.2}, {'content': [], 'parent': 2, 'goal': 0, 'by_tactic': 'fs[APPEND_LENGTH_EQ, LIST_REL_rules, FILTER_NEQ_NIL, REVERSE_11, LENGTH_REVERSE]', 'reward': 5}]\n"
     ]
    }
   ],
   "source": [
    "jax_reps, context_set, fringe_sizes = replay_known_proof(test_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "violent-justice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'}, 'plain': {'assumptions': [], 'goal': 'REVERSE l = [] ⇔ l = []'}}, {'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Clist$NIL Clist$NIL @ @ Cmin$= Clist$NIL Clist$NIL'}, 'plain': {'assumptions': [], 'goal': 'REVERSE [] = [] ⇔ [] = []'}}, {'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}, 'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}}, {'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}, 'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}}] [1, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print (context_set, fringe_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "natural-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_networks import *\n",
    "import policy_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "every-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_context, apply_context = hk.transform(policy_networks._context_forward)\n",
    "rng_key = random.PRNGKey(100)\n",
    "\n",
    "#convert to jax\n",
    "jax_reps = jnp.stack([jnp.array(jax_reps[i][0]) for i in range(len(jax_reps))])\n",
    "\n",
    "\n",
    "context_params = init_context(rng_key, jax_reps)\n",
    "apply_context = jax.jit(apply_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "related-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_scores = apply_context(context_params, rng_key, jax_reps)\n",
    "# print (context_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "executive-truth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_policy/~/linear': {'b': DeviceArray([-1.07554905e-02, -1.54109523e-02,  0.00000000e+00,\n",
      "              0.00000000e+00,  0.00000000e+00,  8.85673426e-03,\n",
      "              0.00000000e+00, -1.18663358e-02, -7.72345625e-03,\n",
      "              0.00000000e+00,  9.60543007e-03, -2.04063312e-04,\n",
      "              1.32207945e-03,  0.00000000e+00, -1.41528305e-02,\n",
      "              1.16932672e-02, -1.79509595e-02,  0.00000000e+00,\n",
      "              0.00000000e+00,  1.49002681e-02,  0.00000000e+00,\n",
      "              0.00000000e+00, -9.41692851e-04, -1.20092742e-02,\n",
      "              5.15160523e-03, -1.57083152e-04,  1.06401350e-02,\n",
      "             -8.68311524e-03,  0.00000000e+00,  2.36719032e-03,\n",
      "              0.00000000e+00,  7.62443151e-03, -8.41719657e-03,\n",
      "             -3.98649648e-03,  0.00000000e+00,  1.52461766e-03,\n",
      "              3.47098918e-04,  0.00000000e+00,  0.00000000e+00,\n",
      "              0.00000000e+00,  0.00000000e+00,  7.20718317e-03,\n",
      "              6.49480056e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "             -1.45647489e-02,  0.00000000e+00, -2.45557223e-02,\n",
      "             -6.45890599e-03,  9.45218094e-03,  1.32161286e-02,\n",
      "              0.00000000e+00,  0.00000000e+00,  1.81922652e-02,\n",
      "             -3.39362444e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "             -5.49717247e-03,  3.00858775e-03,  3.86847369e-03,\n",
      "             -6.37707021e-03, -1.10119181e-02,  0.00000000e+00,\n",
      "             -8.48986208e-03,  1.69391185e-03, -1.57409385e-02,\n",
      "             -4.63587325e-03,  6.85342401e-03,  9.05118417e-03,\n",
      "             -1.23428944e-02, -1.88133679e-03,  2.46317126e-03,\n",
      "              0.00000000e+00, -6.51282538e-03,  1.37006156e-02,\n",
      "              1.07283629e-02,  0.00000000e+00,  1.12008788e-02,\n",
      "              1.62766781e-02,  0.00000000e+00, -8.76940787e-03,\n",
      "              3.37187760e-03, -4.83727269e-03, -1.65651478e-02,\n",
      "              0.00000000e+00, -8.01304355e-03,  0.00000000e+00,\n",
      "              0.00000000e+00, -1.12292096e-02,  6.82025403e-03,\n",
      "             -1.71034150e-02,  0.00000000e+00, -1.03484495e-02,\n",
      "             -8.09190981e-03,  6.18372113e-04,  4.11694031e-03,\n",
      "              6.06002845e-03,  0.00000000e+00,  1.82452053e-03,\n",
      "             -2.17187684e-02,  0.00000000e+00,  0.00000000e+00,\n",
      "              1.33411204e-02,  0.00000000e+00,  7.67145026e-03,\n",
      "             -2.75020488e-03,  6.27558958e-03, -6.70215767e-03,\n",
      "             -2.00387388e-02,  2.13056337e-03, -3.19831190e-03,\n",
      "              0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "              1.00596128e-02,  5.28839324e-03,  0.00000000e+00,\n",
      "              0.00000000e+00, -1.24014728e-03, -4.60697897e-03,\n",
      "             -9.17350594e-03, -8.71822424e-03,  8.58060922e-03,\n",
      "              6.39130175e-03, -3.56327090e-03,  0.00000000e+00,\n",
      "              8.90475325e-03,  0.00000000e+00,  1.41926240e-02,\n",
      "              0.00000000e+00,  1.38226189e-02,  3.74258216e-03,\n",
      "             -2.36509927e-03,  7.59113580e-04,  1.31307822e-02,\n",
      "              0.00000000e+00,  1.83476545e-02, -1.32233202e-02,\n",
      "              0.00000000e+00, -1.30619481e-03, -1.75998732e-02,\n",
      "              0.00000000e+00, -6.63097156e-03,  0.00000000e+00,\n",
      "              0.00000000e+00, -8.07530060e-03, -3.63725726e-03,\n",
      "              0.00000000e+00,  3.22197285e-03,  8.43206653e-04,\n",
      "             -6.95685390e-03,  1.66984787e-03,  1.45759135e-02,\n",
      "             -1.56932846e-02,  3.00116139e-03, -2.10862532e-02,\n",
      "             -2.58540362e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "              6.16768422e-03,  0.00000000e+00,  2.40885559e-03,\n",
      "             -1.14567727e-02,  0.00000000e+00, -9.03919991e-03,\n",
      "              1.07422005e-02,  1.49659421e-02,  0.00000000e+00,\n",
      "              0.00000000e+00, -6.06371090e-04,  2.82041728e-04,\n",
      "              1.86880515e-03,  8.84182099e-03,  8.64916388e-03,\n",
      "              2.61766184e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "              2.24808380e-02,  0.00000000e+00,  0.00000000e+00,\n",
      "              0.00000000e+00,  2.62795761e-03,  7.82225654e-03,\n",
      "              0.00000000e+00, -4.35722433e-03,  0.00000000e+00,\n",
      "              0.00000000e+00, -7.68815540e-03, -9.97669809e-03,\n",
      "             -7.51757994e-03, -1.34043088e-02,  5.83160529e-03,\n",
      "             -5.76735334e-03, -1.30656613e-02, -7.24859536e-04,\n",
      "              0.00000000e+00, -4.56351787e-03,  0.00000000e+00,\n",
      "             -1.72096416e-02,  1.30774826e-03,  5.76710096e-04,\n",
      "              2.28202343e-03, -1.31398893e-03,  1.00862561e-02,\n",
      "             -4.14951518e-03, -1.49130560e-02,  0.00000000e+00,\n",
      "              0.00000000e+00,  0.00000000e+00,  8.44506361e-03,\n",
      "             -3.58462986e-03,  6.24602847e-03, -2.11671516e-02,\n",
      "             -4.11482155e-03, -7.80191179e-03,  7.25608552e-03,\n",
      "              2.27554096e-03, -1.70351542e-03,  9.77168512e-03,\n",
      "              0.00000000e+00,  1.34948045e-02, -5.96932694e-03,\n",
      "              5.93016902e-03,  0.00000000e+00,  2.04462558e-04,\n",
      "              0.00000000e+00, -3.54850851e-02,  4.44894750e-03,\n",
      "             -4.00210125e-03,  4.57159523e-03,  0.00000000e+00,\n",
      "             -5.63014206e-03,  3.76958400e-04,  4.00344282e-03,\n",
      "              0.00000000e+00, -3.97080928e-03,  1.28079648e-03,\n",
      "             -3.46819684e-03, -2.94996612e-03,  0.00000000e+00,\n",
      "             -5.48140705e-03, -2.14988701e-02,  7.60041922e-03,\n",
      "              4.50569810e-03,  6.22220337e-03,  0.00000000e+00,\n",
      "              0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "              1.14097055e-02,  2.45082984e-03,  0.00000000e+00,\n",
      "              2.65949126e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "              7.63146300e-03, -1.04756858e-02,  2.37900019e-03,\n",
      "              3.87053471e-03, -2.88055837e-02, -3.37707251e-03,\n",
      "              0.00000000e+00,  0.00000000e+00, -2.21660025e-02,\n",
      "              9.92910750e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "              0.00000000e+00,  2.33649183e-02, -4.14837850e-03,\n",
      "              3.55538796e-03,  0.00000000e+00,  5.45679964e-03,\n",
      "              1.61547959e-03, -3.48299742e-02, -4.39314172e-03,\n",
      "              3.02348286e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "             -9.91504267e-03, -7.67554250e-03, -1.52031779e-02,\n",
      "              1.34644797e-04,  0.00000000e+00, -1.44429933e-02,\n",
      "              0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "             -7.72196939e-03, -5.55302296e-03,  8.35165754e-03,\n",
      "              1.73049793e-02,  6.83101825e-03,  1.10442694e-02,\n",
      "              0.00000000e+00, -2.43268795e-02,  0.00000000e+00,\n",
      "              3.06552742e-03,  2.16059797e-02,  6.96887076e-03,\n",
      "             -6.13811286e-03,  0.00000000e+00, -8.27805139e-03,\n",
      "             -9.37744509e-03,  0.00000000e+00,  1.15843546e-02,\n",
      "              0.00000000e+00, -1.93589982e-02, -3.34669766e-03,\n",
      "              0.00000000e+00, -1.51433367e-02, -3.82561050e-03,\n",
      "              0.00000000e+00, -5.03932685e-03,  0.00000000e+00,\n",
      "              1.62593462e-02, -1.40863033e-02, -1.35773625e-02,\n",
      "             -5.06201247e-03, -2.20759474e-02,  0.00000000e+00,\n",
      "              5.49698249e-03,  7.45607167e-03,  1.01776794e-03,\n",
      "             -8.48039798e-03, -1.28418934e-02,  8.53929669e-04,\n",
      "              1.24980342e-02, -5.49570471e-03,  5.34972176e-03,\n",
      "              0.00000000e+00, -1.46384435e-02,  0.00000000e+00,\n",
      "              0.00000000e+00, -2.54909135e-03, -4.23373654e-04,\n",
      "              3.12919542e-03, -1.07242111e-02, -1.44192670e-03,\n",
      "             -3.83227435e-03, -1.27913803e-02,  3.39395441e-02,\n",
      "              0.00000000e+00, -1.68136954e-02, -1.50485691e-02,\n",
      "             -4.96033393e-03,  2.76310742e-03, -3.51968221e-04,\n",
      "              1.89217702e-02,  1.94860734e-02,  5.37440833e-03,\n",
      "             -2.12807464e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "             -1.09626958e-03, -2.12581158e-02,  0.00000000e+00,\n",
      "             -1.18962135e-02, -1.46894455e-02,  6.10625092e-03,\n",
      "              8.61729123e-03,  3.70739866e-03, -3.08242673e-03,\n",
      "              8.75438936e-03, -1.94705501e-02,  0.00000000e+00,\n",
      "              0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "              1.00686541e-02, -4.22779843e-03,  9.45416838e-03,\n",
      "              5.38877212e-03, -2.79972795e-03,  0.00000000e+00,\n",
      "              5.06689586e-03,  1.55283622e-02,  4.71487641e-04,\n",
      "             -4.32154629e-03, -4.00043605e-03, -1.50320912e-02,\n",
      "              0.00000000e+00,  0.00000000e+00,  1.64712127e-03,\n",
      "              4.35882993e-03, -6.47328794e-04,  1.63554959e-03,\n",
      "             -9.73626971e-04,  2.51741931e-02,  0.00000000e+00,\n",
      "              0.00000000e+00,  7.50036910e-04, -5.54964319e-03,\n",
      "             -4.47409414e-03,  2.29042210e-02, -6.03502011e-03,\n",
      "             -5.51054580e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "             -7.00278115e-03, -1.31477658e-02,  2.73268810e-03,\n",
      "              1.24712493e-02,  0.00000000e+00,  1.16683533e-02,\n",
      "              0.00000000e+00,  1.63470022e-03,  1.69502757e-02,\n",
      "              0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "              3.97977233e-03, -8.78091156e-03,  4.07619216e-03,\n",
      "              2.43302900e-03, -2.32623369e-02,  2.14302819e-03,\n",
      "             -7.25448690e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "              0.00000000e+00,  2.28146650e-03,  0.00000000e+00,\n",
      "              8.50442611e-03, -5.85681014e-03,  0.00000000e+00,\n",
      "              2.90128402e-04,  5.33404201e-03,  5.19734668e-03,\n",
      "              0.00000000e+00,  8.19944311e-03,  3.29536200e-03,\n",
      "              0.00000000e+00, -2.10650042e-02,  0.00000000e+00,\n",
      "              6.27960730e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "              7.49440584e-03,  0.00000000e+00,  1.48928910e-03,\n",
      "              0.00000000e+00,  0.00000000e+00, -9.23177786e-03,\n",
      "              3.95300798e-03,  2.67600920e-03,  7.37458467e-04,\n",
      "              0.00000000e+00, -1.38529036e-02,  1.09549006e-02,\n",
      "             -5.47491387e-03,  2.36612055e-02,  0.00000000e+00,\n",
      "              0.00000000e+00,  2.37343553e-03,  0.00000000e+00,\n",
      "             -3.89835238e-03, -1.05237532e-02,  0.00000000e+00,\n",
      "              1.03321625e-02,  0.00000000e+00, -6.31392933e-04,\n",
      "              2.60457443e-03,  3.85712534e-02,  7.68407714e-03,\n",
      "             -1.44858463e-02, -9.01808776e-03,  0.00000000e+00,\n",
      "             -3.39888502e-03,  0.00000000e+00,  1.22889876e-02,\n",
      "             -1.21732242e-02,  0.00000000e+00,  1.04450323e-02,\n",
      "             -8.99090059e-03, -8.59379210e-03, -3.62794846e-03,\n",
      "             -1.60055533e-02, -4.09241207e-03, -7.40254880e-04,\n",
      "              1.80889363e-03,  1.88011639e-02, -1.38424803e-03,\n",
      "              0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "              1.22620799e-02, -2.05265218e-03, -1.50838047e-02,\n",
      "              7.32874731e-03,  0.00000000e+00,  5.12911938e-05,\n",
      "              6.80310745e-03,  1.08937509e-02,  0.00000000e+00,\n",
      "              1.85368229e-02, -4.17365041e-03,  0.00000000e+00,\n",
      "              2.92346859e-03,  3.59530374e-03,  0.00000000e+00,\n",
      "              0.00000000e+00, -6.97098672e-04, -1.32922027e-02,\n",
      "             -1.25716571e-02,  8.26536212e-03,  0.00000000e+00,\n",
      "             -1.09258005e-02,  0.00000000e+00,  6.84567261e-04,\n",
      "              1.73127204e-02,  9.03778430e-03], dtype=float32), 'w': DeviceArray([[-0.00952796, -0.01760481,  0.        , ...,  0.00052843,\n",
      "               0.01597242,  0.00697641],\n",
      "             [ 0.0106316 ,  0.01645095,  0.        , ..., -0.00065257,\n",
      "              -0.01730858, -0.00861531],\n",
      "             [-0.00976925, -0.01118999,  0.        , ...,  0.00068152,\n",
      "               0.01529855,  0.00899754],\n",
      "             ...,\n",
      "             [ 0.00483132, -0.00524833,  0.        , ..., -0.00054114,\n",
      "              -0.00578187, -0.00714427],\n",
      "             [-0.00325951,  0.01343648,  0.        , ...,  0.00057898,\n",
      "               0.00241677,  0.00764374],\n",
      "             [-0.00993787, -0.01225259,  0.        , ...,  0.00067189,\n",
      "               0.01567803,  0.00887044]], dtype=float32)}, 'context_policy/~/linear_1': {'b': DeviceArray([ 0.00409084, -0.00740976,  0.        , ...,  0.01294734,\n",
      "              0.00082484,  0.00489538], dtype=float32), 'w': DeviceArray([[ 0.0024361 , -0.00662369,  0.        , ...,  0.00771015,\n",
      "               0.00020299,  0.00437605],\n",
      "             [ 0.00276069, -0.01153258,  0.        , ...,  0.00873746,\n",
      "              -0.0004758 ,  0.00761919],\n",
      "             [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "               0.        ,  0.        ],\n",
      "             ...,\n",
      "             [ 0.        ,  0.00302916,  0.        , ...,  0.        ,\n",
      "               0.00044501, -0.00200126],\n",
      "             [ 0.00306989, -0.01613501,  0.        , ...,  0.00971607,\n",
      "              -0.00091678,  0.01065985],\n",
      "             [ 0.        ,  0.00086551,  0.        , ...,  0.        ,\n",
      "               0.00012715, -0.00057181]], dtype=float32)}, 'context_policy/~/linear_2': {'b': DeviceArray([-0.20932764], dtype=float32), 'w': DeviceArray([[-0.15603204],\n",
      "             [-0.20304242],\n",
      "             [ 0.        ],\n",
      "             ...,\n",
      "             [-0.17328972],\n",
      "             [ 0.05182172],\n",
      "             [ 0.08621318]], dtype=float32)}} 2 [[{'plain': {'assumptions': [], 'goal': 'REVERSE l = [] ⇔ l = []'}, 'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'}}], [{'plain': {'assumptions': [], 'goal': 'REVERSE [] = [] ⇔ [] = []'}, 'polished': {'assumptions': [], 'goal': '@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Clist$NIL Clist$NIL @ @ Cmin$= Clist$NIL Clist$NIL'}}, {'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}, 'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}}], [{'plain': {'assumptions': ['REVERSE l = [] ⇔ l = []'], 'goal': '∀h. REVERSE (h::l) = [] ⇔ h::l = []'}, 'polished': {'assumptions': ['@ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE Vl Clist$NIL @ @ Cmin$= Vl Clist$NIL'], 'goal': '@ Cbool$! | Vh @ @ Cmin$= @ @ Cmin$= @ Clist$REVERSE @ @ Clist$CONS Vh Vl Clist$NIL @ @ Cmin$= @ @ Clist$CONS Vh Vl Clist$NIL'}}], []]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#function to give the log probability of pi(f | s) so gradient can be computed directly\n",
    "#also returns sampled index and contexts to determine goal to give tactic network\n",
    "def sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes):\n",
    "    context_scores = context_net(context_params, rng_key, jax_reps)\n",
    "    contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "    fringe_scores = []\n",
    "    for s in scores_by_fringe:\n",
    "        fringe_score = jnp.sum(s)\n",
    "        fringe_scores.append(fringe_score)\n",
    "    #TODO some fringes can be empty, but still give value 0 which assigns nonzero probability?\n",
    "    fringe_scores = jnp.stack(fringe_scores)\n",
    "    fringe_probs = jax.nn.softmax(fringe_scores)\n",
    "\n",
    "    #samples, gives an index (looks like it does gumbel softmax under the hood to keep differentiability?)\n",
    "    sampled_idx = random.categorical(rng_key,fringe_probs)\n",
    "\n",
    "    prob = fringe_probs[sampled_idx]\n",
    "    log_prob = jnp.log(prob)\n",
    "    return log_prob, (sampled_idx, contexts_by_fringe)\n",
    "                                                           \n",
    "grad_log_context, (fringe_idx, contexts_by_fringe) = jax.grad(sample_fringe, has_aux=True)(context_params, apply_context, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "print (grad_log_context, fringe_idx, contexts_by_fringe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "underlying-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_context = contexts_by_fringe[fringe_idx][0]\n",
    "target_goal = target_context[\"polished\"][\"goal\"]\n",
    "target_representation = jax_reps[context_set.index(target_context)]\n",
    "# print(target_representation.shape)\n",
    "# exit()\n",
    "\n",
    "# size: (1, max_contexts, max_assumptions+1, max_len)\n",
    "# tac_input = target_representation.unsqueeze(0)\n",
    "# tac_input = tac_input.to(device)\n",
    "\n",
    "# # compute scores of tactics\n",
    "# tac_probs = tac_net(tac_input)\n",
    "# # print(tac_probs)\n",
    "# tac_m = Categorical(tac_probs)\n",
    "# tac = tac_m.sample()\n",
    "\n",
    "# print (target_context)\n",
    "# print (target_goal)\n",
    "#print (target_representation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "greenhouse-taiwan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "MORE_TACTICS = True\n",
    "if not MORE_TACTICS:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\"]\n",
    "    thm_tactic = [\"irule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\"]\n",
    "else:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "    thm_tactic = [\"irule\", \"drule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\", \"EQ_TAC\"]\n",
    "    \n",
    "    # thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "    # thm_tactic = [] #[\"irule\", \"drule\"] \n",
    "    # term_tactic = [\"Induct_on\"]\n",
    "    # no_arg_tactic = [\"strip_tac\", \"EQ_TAC\", \"simp[]\", \"rw[]\", \"metis_tac[]\", \"fs[]\"]\n",
    "\n",
    "tactic_pool = thms_tactic + thm_tactic + term_tactic + no_arg_tactic\n",
    "print (len(tactic_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cubic-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tac, apply_tac = hk.transform(policy_networks._tac_forward)\n",
    "\n",
    "tactic_params = init_tac(rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "apply_tac = partial(jax.jit, static_argnums=3)(apply_tac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "motivated-walnut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(4, dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = apply_tac(tactic_params, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "random.categorical(rng_key, p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fewer-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a goal encoding and samples tactic from network, and returns log prob for gradient \n",
    "def sample_tactic(tactic_params, tac_net, rng_key, goal_endcoding, action_size=len(tactic_pool)):\n",
    "    tac_probs = tac_net(tactic_params, rng_key, goal_endcoding, action_size)[0]\n",
    "    tac_idx = random.categorical(rng_key, tac_probs)\n",
    "    log_prob = jnp.log(tac_probs[tac_idx])\n",
    "    return log_prob, tac_idx\n",
    "\n",
    "grad_log_tac, tac_idx = jax.grad(sample_tactic, has_aux=True)(tactic_params, apply_tac, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "\n",
    "sampled_tac = tactic_pool[tac_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "floating-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can multiply learning rate by reward for each update to give the policy gradient after grad of log probs is done \n",
    "# context_lr = 1e-2\n",
    "# # tactic_lr = 1e-2 \n",
    "# # arg_lr = 1e-2\n",
    "# # term_lr = 1e-2\n",
    "\n",
    "# context_optimiser = optax.rmsprop(context_lr)\n",
    "# # tactic_optimiser = optax.rmsprop(tactic_lr)\n",
    "# # arg_optimiser = optax.rmsprop(arg_lr)\n",
    "# # term_optimiser = optax.rmsprop(term_lr)\n",
    "\n",
    "# opt_state_context = context_optimiser.init(context_params)\n",
    "# # opt_state_tactic = tactic_optimiser.init(tactic_params)\n",
    "# # opt_state_arg = arg_optimiser.init(arg_params)\n",
    "# # opt_state_term = term_optimiser.init(term_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#gradient example. May need to construct separate function for each net during training \n",
    "\n",
    "# def compute_probs(params, net, *args):\n",
    "#     probs = jax.nn.softmax(jnp.ravel(net(params,*args)))\n",
    "#     logits = jnp.log(probs)\n",
    "#     ind = random.categorical(rng_key, logits)\n",
    "#     log_prob = logits[ind]\n",
    "#     return log_prob\n",
    "\n",
    "# grad = jax.grad(compute_probs)(context_params, apply_context, rng_key, jax_reps)#c_term, x_arg,TAC_SIZE, MAX_LEN)\n",
    "\n",
    "# # updates, opt_state_term = context_optimiser.update(grad, opt_state_context)\n",
    "# context_params = optax.apply_updates(context_params, updates)\n",
    "\n",
    "\n",
    "# context_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-deputy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
