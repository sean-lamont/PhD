{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "import sys\n",
    "from time import sleep\n",
    "import json\n",
    "import pexpect\n",
    "import re\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLPATH = \"/home/sean/Documents/PhD/HOL4/HOL/bin/hol --maxheap=256\"\n",
    "TARGET_THEORIES = [\"pred_set\"] #[\"arithmetic\"]#[\"list\"] #[\"rich_list\"] # [\"integer\"] #[\"arithmetic\"] # [\"rich_list\"] #[\"pred_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"typed_database.json\") as f:\n",
    "    database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "MORE_TACTICS = True\n",
    "if not MORE_TACTICS:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\"]\n",
    "    thm_tactic = [\"irule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\"]\n",
    "else:\n",
    "    thms_tactic = [\"simp\", \"fs\", \"metis_tac\", \"rw\"]\n",
    "    thm_tactic = [\"irule\", \"drule\"]\n",
    "    term_tactic = [\"Induct_on\"]\n",
    "    no_arg_tactic = [\"strip_tac\", \"EQ_TAC\"]\n",
    "    \n",
    "tactic_pool = thms_tactic + thm_tactic + term_tactic + no_arg_tactic\n",
    "print (len(tactic_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polish(raw_goal):\n",
    "        goal = construct_goal(raw_goal)\n",
    "        process.sendline(goal.encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer (HOLPP.add_string o pt);\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"top_goals();\".encode(\"utf-8\"))\n",
    "        process.expect(\"val it =\")\n",
    "        process.expect([\": goal list\", \":\\r\\n +goal list\"])\n",
    "\n",
    "        polished_raw = process.before.decode(\"utf-8\")\n",
    "        polished_subgoals = re.sub(\"“|”\",\"\\\"\", polished_raw)\n",
    "        polished_subgoals = re.sub(\"\\r\\n +\",\" \", polished_subgoals)\n",
    "\n",
    "        # print(\"content:{}\".format(subgoals))\n",
    "        # exit()\n",
    "        pd = eval(polished_subgoals)\n",
    "        \n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"drop();\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "        process.sendline(\"val _ = set_term_printer default_pt;\".encode(\"utf-8\"))\n",
    "        process.expect(\"\\r\\n>\")\n",
    "\n",
    "        data = [{\"polished\":{\"assumptions\": e[0][0], \"goal\":e[0][1]},\n",
    "                 \"plain\":{\"assumptions\": e[1][0], \"goal\":e[1][1]}}\n",
    "                for e in zip(pd, [([], raw_goal)])]\n",
    "        return data # list(zip(pd, [([], raw_goal)]))f\n",
    "\n",
    "def construct_goal(goal):\n",
    "    s = \"g \" + \"`\" + goal + \"`;\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "tactic_zero_path = \"/home/sean/Documents/PhD/git/repo/PhD/tacticzero/holgym/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_encoded_content_(history, encoder):\n",
    "    fringe_sizes = []\n",
    "    contexts = []\n",
    "    reverted = []\n",
    "    for i in history:\n",
    "        c = i[\"content\"]\n",
    "        contexts.extend(c)\n",
    "        fringe_sizes.append(len(c))\n",
    "    for e in contexts:\n",
    "        g = revert_with_polish(e)\n",
    "        reverted.append(g.strip().split())\n",
    "    out = []\n",
    "    sizes = []\n",
    "    for goal in reverted:\n",
    "        out_, sizes_ = encoder.encode([goal])\n",
    "        out.append(torch.cat(out_.split(1), dim=2).squeeze(0))\n",
    "        sizes.append(sizes_)\n",
    "\n",
    "    representations = out\n",
    "\n",
    "    return representations, contexts, fringe_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_theory(pg):\n",
    "    theories = re.findall(r'C\\$(\\w+)\\$ ', pg)\n",
    "    theories = set(theories)\n",
    "    for th in EXCLUDED_THEORIES:\n",
    "        theories.discard(th)\n",
    "    return list(theories)\n",
    "\n",
    "def revert_with_polish(context):\n",
    "    target = context[\"polished\"]\n",
    "    assumptions = target[\"assumptions\"]\n",
    "    goal = target[\"goal\"]\n",
    "    for i in reversed(assumptions): \n",
    "        goal = \"@ @ Dmin$==> {} {}\".format(i, goal)\n",
    "    return goal \n",
    "\n",
    "def split_by_fringe(goal_set, goal_scores, fringe_sizes):\n",
    "    # group the scores by fringe\n",
    "    fs = []\n",
    "    gs = []\n",
    "    counter = 0\n",
    "    for i in fringe_sizes:\n",
    "        end = counter + i\n",
    "        fs.append(goal_scores[counter:end])\n",
    "        gs.append(goal_set[counter:end])\n",
    "        counter = end\n",
    "    return gs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_env import *\n",
    "\n",
    "with open(\"include_probability.json\") as f:\n",
    "    database = json.load(f)\n",
    "\n",
    "#all theories in database\n",
    "#TARGET_THEORIES = [\"probability\", \"martingale\", \"lebesgue\", \"borel\", \"real_borel\", \"sigma_algebra\",\"util_prob\", \"fcp\", \"indexedLists\", \"rich_list\", \"list\", \"pred_set\",\"numpair\", \"basicSize\", \"numeral\", \"arithmetic\", \"prim_rec\", \"num\",\"marker\", \"bool\", \"min\", \"normalForms\", \"relation\", \"sum\", \"pair\", \"sat\",\"while\", \"bit\", \"logroot\", \"transc\", \"powser\", \"lim\", \"seq\", \"nets\",\"metric\", \"real\", \"realax\", \"hreal\", \"hrat\", \"quotient_sum\", \"quotient\",\"res_quan\", \"product\", \"iterate\", \"cardinal\", \"wellorder\",\"set_relation\", \"derivative\", \"real_topology\"]\n",
    "\n",
    "TARGET_THEORIES = [\"bool\", \"min\", \"list\"]\n",
    "GOALS = [(key, value[4]) for key, value in database.items() if value[3] == \"thm\" and value[0] in TARGET_THEORIES]\n",
    "\n",
    "with open(\"polished_def_dict.json\") as f:\n",
    "    defs = json.load(f)\n",
    "\n",
    "fact_pool = list(defs.keys())\n",
    "\n",
    "#parse theory\n",
    "g = GOALS[19][1]\n",
    "\n",
    "env = HolEnv(g)\n",
    "\n",
    "theories = re.findall(r'C\\$(\\w+)\\$ ', GOALS[19][0])\n",
    "theories = set(theories)\n",
    "theories = list(theories)\n",
    "\n",
    "allowed_theories = theories\n",
    "\n",
    "goal_theory = g\n",
    "\n",
    "print (\"Target goal: {}\".format(g))\n",
    "try:\n",
    "    allowed_arguments_ids = []\n",
    "    candidate_args = []\n",
    "    goal_theory = g#database[polished_goal][0] # plain_database[goal][0]\n",
    "    for i,t in enumerate(database):\n",
    "        if database[t][0] in allowed_theories and (database[t][0] != goal_theory or int(database[t][2]) < int(database[polished_goal][2])):\n",
    "            allowed_arguments_ids.append(i)\n",
    "            candidate_args.append(t)\n",
    "\n",
    "    env.toggle_simpset(\"diminish\", goal_theory)\n",
    "    print(\"Removed simpset of {}\".format(goal_theory))\n",
    "\n",
    "except:\n",
    "    allowed_arguments_ids = []\n",
    "    candidate_args = []\n",
    "    for i,t in enumerate(database):\n",
    "        if database[t][0] in allowed_theories:\n",
    "            allowed_arguments_ids.append(i)\n",
    "            candidate_args.append(t)\n",
    "    print(\"Theorem not found in database.\")\n",
    "\n",
    "print (\"Number of candidate facts to use: {}\".format(len(candidate_args)))\n",
    "\n",
    "encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "import seq2seq\n",
    "from batch_predictor import BatchPredictor\n",
    "from checkpoint import Checkpoint\n",
    "\n",
    "#checkpoint_path = \"models/2020_04_22_20_36_50\" # 91% accuracy model, only core theories\n",
    "#checkpoint_path = \"models/2020_04_26_20_11_28\" # 95% accuracy model, core theories + integer + sorting\n",
    "#checkpoint_path = \"models/2020_09_24_23_38_06\" # 98% accuracy model, core theories + integer + sorting | separate theory tokens\n",
    "#checkpoint_path = \"models/2020_11_28_16_45_10\" # 96-98% accuracy model, core theories + integer + sorting + real | separate theory tokens\n",
    "\n",
    "checkpoint_path = \"models/2020_12_04_03_47_22\" # 97% accuracy model, core theories + integer + sorting + real + bag | separate theory tokens\n",
    "\n",
    "#checkpoint_path = \"models/2021_02_21_15_46_04\" # 98% accuracy model, up to probability theory\n",
    "\n",
    "#checkpoint_path = \"models/2021_02_22_16_07_03\" # 97-98% accuracy model, up to and include probability theory\n",
    "\n",
    "checkpoint = Checkpoint.load(checkpoint_path)\n",
    "print (checkpoint)\n",
    "seq2seq = checkpoint.model\n",
    "input_vocab = checkpoint.input_vocab\n",
    "output_vocab = checkpoint.output_vocab\n",
    "\n",
    "batch_encoder_ = BatchPredictor(seq2seq, input_vocab, output_vocab)\n",
    "\n",
    "jax_reps, context_set, fringe_sizes = gather_encoded_content_(env.history, batch_encoder_)\n",
    "\n",
    "#convert to jax\n",
    "jax_reps = jnp.stack([jnp.array(jax_reps[i][0]) for i in range(len(jax_reps))])\n",
    "\n",
    "\n",
    "#print (jax_reps, context_set, fringe_sizes)\n",
    "print (jax_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get candidate arguments\n",
    "\n",
    "with open(\"polished_def_dict.json\") as f:\n",
    "    defs = json.load(f)\n",
    "\n",
    "fact_pool = list(defs.keys())\n",
    "\n",
    "encoded_database = torch.load('encoded_include_probability.pt')\n",
    "\n",
    "encoded_fact_pool = torch.index_select(encoded_database, 0, torch.tensor(allowed_arguments_ids))\n",
    "encoded_fact_pool = jnp.array(encoded_fact_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_networks import *\n",
    "import policy_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to give the log probability of pi(f | s) so gradient can be computed directly\n",
    "#also returns sampled index and contexts to determine goal to give tactic network\n",
    "def sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes):\n",
    "    context_scores = context_net(context_params, rng_key, jax_reps)\n",
    "    contexts_by_fringe, scores_by_fringe = split_by_fringe(context_set, context_scores, fringe_sizes)\n",
    "    fringe_scores = []\n",
    "    for s in scores_by_fringe:\n",
    "        fringe_score = jnp.sum(s)\n",
    "        fringe_scores.append(fringe_score)\n",
    "    #TODO some fringes can be empty, but still give value 0 which assigns nonzero probability?\n",
    "    fringe_scores = jnp.stack(fringe_scores)\n",
    "    fringe_probs = jax.nn.softmax(fringe_scores)\n",
    "\n",
    "    #samples, gives an index (looks like it does gumbel softmax under the hood to keep differentiability?)\n",
    "    sampled_idx = random.categorical(rng_key,fringe_probs)\n",
    "\n",
    "    prob = fringe_probs[sampled_idx]\n",
    "    log_prob = jnp.log(prob)\n",
    "    return log_prob, (sampled_idx, contexts_by_fringe)\n",
    "                                                           \n",
    "#grad_log_context, (fringe_idx, contexts_by_fringe) = jax.grad(sample_fringe, has_aux=True)(context_params, apply_context, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "\n",
    "#takes a goal encoding and samples tactic from network, and returns log prob for gradient \n",
    "def sample_tactic(tactic_params, tac_net, rng_key, goal_endcoding, action_size=len(tactic_pool)):\n",
    "    tac_probs = tac_net(tactic_params, rng_key, goal_endcoding, action_size)[0]\n",
    "    tac_idx = random.categorical(rng_key, tac_probs)\n",
    "    log_prob = jnp.log(tac_probs[tac_idx])\n",
    "    return log_prob, tac_idx\n",
    "\n",
    "#grad_log_tac, tac_idx = jax.grad(sample_tactic, has_aux=True)(tactic_params, apply_tac, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "\n",
    "#sampled_tac = tactic_pool[tac_idx]\n",
    "\n",
    "def sample_term(term_params, term_net, rng_key, candidates):\n",
    "    term_scores = term_net(term_params, rng_key, candidates)\n",
    "    term_scores = jnp.ravel(term_scores)\n",
    "    term_probs = jax.nn.softmax(term_scores)\n",
    "    term_idx = random.categorical(rng_key, term_probs)\n",
    "    log_prob = jnp.log(term_probs[term_idx])\n",
    "    return log_prob, term_idx\n",
    "\n",
    "#grad_log_term, term_idx = jax.grad(sample_term, has_aux=True)(term_params, apply_term, rng_key, candidates)#, tac_idx, len(tactic_pool), candidates.shape[1])\n",
    "\n",
    "#function for sampling single argument given previous arguments, \n",
    "def sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, tactic_size, embedding_dim):\n",
    "    hidden, arg_scores = arg_net(arg_params, rng_key, input_, candidates, hidden, tactic_size, embedding_dim)\n",
    "    arg_scores = jnp.ravel(arg_scores)\n",
    "    arg_probs = jax.nn.softmax(arg_scores)\n",
    "    arg_idx = random.categorical(rng_key, arg_probs)\n",
    "    log_prob = jnp.log(arg_probs[arg_idx])\n",
    "    return log_prob, (arg_idx, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (env.history)\n",
    "#env.step((0,0,'strip_tac'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(1111)\n",
    "\n",
    "init_context, apply_context = hk.transform(policy_networks._context_forward)\n",
    "apply_context = jax.jit(apply_context)\n",
    "\n",
    "init_tac, apply_tac = hk.transform(policy_networks._tac_forward)\n",
    "apply_tac = partial(jax.jit, static_argnums=3)(apply_tac)\n",
    "\n",
    "init_term, apply_term = hk.transform(policy_networks._term_no_tac_forward)\n",
    "apply_term = jax.jit(apply_term)\n",
    "\n",
    "init_arg, apply_arg = hk.transform(policy_networks._arg_forward)\n",
    "apply_arg = partial(jax.jit, static_argnums=(5,6))(apply_arg)\n",
    "\n",
    "#initialise these with e.g. random uniform, glorot, He etc. should exist outside function for action selection \n",
    "context_params = init_context(rng_key, random.normal(rng_key, jax_reps.shape))\n",
    "\n",
    "tactic_params = init_tac(rng_key, random.normal(rng_key, (1,256)), len(tactic_pool))\n",
    "\n",
    "#term_policy for now is only considering variables for induction, hence does not need any arguments \n",
    "term_params = init_term(rng_key, random.normal(rng_key, (1,512)))\n",
    "\n",
    "hidden = random.normal(rng_key, (1,256))\n",
    "\n",
    "init_state = hk.LSTMState(hidden, hidden)\n",
    "\n",
    "arg_params = init_arg(rng_key, random.randint(rng_key, (), 0, len(tactic_pool)), random.normal(rng_key, (1,512)), init_state, len(tactic_pool), 256)\n",
    "\n",
    "\n",
    "def loss(context_params, tactic_params, term_params, arg_params, context_net, tactic_net, term_net, arg_net, jax_reps, context_set, fringe_sizes, rng_key, env):\n",
    "    \n",
    "    log_context, (fringe_idx, contexts_by_fringe) = sample_fringe(context_params, context_net, rng_key, jax_reps, context_set, fringe_sizes)\n",
    "    \n",
    "    target_context = contexts_by_fringe[fringe_idx][0]\n",
    "    target_goal = target_context[\"polished\"][\"goal\"]\n",
    "    target_representation = jax_reps[context_set.index(target_context)]\n",
    "    \n",
    "    log_tac, tac_idx = sample_tactic(tactic_params, tactic_net, rng_key, jnp.expand_dims(target_representation,0), len(tactic_pool))\n",
    "    \n",
    "    sampled_tac = tactic_pool[tac_idx]\n",
    "    arg_logs = []\n",
    "\n",
    "    tactic = sampled_tac\n",
    "    #for testing\n",
    "    \n",
    "    #sampled_tac = \"Induct_on\"\n",
    "\n",
    "    #if tactic requires no argument\n",
    "    if sampled_tac in no_arg_tactic:\n",
    "        full_tactic = sampled_tac #tactic_pool[tac]\n",
    "\n",
    "\n",
    "    #Induct_on case; use term policy to find which term to induct on \n",
    "    elif sampled_tac in term_tactic:\n",
    "\n",
    "        goal_tokens = target_goal.split()\n",
    "        term_tokens = [[t] for t in set(goal_tokens) if t[0] == \"V\"]\n",
    "        #add conditional if tokens is empty \n",
    "\n",
    "        #now want encodings for terms from AE\n",
    "\n",
    "        term_reps = []\n",
    "\n",
    "        for term in term_tokens:\n",
    "            term_rep, _ = batch_encoder_.encode([term])\n",
    "            #output is bidirectional so concat vectors\n",
    "            term_reps.append(torch.cat(term_rep.split(1), dim=2).squeeze(0))\n",
    "\n",
    "        # convert to jax\n",
    "        term_reps = jnp.stack([jnp.array(term_reps[i][0]) for i in range(len(term_reps))])\n",
    "\n",
    "        # now want inputs to term_net to be target_representation (i.e. goal) concatenated with terms\n",
    "        # models the policies conditional dependence of the term given the goal\n",
    "\n",
    "        #stack goal representation for each token\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in term_tokens])\n",
    "\n",
    "        #concat with term encodings to give candidate matrix\n",
    "        candidates = jnp.concatenate([goal_stack, term_reps], 1)\n",
    "\n",
    "        log_term, term_idx = sample_term(term_params, term_net, rng_key, candidates)\n",
    "\n",
    "        sampled_term = term_tokens[term_idx]\n",
    "\n",
    "        tm = sampled_term[0][1:] # remove headers, e.g., \"V\" / \"C\" / ...\n",
    "    \n",
    "        arg_logs = [log_term]\n",
    "        \n",
    "        if tm:\n",
    "            tactic = \"Induct_on `{}`\".format(tm)\n",
    "        else:\n",
    "            # only to raise an error\n",
    "            tactic = \"Induct_on\"\n",
    "        \n",
    "    #argument tactic\n",
    "    else:\n",
    "        #stack goals to possible arguments to feed into FFN\n",
    "        goal_stack = jnp.concatenate([jnp.expand_dims(target_representation,0) for _ in encoded_fact_pool])\n",
    "        candidates = jnp.concatenate([encoded_fact_pool, goal_stack], 1)\n",
    "        \n",
    "        #initial state set as goal\n",
    "        hidden = jnp.expand_dims(target_representation,0)\n",
    "        init_state = hk.LSTMState(hidden,hidden)\n",
    "    \n",
    "        # run through first with tac_idx to initialise state with tactic as c_0\n",
    "        hidden, _ = arg_net(arg_params, rng_key, tac_idx, candidates, init_state, len(tactic_pool), 256)\n",
    "        \n",
    "        ARG_LEN = 5\n",
    "        arg_inds = []\n",
    "        arg_logs = []\n",
    "        input_ = tac_idx\n",
    "        for _ in range(ARG_LEN):\n",
    "            log_arg, (arg_idx, hidden) = sample_arg(arg_params, arg_net, rng_key, input_, candidates, hidden, len(tactic_pool), 256)\n",
    "            arg_logs.append(log_arg)\n",
    "            arg_inds.append(arg_idx)\n",
    "            input_ = jnp.expand_dims(encoded_fact_pool[arg_idx], 0)\n",
    "        \n",
    "        arg = [candidate_args[i] for i in arg_inds]\n",
    "\n",
    "        tactic = env.assemble_tactic(sampled_tac, arg)\n",
    "        \n",
    "        #print (tactic)\n",
    "    \n",
    "    \n",
    "    action = (int(fringe_idx), 0, tactic)\n",
    "    #print (action)\n",
    "    \n",
    "    try:\n",
    "        # when step is performed, env.history (probably) changes\n",
    "        # if goal_index == 0:\n",
    "        #     raise \"boom\"\n",
    "        reward, done = env.step(action)\n",
    "\n",
    "    except:\n",
    "        print(\"Step exception raised.\")\n",
    "        # print(\"Fringe: {}\".format(env.history))\n",
    "        print(\"Handling: {}\".format(env.handling))\n",
    "        print(\"Using: {}\".format(env.using))\n",
    "        # try again\n",
    "        # counter = env.counter\n",
    "        frequency = env.frequency\n",
    "        env.close()\n",
    "        print(\"Aborting current game ...\")\n",
    "        print(\"Restarting environment ...\")\n",
    "        print(env.goal)\n",
    "        env = HolEnv(env.goal)\n",
    "        flag = False\n",
    "        return \n",
    "        \n",
    "    #print (env.history)\n",
    "\n",
    "    \n",
    "    loss = (log_tac + log_context  + sum(arg_logs)) * reward\n",
    "\n",
    "    return loss\n",
    "\n",
    "gradients = jax.grad(loss, argnums=(0,1,2,3))(context_params, tactic_params, term_params, arg_params, apply_context, apply_tac, apply_term, apply_arg, jax_reps, context_set, fringe_sizes, rng_key, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can multiply learning rate by reward for each update to give the policy gradient after grad of log probs is done \n",
    "context_lr = 1e-2\n",
    "tactic_lr = 1e-2 \n",
    "arg_lr = 1e-2\n",
    "term_lr = 1e-2\n",
    "\n",
    "context_optimiser = optax.rmsprop(context_lr)\n",
    "tactic_optimiser = optax.rmsprop(tactic_lr)\n",
    "arg_optimiser = optax.rmsprop(arg_lr)\n",
    "term_optimiser = optax.rmsprop(term_lr)\n",
    "\n",
    "opt_state_context = context_optimiser.init(context_params)\n",
    "opt_state_tactic = tactic_optimiser.init(tactic_params)\n",
    "opt_state_arg = arg_optimiser.init(arg_params)\n",
    "opt_state_term = term_optimiser.init(term_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_updates, opt_state_context = context_optimiser.update(gradients[0], opt_state_context)\n",
    "context_params = optax.apply_updates(context_params, context_updates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
