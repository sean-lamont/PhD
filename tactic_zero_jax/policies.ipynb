{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "alternate-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import optax\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "from jax import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "revised-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define networks for agent as in torch implementation\n",
    "\n",
    "class ContextPolicy(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)    \n",
    "        \n",
    "        \n",
    "        self.conv1 = hk.Conv2D(32, 2, stride=2) #out_channels, kernel_size (stride defaults to 1, in_channels done automatically) \n",
    "        self.bn1 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.conv2 = hk.Conv2D(64, 2, stride=2)\n",
    "        self.bn2 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.fc = hk.Linear(512)\n",
    "        self.head = hk.Linear(1)\n",
    "\n",
    "    def __call__(self, x, is_training=True):\n",
    "    \n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = jax.nn.relu(self.bn1(x, is_training))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "  \n",
    "        x = self.bn2(x, is_training)\n",
    "\n",
    "        x = jax.nn.relu(x)\n",
    "        #x = jax.nn.relu(self.bn2(self.conv2(x), is_training))\n",
    " \n",
    "        x = jax.nn.relu(self.fc(x)) #check correct when compared to torch version\n",
    "\n",
    "        x = jax.nn.sigmoid(self.head(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "def _context_forward(x):\n",
    "    module = ContextPolicy()\n",
    "    return module(x)\n",
    "\n",
    "\n",
    "\n",
    "# # input size: (1, 1, 4, 64)\n",
    "\n",
    "class TacPolicy(hk.Module):\n",
    "    def __init__(self, action_size, name=None):\n",
    "        super().__init__(name=name)    \n",
    "        \n",
    "        \n",
    "        self.conv1 = hk.Conv2D(32, 2, stride=2) #out_channels, kernel_size (stride defaults to 1, in_channels done automatically) \n",
    "        self.bn1 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.conv2 = hk.Conv2D(64, 2, stride=2)\n",
    "        self.bn2 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.fc = hk.Linear(512)\n",
    "        self.head = hk.Linear(action_size)\n",
    "\n",
    "    def __call__(self, x, is_training=True):\n",
    "        x = self.conv1(x)\n",
    "        x = jax.nn.relu(self.bn1(x, is_training))\n",
    "        x = jax.nn.relu(self.bn2(self.conv2(x), is_training))\n",
    "        x = jax.nn.relu(self.fc(x)) #check correct when compared to torch version\n",
    "        x = jax.nn.softmax(self.head(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "def _tac_forward(x, action_size):\n",
    "    module = TacPolicy(action_size)\n",
    "    return module(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ArgPolicy(hk.Module):\n",
    "    def __init__(self, hidden_dim, name=None):\n",
    "        super().__init__(name=name)    \n",
    "        self.lstm = hk.LSTM(hidden_dim)\n",
    "        \n",
    "        self.conv1 = hk.Conv2D(32, 2, stride=2) #out_channels, kernel_size (in_channels done automatically) \n",
    "        self.bn1 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.conv2 = hk.Conv2D(64, 2, stride=2)\n",
    "        self.bn2 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.conv3 = hk.Conv2D(128, 2, stride=2)\n",
    "        self.bn3 = hk.BatchNorm(False, False, 0.999)\n",
    "\n",
    "        self.fc = hk.Linear(128)\n",
    "        self.head = hk.Linear(1)\n",
    "\n",
    "        \n",
    "\n",
    "    # x is the previously predicted argument / tactic.\n",
    "    # candidates is a matrix of possible arguments concatenated with the hidden states.\n",
    "\n",
    "    def __call__(self, x, candidates, hidden, is_training=True):\n",
    "        #x = jnp.reshape(x, (1,-1))\n",
    "        \n",
    "        s = self.conv1(candidates)\n",
    "        s = jax.nn.relu(self.bn1(s, is_training))\n",
    "        s = jax.nn.relu(self.bn2(self.conv2(s), is_training))\n",
    "        s = jax.nn.relu(self.bn3(self.conv3(s), is_training))\n",
    "\n",
    "        s = jax.nn.relu(self.fc(s)) #check correct when compared to torch version i.e. s.view(s.size(0), -1)\n",
    "        scores = jax.nn.sigmoid(self.head(s))\n",
    "                \n",
    "        o, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        return hidden, scores\n",
    "\n",
    "def _arg_forward(x, hidden_dim, candidates, hidden, is_init=False):\n",
    "    module = ArgPolicy(hidden_dim)\n",
    "    if is_init:\n",
    "        #TODO add batch dimension\n",
    "        #TODO intitialise initial state as g\n",
    "        return module(x, candidates, module.lstm.initial_state(10))\n",
    "    return module(x, candidates, hidden)\n",
    "\n",
    "\n",
    "\n",
    "class TermPolicy(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)    \n",
    "        \n",
    "        \n",
    "        self.conv1 = hk.Conv2D(32, 2) #out_channels, kernel_size (stride defaults to 1, in_channels done automatically) \n",
    "        self.bn1 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.conv2 = hk.Conv2D(64, 2)\n",
    "        self.bn2 = hk.BatchNorm(False, False, 0.999)\n",
    "        self.fc = hk.Linear(128)\n",
    "        self.head = hk.Linear(1)\n",
    "\n",
    "    def __call__(self, x, is_training=True):\n",
    "        x = self.conv1(x)\n",
    "        x = jax.nn.relu(self.bn1(x, is_training))\n",
    "        x = jax.nn.relu(self.bn2(self.conv2(x), is_training))\n",
    "        x = jax.nn.relu(self.fc(x)) #check correct when compared to torch version\n",
    "        x = jax.nn.sigmoid(self.head(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "def _term_forward(x):\n",
    "    module = TermPolicy()\n",
    "    return module(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "offensive-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_context, apply_context = hk.transform_with_state(_context_forward)\n",
    "init_tac, apply_tac = hk.transform_with_state(_tac_forward)\n",
    "init_arg, apply_arg = hk.transform_with_state(_arg_forward)\n",
    "init_term, apply_term = hk.transform_with_state(_term_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "other-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(100)\n",
    "\n",
    "batch_size = 10\n",
    "goal_dim = 256\n",
    "\n",
    "#goal and tactic networks, which take as input a goal\n",
    "x_term = random.normal(rng_key, (batch_size, goal_dim))\n",
    "x_tac = random.normal(rng_key, (batch_size, goal_dim))\n",
    "\n",
    "#candidate network, TODO make sure shapes match what is expected given the old implementation with e.g. MAX_LEN, MAX_CONTEX\n",
    "                      \n",
    "c_arg = random.normal(rng_key, (batch_size, goal_dim))\n",
    "\n",
    "h1 = random.normal(rng_key, (batch_size, goal_dim))\n",
    "\n",
    "x_arg = random.normal(rng_key, (batch_size, goal_dim))\n",
    "\n",
    "x_context = random.normal(rng_key, (batch_size, goal_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "excessive-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_params_context, initial_state_context = init_context(rng_key, x_context)\n",
    "# initial_params_tac, initial_state_tac = init_tac(rng_key, x_tac, 4)\n",
    "# initial_params_arg, initial_state_arg = init_arg(rng_key, x_arg, 128, c_arg, h1, True)\n",
    "# initial_params_term, initial_state_term = init_term(rng_key, x_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "advisory-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_context, new_state_context = apply_context(initial_params_context, initial_state_context, rng_key, x_context)\n",
    "# out_tac, new_state_tac = apply_tac(initial_params_tac, initial_state_tac, rng_key, x_tac, 4)\n",
    "# out_arg, new_state_arg = apply_arg(initial_params_arg, initial_state_arg, rng_key, x_arg, 128, c_arg, h1, True)\n",
    "# out_term, new_state_term = apply_term(initial_params_term, initial_state_term, rng_key, x_term)\n",
    "\n",
    "# print (out_arg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
